[
    {
        "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "url": "https://www.semanticscholar.org/paper/3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
        "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
        "year": 2019,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.01703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "3407277",
                "name": "Adam Paszke"
            },
            {
                "authorId": "39793298",
                "name": "Sam Gross"
            },
            {
                "authorId": "1403239967",
                "name": "Francisco Massa"
            },
            {
                "authorId": "1977806",
                "name": "Adam Lerer"
            },
            {
                "authorId": "2065251344",
                "name": "James Bradbury"
            },
            {
                "authorId": "114250963",
                "name": "Gregory Chanan"
            },
            {
                "authorId": "2059271276",
                "name": "Trevor Killeen"
            },
            {
                "authorId": "3370429",
                "name": "Zeming Lin"
            },
            {
                "authorId": "3365851",
                "name": "N. Gimelshein"
            },
            {
                "authorId": "3029482",
                "name": "L. Antiga"
            },
            {
                "authorId": "3050846",
                "name": "Alban Desmaison"
            },
            {
                "authorId": "1473151134",
                "name": "Andreas K\u00f6pf"
            },
            {
                "authorId": "2052812305",
                "name": "E. Yang"
            },
            {
                "authorId": "2253681376",
                "name": "Zachary DeVito"
            },
            {
                "authorId": "10707709",
                "name": "Martin Raison"
            },
            {
                "authorId": "41203992",
                "name": "Alykhan Tejani"
            },
            {
                "authorId": "22236100",
                "name": "Sasank Chilamkurthy"
            },
            {
                "authorId": "32163737",
                "name": "Benoit Steiner"
            },
            {
                "authorId": "152599430",
                "name": "Lu Fang"
            },
            {
                "authorId": "2113829116",
                "name": "Junjie Bai"
            },
            {
                "authorId": "2127604",
                "name": "Soumith Chintala"
            }
        ],
        "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks."
    },
    {
        "paperId": "f28e387d4229c5f690ce4570a391c0f47e7155c7",
        "url": "https://www.semanticscholar.org/paper/f28e387d4229c5f690ce4570a391c0f47e7155c7",
        "title": "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
        "year": 2020,
        "openAccessPdf": {
            "url": "https://arxiv.org/pdf/1904.08128",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s41592-020-01008-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s41592-020-01008-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "7886986",
                "name": "Fabian Isensee"
            },
            {
                "authorId": "51433108",
                "name": "P. Jaeger"
            },
            {
                "authorId": "51011129",
                "name": "Simon A. A. Kohl"
            },
            {
                "authorId": "152800798",
                "name": "Jens Petersen"
            },
            {
                "authorId": "1397951928",
                "name": "Klaus Hermann Maier-Hein"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "0084f3cb0a1754272151c5268a783f24bf5676a0",
        "url": "https://www.semanticscholar.org/paper/0084f3cb0a1754272151c5268a783f24bf5676a0",
        "title": "Review of deep learning: concepts, CNN architectures, challenges, applications, future directions",
        "year": 2021,
        "openAccessPdf": {
            "url": "https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-021-00444-8",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8010506, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2281247548",
                "name": "Laith Alzubaidi"
            },
            {
                "authorId": "1754598",
                "name": "Jinglan Zhang"
            },
            {
                "authorId": "2073994643",
                "name": "A. Humaidi"
            },
            {
                "authorId": "1409267833",
                "name": "Ayad Al-dujaili"
            },
            {
                "authorId": "145013346",
                "name": "Y. Duan"
            },
            {
                "authorId": "1410016448",
                "name": "O. Al-Shamma"
            },
            {
                "authorId": "2146732191",
                "name": "Jos\u00e9 I. Santamar\u00eda"
            },
            {
                "authorId": "1412407676",
                "name": "M. Fadhel"
            },
            {
                "authorId": "1412958882",
                "name": "Muthana Al-Amidie"
            },
            {
                "authorId": "144121680",
                "name": "Laith Farhan"
            }
        ],
        "abstract": "In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational approach in the field of ML, thus achieving outstanding results on several complex cognitive tasks, matching or even beating those provided by human performance. One of the benefits of DL is the ability to learn massive amounts of data. The DL field has grown fast in the last few years and it has been extensively used to successfully address a wide range of traditional applications. More importantly, DL has outperformed well-known ML techniques in many domains, e.g., cybersecurity, natural language processing, bioinformatics, robotics and control, and medical information processing, among many others. Despite it has been contributed several works reviewing the State-of-the-Art on DL, all of them only tackled one aspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this contribution, we propose using a more holistic approach in order to provide a more suitable starting point from which to develop a full understanding of DL. Specifically, this review attempts to provide a more comprehensive survey of the most important aspects of DL and including those enhancements recently added to the field. In particular, this paper outlines the importance of DL, presents the types of DL techniques and networks. It then presents convolutional neural networks (CNNs) which the most utilized DL network type and describes the development of CNNs architectures together with their main features, e.g., starting with the AlexNet network and closing with the High-Resolution network (HR.Net). Finally, we further present the challenges and suggested solutions to help researchers understand the existing research gaps. It is followed by a list of the major DL applications. Computational tools including FPGA, GPU, and CPU are summarized along with a description of their influence on DL. The paper ends with the evolution matrix, benchmark datasets, and summary and conclusion."
    },
    {
        "paperId": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
        "url": "https://www.semanticscholar.org/paper/7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
        "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
        "year": 2017,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1706.06083, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "143826246",
                "name": "A. Ma\u0327dry"
            },
            {
                "authorId": "17775913",
                "name": "Aleksandar Makelov"
            },
            {
                "authorId": "152772922",
                "name": "Ludwig Schmidt"
            },
            {
                "authorId": "2754804",
                "name": "Dimitris Tsipras"
            },
            {
                "authorId": "2869958",
                "name": "Adrian Vladu"
            }
        ],
        "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL."
    },
    {
        "paperId": "ff7bcaa4556cb13fc7bf03e477172493546172cd",
        "url": "https://www.semanticscholar.org/paper/ff7bcaa4556cb13fc7bf03e477172493546172cd",
        "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
        "year": 2017,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.04977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "47645184",
                "name": "Alex Kendall"
            },
            {
                "authorId": "2681954",
                "name": "Y. Gal"
            }
        ],
        "abstract": "There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks."
    },
    {
        "paperId": "3813b88a4ec3c63919df47e9694b577f4691f7e5",
        "url": "https://www.semanticscholar.org/paper/3813b88a4ec3c63919df47e9694b577f4691f7e5",
        "title": "A survey on Image Data Augmentation for Deep Learning",
        "year": 2019,
        "openAccessPdf": {
            "url": "https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-019-0197-0",
            "status": "GOLD",
            "license": "CCBY",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1186/s40537-019-0197-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1186/s40537-019-0197-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "150064574",
                "name": "Connor Shorten"
            },
            {
                "authorId": "1725285",
                "name": "T. Khoshgoftaar"
            }
        ],
        "abstract": "Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data."
    },
    {
        "paperId": "d86084808994ac54ef4840ae65295f3c0ec4decd",
        "url": "https://www.semanticscholar.org/paper/d86084808994ac54ef4840ae65295f3c0ec4decd",
        "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
        "year": 2019,
        "openAccessPdf": {
            "url": "http://manuscript.elsevier.com/S0021999118307125/pdf/S0021999118307125.pdf",
            "status": "BRONZE",
            "license": "publisher-specific-oa",
            "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/J.JCP.2018.10.045?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/J.JCP.2018.10.045, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "145401977",
                "name": "M. Raissi"
            },
            {
                "authorId": "3410970",
                "name": "P. Perdikaris"
            },
            {
                "authorId": "1720124",
                "name": "G. Karniadakis"
            }
        ],
        "abstract": null
    },
    {
        "paperId": "7998468d99ab07bb982294d1c9b53a3bf3934fa6",
        "url": "https://www.semanticscholar.org/paper/7998468d99ab07bb982294d1c9b53a3bf3934fa6",
        "title": "Object Detection With Deep Learning: A Review",
        "year": 2018,
        "openAccessPdf": {
            "url": "http://arxiv.org/pdf/1807.05511",
            "status": "GREEN",
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.05511, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "33698309",
                "name": "Zhong-Qiu Zhao"
            },
            {
                "authorId": null,
                "name": "Peng Zheng"
            },
            {
                "authorId": "51132438",
                "name": "Shou-tao Xu"
            },
            {
                "authorId": "1748808",
                "name": "Xindong Wu"
            }
        ],
        "abstract": "Due to object detection\u2019s close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems."
    },
    {
        "paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
        "url": "https://www.semanticscholar.org/paper/f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6",
        "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
        "year": 2015,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null,
            "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.02142, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
        },
        "authors": [
            {
                "authorId": "2681954",
                "name": "Y. Gal"
            },
            {
                "authorId": "1744700",
                "name": "Zoubin Ghahramani"
            }
        ],
        "abstract": "Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning."
    },
    {
        "paperId": "2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb",
        "url": "https://www.semanticscholar.org/paper/2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb",
        "title": "Deep Learning",
        "year": 2015,
        "openAccessPdf": {
            "url": "",
            "status": null,
            "license": null
        },
        "authors": [
            {
                "authorId": "1688882",
                "name": "Yann LeCun"
            },
            {
                "authorId": "1751762",
                "name": "Yoshua Bengio"
            },
            {
                "authorId": "1695689",
                "name": "Geoffrey E. Hinton"
            }
        ],
        "abstract": null
    }
]