[
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "",
            "methodology": "",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the"
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired DataFrame:\n                   Pearson Correlation Coefficient\nCol1 Col2                                 \n0        3                            0.373153\n1        3                            0.419219\n          4                            0.356149\n3        4                            0.389972\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired Series:\n0  3    0.373153\n1  3    0.419219\n    4    0.356149\n3  4    0.389972\ndtype: float64\nFigure 22: An example problem of semantic perturbation. The type of the desired result has been changed but the content\nstill keeps the same.\nOrigin\nProblem:\nI have a logistic regression model using Pytorch, where my input is high-dimensional and my output must be a \nscalar - 0, 1 or 2.\nI'm using a linear layer combined with a softmax layer to return a n x 3 tensor, where each column represents the \nprobability of the input falling in one of the three classes (0, 1 or 2).\nHowever, I must return a n x 1 tensor, so I need to somehow pick the highest probability for each input and create \na tensor indicating which class had the highest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.2, 0.2],\n [0.1, 0.8, 0.1]]\nAnd I must return this:\n[[2],\n [0],\n [1]]\nProblem:\n\u2026[omit for brevity]\nHowever, I must return a 1 x n tensor, and I want to somehow pick the lowest probability for each input and \ncreate a tensor indicating which class had the lowest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.3, 0.1],\n [0.15, 0.8, 0.05]]\nAnd I must return this:\n[1, 2, 2], which has the type torch.LongTensor\nFigure 23: An example problem that is dif\ufb01cult re-written with a combination of surface and semantic perturbations"
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired DataFrame:\n                   Pearson Correlation Coefficient\nCol1 Col2                                 \n0        3                            0.373153\n1        3                            0.419219\n          4                            0.356149\n3        4                            0.389972\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired Series:\n0  3    0.373153\n1  3    0.419219\n    4    0.356149\n3  4    0.389972\ndtype: float64\nFigure 22: An example problem of semantic perturbation. The type of the desired result has been changed but the content\nstill keeps the same.\nOrigin\nProblem:\nI have a logistic regression model using Pytorch, where my input is high-dimensional and my output must be a \nscalar - 0, 1 or 2.\nI'm using a linear layer combined with a softmax layer to return a n x 3 tensor, where each column represents the \nprobability of the input falling in one of the three classes (0, 1 or 2).\nHowever, I must return a n x 1 tensor, so I need to somehow pick the highest probability for each input and create \na tensor indicating which class had the highest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.2, 0.2],\n [0.1, 0.8, 0.1]]\nAnd I must return this:\n[[2],\n [0],\n [1]]\nProblem:\n\u2026[omit for brevity]\nHowever, I must return a 1 x n tensor, and I want to somehow pick the lowest probability for each input and \ncreate a tensor indicating which class had the lowest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.3, 0.1],\n [0.15, 0.8, 0.05]]\nAnd I must return this:\n[1, 2, 2], which has the type torch.LongTensor\nFigure 23: An example problem that is dif\ufb01cult re-written with a combination of surface and semantic perturbations\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI can't figure out how to do a Two-sample KS test in Scipy.\n\u2026[omit for brevity]\ntest_stat = kstest(x, 'norm')\n#>>> test_stat\n#(0.021080234718821145, 0.76584491300591395)\nWhich means that at p-value of 0.76 we can not reject the null hypothesis that the two distributions are identical.\nHowever, I want to compare two distributions and see if I can reject the null hypothesis that they are identical.\n\u2026[omit for brevity]\nI tried the naive:\ntest_stat = kstest(x, z)\nand got the following error:\nTypeError: 'numpy.ndarray' object is not callable\nIs there a way to do a two-sample KS test in Python? If so, how should I do it?\nThank You in Advance\nProblem:\n\u2026[omit for brevity]\nIs there a way to do a two-sample KS test in Python, then test whether I can reject the null hypothesis that the \ntwo distributions are identical(result=True means able to reject, and the vice versa) based on alpha? If so, how \nshould I do it?\nThank You in Advance\nFigure 24: An example problem that is dif\ufb01cult re-written for more complexity\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. inv_A is an inverse of column A and so on.\n\u2026 [omitted for brevity]\nObviously there are redundant methods like doing this in a loop, but there should exist \nmuch more pythonic ways of doing it \u2026 [omitted for brevity]\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nresult = ...# put solution in this variable\nBEGIN SOLUTION\n<code>\nFigure 25: Completion prompt corresponding to Figure 1."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired DataFrame:\n                   Pearson Correlation Coefficient\nCol1 Col2                                 \n0        3                            0.373153\n1        3                            0.419219\n          4                            0.356149\n3        4                            0.389972\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired Series:\n0  3    0.373153\n1  3    0.419219\n    4    0.356149\n3  4    0.389972\ndtype: float64\nFigure 22: An example problem of semantic perturbation. The type of the desired result has been changed but the content\nstill keeps the same.\nOrigin\nProblem:\nI have a logistic regression model using Pytorch, where my input is high-dimensional and my output must be a \nscalar - 0, 1 or 2.\nI'm using a linear layer combined with a softmax layer to return a n x 3 tensor, where each column represents the \nprobability of the input falling in one of the three classes (0, 1 or 2).\nHowever, I must return a n x 1 tensor, so I need to somehow pick the highest probability for each input and create \na tensor indicating which class had the highest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.2, 0.2],\n [0.1, 0.8, 0.1]]\nAnd I must return this:\n[[2],\n [0],\n [1]]\nProblem:\n\u2026[omit for brevity]\nHowever, I must return a 1 x n tensor, and I want to somehow pick the lowest probability for each input and \ncreate a tensor indicating which class had the lowest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.3, 0.1],\n [0.15, 0.8, 0.05]]\nAnd I must return this:\n[1, 2, 2], which has the type torch.LongTensor\nFigure 23: An example problem that is dif\ufb01cult re-written with a combination of surface and semantic perturbations\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI can't figure out how to do a Two-sample KS test in Scipy.\n\u2026[omit for brevity]\ntest_stat = kstest(x, 'norm')\n#>>> test_stat\n#(0.021080234718821145, 0.76584491300591395)\nWhich means that at p-value of 0.76 we can not reject the null hypothesis that the two distributions are identical.\nHowever, I want to compare two distributions and see if I can reject the null hypothesis that they are identical.\n\u2026[omit for brevity]\nI tried the naive:\ntest_stat = kstest(x, z)\nand got the following error:\nTypeError: 'numpy.ndarray' object is not callable\nIs there a way to do a two-sample KS test in Python? If so, how should I do it?\nThank You in Advance\nProblem:\n\u2026[omit for brevity]\nIs there a way to do a two-sample KS test in Python, then test whether I can reject the null hypothesis that the \ntwo distributions are identical(result=True means able to reject, and the vice versa) based on alpha? If so, how \nshould I do it?\nThank You in Advance\nFigure 24: An example problem that is dif\ufb01cult re-written for more complexity\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. inv_A is an inverse of column A and so on.\n\u2026 [omitted for brevity]\nObviously there are redundant methods like doing this in a loop, but there should exist \nmuch more pythonic ways of doing it \u2026 [omitted for brevity]\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nresult = ...# put solution in this variable\nBEGIN SOLUTION\n<code>\nFigure 25: Completion prompt corresponding to Figure 1.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nsolve this question with example variable `clf` and \nput result in `proba`\nBEGIN SOLUTION\n<code>\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nproba = clf.predict_proba(X_test)\nprint(proba)\n</code>\nProblem:\nSay that I want to train BaggingClassifier that uses DecisionTreeClassifier:\ndt = DecisionTreeClassifier(max_depth = 1)\nbc = BaggingClassifier(dt, n_estimators = 20, max_samples = 0.5, max_features = 0.5)\nbc = bc.fit(X_train, y_train)\nI would like to use GridSearchCV to find the best parameters for both BaggingClassifier and \nDecisionTreeClassifier(e.g. max_depth from DecisionTreeClassifier and max_samples from \nBaggingClassifier), what is the syntax for this? Besides, you can just use the default arguments of GridSearchCV.\nFigure 26: More complex Completion (on the right) prompt that requires additional information for a solution.\nProblem:\nI am using Pandas to get a dataframe like this:\n    name  a  b   c\n0  Aaron 3  5   7\n1  Aaron 3  6   9\n2  Aaron 3  6  10\n3  Brave  4  6   0\n4  Brave  3  6   1\nI want to replace each name with a unique ID so output looks like:\n  name  a  b   c\n0    1     3   5   7\n1    1     3   6   9\n2    1     3   6  10\n3    2     4   6   0\n4    2     3   6   1\nHow can I do that?\nReference Solution\n# df: pd.DataFrame as input\nresult = df.replace(df['name'].unique(),\n range(1, len(df['name'].unique()) + 1))\nWrong Solution\n# create a column named \"ID\"\ndf['ID'] = df.groupby(['name']).ngroup()\nresult = df\nFigure 27: An example wrong solution that misunderstands the requirements and modi\ufb01es on the wrong column."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired DataFrame:\n                   Pearson Correlation Coefficient\nCol1 Col2                                 \n0        3                            0.373153\n1        3                            0.419219\n          4                            0.356149\n3        4                            0.389972\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired Series:\n0  3    0.373153\n1  3    0.419219\n    4    0.356149\n3  4    0.389972\ndtype: float64\nFigure 22: An example problem of semantic perturbation. The type of the desired result has been changed but the content\nstill keeps the same.\nOrigin\nProblem:\nI have a logistic regression model using Pytorch, where my input is high-dimensional and my output must be a \nscalar - 0, 1 or 2.\nI'm using a linear layer combined with a softmax layer to return a n x 3 tensor, where each column represents the \nprobability of the input falling in one of the three classes (0, 1 or 2).\nHowever, I must return a n x 1 tensor, so I need to somehow pick the highest probability for each input and create \na tensor indicating which class had the highest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.2, 0.2],\n [0.1, 0.8, 0.1]]\nAnd I must return this:\n[[2],\n [0],\n [1]]\nProblem:\n\u2026[omit for brevity]\nHowever, I must return a 1 x n tensor, and I want to somehow pick the lowest probability for each input and \ncreate a tensor indicating which class had the lowest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.3, 0.1],\n [0.15, 0.8, 0.05]]\nAnd I must return this:\n[1, 2, 2], which has the type torch.LongTensor\nFigure 23: An example problem that is dif\ufb01cult re-written with a combination of surface and semantic perturbations\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI can't figure out how to do a Two-sample KS test in Scipy.\n\u2026[omit for brevity]\ntest_stat = kstest(x, 'norm')\n#>>> test_stat\n#(0.021080234718821145, 0.76584491300591395)\nWhich means that at p-value of 0.76 we can not reject the null hypothesis that the two distributions are identical.\nHowever, I want to compare two distributions and see if I can reject the null hypothesis that they are identical.\n\u2026[omit for brevity]\nI tried the naive:\ntest_stat = kstest(x, z)\nand got the following error:\nTypeError: 'numpy.ndarray' object is not callable\nIs there a way to do a two-sample KS test in Python? If so, how should I do it?\nThank You in Advance\nProblem:\n\u2026[omit for brevity]\nIs there a way to do a two-sample KS test in Python, then test whether I can reject the null hypothesis that the \ntwo distributions are identical(result=True means able to reject, and the vice versa) based on alpha? If so, how \nshould I do it?\nThank You in Advance\nFigure 24: An example problem that is dif\ufb01cult re-written for more complexity\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. inv_A is an inverse of column A and so on.\n\u2026 [omitted for brevity]\nObviously there are redundant methods like doing this in a loop, but there should exist \nmuch more pythonic ways of doing it \u2026 [omitted for brevity]\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nresult = ...# put solution in this variable\nBEGIN SOLUTION\n<code>\nFigure 25: Completion prompt corresponding to Figure 1.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nsolve this question with example variable `clf` and \nput result in `proba`\nBEGIN SOLUTION\n<code>\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nproba = clf.predict_proba(X_test)\nprint(proba)\n</code>\nProblem:\nSay that I want to train BaggingClassifier that uses DecisionTreeClassifier:\ndt = DecisionTreeClassifier(max_depth = 1)\nbc = BaggingClassifier(dt, n_estimators = 20, max_samples = 0.5, max_features = 0.5)\nbc = bc.fit(X_train, y_train)\nI would like to use GridSearchCV to find the best parameters for both BaggingClassifier and \nDecisionTreeClassifier(e.g. max_depth from DecisionTreeClassifier and max_samples from \nBaggingClassifier), what is the syntax for this? Besides, you can just use the default arguments of GridSearchCV.\nFigure 26: More complex Completion (on the right) prompt that requires additional information for a solution.\nProblem:\nI am using Pandas to get a dataframe like this:\n    name  a  b   c\n0  Aaron 3  5   7\n1  Aaron 3  6   9\n2  Aaron 3  6  10\n3  Brave  4  6   0\n4  Brave  3  6   1\nI want to replace each name with a unique ID so output looks like:\n  name  a  b   c\n0    1     3   5   7\n1    1     3   6   9\n2    1     3   6  10\n3    2     4   6   0\n4    2     3   6   1\nHow can I do that?\nReference Solution\n# df: pd.DataFrame as input\nresult = df.replace(df['name'].unique(),\n range(1, len(df['name'].unique()) + 1))\nWrong Solution\n# create a column named \"ID\"\ndf['ID'] = df.groupby(['name']).ngroup()\nresult = df\nFigure 27: An example wrong solution that misunderstands the requirements and modi\ufb01es on the wrong column.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI'm using tensorflow 2.10.0.\nI have a list of bytes and I want to convert it to a list of strings:\nx=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd\n8\\xa9',\n\u00a0\u00a0\u00a0b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n\u00a0\u00a0\u00a0\u00a0b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n\u00a0\u00a0\u00a0\u00a0b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n\u00a0\u00a0\u00a0\u00a0b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\u00a0\nHow can I get the string result list in Tensorflow? \nReference Solution\n# x: list of bytes as input\nresult = [tf.compat.as_str_any(a) for a in x]\nWrong Solution\n# Not using method in Tensorflow\nresult = [item.decode('utf-8') for item in x]\nFigure 28: An example wrong solution that uses a common function instead of a function of TensorFlow.\n102\nI think it's a pretty common message for PyTorch users with low GPU memory:\nRuntimeError: CUDA out of memory. Tried to allocate \ud83d\ude0a MiB (GPU \ud83d\ude0a; \ud83d\ude0a GiB total\ncapacity; \ud83d\ude0a GiB already allocated; \ud83d\ude0a MiB free; \ud83d\ude0a cached)\nI tried to process an image by loading each layer to GPU and then loading it back:\n m \n self.children():\n    m.cuda()\n    x = m(x)\n    m.cpu()\n    torch.cuda.empty_cache()\nfor\nin\nBut it doesn't seem to be very effective. I'm wondering is there any tips and tricks to train\nlarge deep learning models while using little GPU memory.\npython\ndeep-learning\npytorch\nobject-detection\nlow-memory\nFigure 29: An example untestable problem involving hardware problems."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1.",
            "introduction": "Data science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant",
            "methodology": "s like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION",
            "conclusion": "We propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired DataFrame:\n                   Pearson Correlation Coefficient\nCol1 Col2                                 \n0        3                            0.373153\n1        3                            0.419219\n          4                            0.356149\n3        4                            0.389972\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired Series:\n0  3    0.373153\n1  3    0.419219\n    4    0.356149\n3  4    0.389972\ndtype: float64\nFigure 22: An example problem of semantic perturbation. The type of the desired result has been changed but the content\nstill keeps the same.\nOrigin\nProblem:\nI have a logistic regression model using Pytorch, where my input is high-dimensional and my output must be a \nscalar - 0, 1 or 2.\nI'm using a linear layer combined with a softmax layer to return a n x 3 tensor, where each column represents the \nprobability of the input falling in one of the three classes (0, 1 or 2).\nHowever, I must return a n x 1 tensor, so I need to somehow pick the highest probability for each input and create \na tensor indicating which class had the highest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.2, 0.2],\n [0.1, 0.8, 0.1]]\nAnd I must return this:\n[[2],\n [0],\n [1]]\nProblem:\n\u2026[omit for brevity]\nHowever, I must return a 1 x n tensor, and I want to somehow pick the lowest probability for each input and \ncreate a tensor indicating which class had the lowest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.3, 0.1],\n [0.15, 0.8, 0.05]]\nAnd I must return this:\n[1, 2, 2], which has the type torch.LongTensor\nFigure 23: An example problem that is dif\ufb01cult re-written with a combination of surface and semantic perturbations\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI can't figure out how to do a Two-sample KS test in Scipy.\n\u2026[omit for brevity]\ntest_stat = kstest(x, 'norm')\n#>>> test_stat\n#(0.021080234718821145, 0.76584491300591395)\nWhich means that at p-value of 0.76 we can not reject the null hypothesis that the two distributions are identical.\nHowever, I want to compare two distributions and see if I can reject the null hypothesis that they are identical.\n\u2026[omit for brevity]\nI tried the naive:\ntest_stat = kstest(x, z)\nand got the following error:\nTypeError: 'numpy.ndarray' object is not callable\nIs there a way to do a two-sample KS test in Python? If so, how should I do it?\nThank You in Advance\nProblem:\n\u2026[omit for brevity]\nIs there a way to do a two-sample KS test in Python, then test whether I can reject the null hypothesis that the \ntwo distributions are identical(result=True means able to reject, and the vice versa) based on alpha? If so, how \nshould I do it?\nThank You in Advance\nFigure 24: An example problem that is dif\ufb01cult re-written for more complexity\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. inv_A is an inverse of column A and so on.\n\u2026 [omitted for brevity]\nObviously there are redundant methods like doing this in a loop, but there should exist \nmuch more pythonic ways of doing it \u2026 [omitted for brevity]\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nresult = ...# put solution in this variable\nBEGIN SOLUTION\n<code>\nFigure 25: Completion prompt corresponding to Figure 1.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nsolve this question with example variable `clf` and \nput result in `proba`\nBEGIN SOLUTION\n<code>\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nproba = clf.predict_proba(X_test)\nprint(proba)\n</code>\nProblem:\nSay that I want to train BaggingClassifier that uses DecisionTreeClassifier:\ndt = DecisionTreeClassifier(max_depth = 1)\nbc = BaggingClassifier(dt, n_estimators = 20, max_samples = 0.5, max_features = 0.5)\nbc = bc.fit(X_train, y_train)\nI would like to use GridSearchCV to find the best parameters for both BaggingClassifier and \nDecisionTreeClassifier(e.g. max_depth from DecisionTreeClassifier and max_samples from \nBaggingClassifier), what is the syntax for this? Besides, you can just use the default arguments of GridSearchCV.\nFigure 26: More complex Completion (on the right) prompt that requires additional information for a solution.\nProblem:\nI am using Pandas to get a dataframe like this:\n    name  a  b   c\n0  Aaron 3  5   7\n1  Aaron 3  6   9\n2  Aaron 3  6  10\n3  Brave  4  6   0\n4  Brave  3  6   1\nI want to replace each name with a unique ID so output looks like:\n  name  a  b   c\n0    1     3   5   7\n1    1     3   6   9\n2    1     3   6  10\n3    2     4   6   0\n4    2     3   6   1\nHow can I do that?\nReference Solution\n# df: pd.DataFrame as input\nresult = df.replace(df['name'].unique(),\n range(1, len(df['name'].unique()) + 1))\nWrong Solution\n# create a column named \"ID\"\ndf['ID'] = df.groupby(['name']).ngroup()\nresult = df\nFigure 27: An example wrong solution that misunderstands the requirements and modi\ufb01es on the wrong column.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI'm using tensorflow 2.10.0.\nI have a list of bytes and I want to convert it to a list of strings:\nx=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd\n8\\xa9',\n\u00a0\u00a0\u00a0b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n\u00a0\u00a0\u00a0\u00a0b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n\u00a0\u00a0\u00a0\u00a0b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n\u00a0\u00a0\u00a0\u00a0b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\u00a0\nHow can I get the string result list in Tensorflow? \nReference Solution\n# x: list of bytes as input\nresult = [tf.compat.as_str_any(a) for a in x]\nWrong Solution\n# Not using method in Tensorflow\nresult = [item.decode('utf-8') for item in x]\nFigure 28: An example wrong solution that uses a common function instead of a function of TensorFlow.\n102\nI think it's a pretty common message for PyTorch users with low GPU memory:\nRuntimeError: CUDA out of memory. Tried to allocate \ud83d\ude0a MiB (GPU \ud83d\ude0a; \ud83d\ude0a GiB total\ncapacity; \ud83d\ude0a GiB already allocated; \ud83d\ude0a MiB free; \ud83d\ude0a cached)\nI tried to process an image by loading each layer to GPU and then loading it back:\n m \n self.children():\n    m.cuda()\n    x = m(x)\n    m.cpu()\n    torch.cuda.empty_cache()\nfor\nin\nBut it doesn't seem to be very effective. I'm wondering is there any tips and tricks to train\nlarge deep learning models while using little GPU memory.\npython\ndeep-learning\npytorch\nobject-detection\nlow-memory\nFigure 29: An example untestable problem involving hardware problems.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n247\nI am using python 2.7 in Ubuntu 14.04. I installed scikit-learn, numpy and matplotlib with\nthese commands:\nsudo apt-get install build-essential python-dev python-numpy \\\npython-numpy-dev python-scipy libatlas-dev g++ python-matplotlib \\\nipython\nBut when I import these packages:\n sklearn.cross_validation \n train_test_split\nfrom\nimport\nIt returns me this error:\nImportError: No module named sklearn.cross_validation\nWhat I need to do?\npython\nscikit-learn\nFigure 30: An example untestable problem involving software errors.\n55\nI would like to understand what \n does in a bit more detail.\nA\n:\ntf.global_variables_initializer\nsparse description is given here\nReturns an Op that initializes global variables.\nBut that doesn't really help me. I know that the op is necessary to initialize the graph, but what\ndoes that actually mean? Is this the step where the graph is complied?\ntensorflow\ndeep-learning\nFigure 31: An example untestable problem involving explanations."
        }
    }
]