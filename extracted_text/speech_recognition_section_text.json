[
    {
        "pdf_file": "paper1.pdf",
        "sections": {
            "abstract": "Recently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs).\nTransformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies.\nOn the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother.\nWe also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1.",
            "introduction": "",
            "methodology": "",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper1.pdf",
        "sections": {
            "abstract": "Recently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs).\nTransformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies.\nOn the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother.\nWe also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1.",
            "introduction": "",
            "methodology": "",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper1.pdf",
        "sections": {
            "abstract": "Recently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs).\nTransformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies.\nOn the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother.\nWe also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1.",
            "introduction": "End-to-end automatic speech recognition (ASR) systems based\non neural networks have seen large improvements in recent\nyears. Recurrent neural networks (RNNs) have been the de-\nfacto choice for ASR [1, 2, 3, 4] as they can model the temporal\ndependencies in the audio sequences effectively [5]. Recently,\nthe Transformer architecture based on self-attention [6, 7] has\nenjoyed widespread adoption for modeling sequences due to its\nability to capture long distance interactions and the high train-\ning ef\ufb01ciency. Alternatively, convolutions have also been suc-\ncessful for ASR [8, 9, 10, 11, 12], which capture local context\nprogressively via a local receptive \ufb01eld layer by layer.\nHowever, models with self-attention or convolutions each\nhas its limitations. While Transformers are good at modeling\nlong-range global context, they are less capable to extract \ufb01ne-\ngrained local feature patterns.\nConvolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation. To combat this issue, contemporary work Con-\ntextNet [10] adopts the squeeze-and-excitation module [13] in\neach residual block to capture longer context. However, it is still\nlimited in capturing dynamic global context as it only applies a\nglobal averaging over the entire sequence.\nRecent works have shown that combining convolution and\nFeed Forward Module\nMulti-Head Self Attention\nModule\nConvolution Module\n1/2 x\n1/2 x\nFeed Forward Module\nDropout\nConformer Blocks\nLinear\nSpecAug\n10 ms rate\n10 ms rate\nConvolution\nSubsampling\n40 ms rate\nx N\n40 ms rate\n40 ms rate\n+\n+\n+\n+\nLayernorm\nFigure 1: Conformer encoder model architecture. Conformer\ncomprises of two macaron-like feed-forward layers with half-\nstep residual connections sandwiching the multi-headed self-\nattention and convolution modules. This is followed by a post\nlayernorm.\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.\n[17] proposed a multi-branch architecture with splitting the in-\nput into two branches: self-attention and convolution; and con-\ncatenating their outputs. Their work targeted mobile applica-\ntions and showed improvements in machine translation tasks.\nIn this work, we study how to organically combine con-\nvolutions with self-attention in ASR models. We hypothesize\nthat both global and local interactions are important for being\nparameter ef\ufb01cient. To achieve this, we propose a novel combi-\nnation of self-attention and convolution will achieve the best of\nboth worlds \u2013 self-attention learns the global interaction whilst\nthe convolutions ef\ufb01ciently capture the relative-offset-based lo-\ncal correlations. Inspired by Wu et al. [17, 18], we introduce\na novel combination of self-attention and convolution, sand-\nwiched between a pair feed forward modules, as illustrated in\nFig 1.\nOur proposed model, named Conformer, achieves state-of-\nthe-art results on LibriSpeech, outperforming the previous best\npublished Transformer Transducer [7] by 15% relative improve-\narXiv:2005.08100v1  [eess.AS]  16 May 2020\nLayernorm\nGlu\nActivation\nPointwise\nConv\nBatchNorm\nSwish\nActivation\n1D\nDepthwise\nConv\nPointwise\nConv\nDropout\n+\nFigure 2: Convolution module. The convolution module contains a pointwise convolution with an expansion factor of 2 projecting the\nnumber of channels with a GLU activation layer, followed by a 1-D Depthwise convolution. The 1-D depthwise conv is followed by a\nBatchnorm and then a swish activation layer.\nment on the testother dataset with an external language model.\nWe present three models based on model parameter limit con-\nstraints of 10M , 30M and 118M. Our 10M model shows an im-\nprovement when compared to similar sized contemporary work\n[10] with 2.7%/6.3% on test/testother datasets. Our medium\n30M parameters-sized model already outperforms transformer\ntransducer published in [7] which uses 139M model parameters.\nWith the big 118M parameter model, we are able to achieve\n2.1%/4.3% without using language models and 1.9%/3.9% with\nan external language model.\nWe further carefully study the effects of the number of at-\ntention heads, convolution kernel sizes, activation functions,\nplacement of feed-forward layers, and different strategies of\nadding convolution modules to a Transformer-based network,\nand shed light on how each contributes to the accuracy improve-\nments.\n2. Conformer Encoder\nOur audio encoder \ufb01rst processes the input with a convolution\nsubsampling layer and then with a number of conformer blocks,\nas illustrated in Figure 1. The distinctive feature of our model is\nthe use of Conformer blocks in the place of Transformer blocks\nas in [7, 19].\nA conformer block is composed of four modules stacked\ntogether, i.e, a feed-forward module, a self-attention module,\na convolution module, and a second feed-forward module in\nthe end. Sections 2.1, 1, and 2.3 introduce the self-attention,\nconvolution, and feed-forward modules, respectively. Finally,\n2.4 describes how these sub blocks are combined.\n2.1. Multi-Headed Self-Attention Module\nWe employ multi-headed self-attention (MHSA) while integrat-\ning an important technique from Transformer-XL [20], the rel-\native sinusoidal positional encoding scheme. The relative po-\nsitional encoding allows the self-attention module to general-\nize better on different input length and the resulting encoder is\nmore robust to the variance of the utterance length. We use pre-\nnorm residual units [21, 22] with dropout which helps training\nand regularizing deeper models. Figure 3 below illustrates the\nmulti-headed self-attention block.\nLayernorm\nMulti-Head Attention with\nRelative Positional\nEmbedding\nDropout\n+\nFigure 3: Multi-Headed self-attention module. We use multi-\nheaded self-attention with relative positional embedding in a\npre-norm residual unit.\n2.2. Convolution Module\nInspired by [17], the convolution module starts with a gating\nmechanism [23]\u2014a pointwise convolution and a gated linear\nunit (GLU). This is followed by a single 1-D depthwise convo-\nlution layer. Batchnorm is deployed just after the convolution\nto aid training deep models. Figure 2 illustrates the convolution\nblock.\n2.3. Feed Forward Module\nThe Transformer architecture as proposed in [6] deploys a feed\nforward module after the MHSA layer and is composed of two\nlinear transformations and a nonlinear activation in between. A\nresidual connection is added over the feed-forward layers, fol-\nlowed by layer normalization. This structure is also adopted by\nTransformer ASR models [7, 24].\nWe follow pre-norm residual units [21, 22] and apply layer\nnormalization within the residual unit and on the input before\nthe \ufb01rst linear layer. We also apply Swish activation [25] and\ndropout, which helps regularizing the network. Figure 4 illus-\ntrates the Feed Forward (FFN) module.\n2.4. Conformer Block\nOur proposed Conformer block contains two Feed Forward\nmodules sandwiching the Multi-Headed Self-Attention module\nand the Convolution module, as shown in Figure 1.\nThis sandwich structure is inspired by Macaron-Net [18],\nwhich proposes replacing the original feed-forward layer in the\nTransformer block into two half-step feed-forward layers, one\nbefore the attention layer and one after. As in Macron-Net, we\nemploy half-step residual weights in our feed-forward (FFN)\nmodules. The second feed-forward module is followed by a\n\ufb01nal layernorm layer. Mathematically, this means, for input xi\nto a Conformer block i, the output yi of the block is:\n\u02dcxi = xi + 1\n2FFN(xi)\nx\u2032\ni = \u02dcxi + MHSA( \u02dcxi)\nx\u2032\u2032\ni = x\u2032\ni + Conv(x\u2032\ni)\nyi = Layernorm(x\u2032\u2032\ni + 1\n2FFN(x\u2032\u2032\ni ))\n(1)\nwhere FFN refers to the Feed forward module, MHSA refers to\nthe Multi-Head Self-Attention module, and Conv refers to the\nConvolution module as described in the preceding sections.\nOur ablation study discussed in Sec 3.4.3 compares the\nMacaron-style half-step FFNs with the vanilla FFN as used in\nprevious works. We \ufb01nd that having two Macaron-net style\nfeed-forward layers with half-step residual connections sand-\nwiching the attention and convolution modules in between pro-\nvides a signi\ufb01cant improvement over having a single feed-\nforward module in our Conformer architecture.\nThe combination of convolution and self-attention has been\nstudied before and one can imagine many ways to achieve\nLayernorm\nLinear\nLayer\nDropout\nLinear\nLayer\nSwish\nActivation\nDropout\n+\nFigure 4: Feed forward module. The \ufb01rst linear layer uses an expansion factor of 4 and the second linear layer projects it back to the\nmodel dimension. We use swish activation and a pre-norm residual units in feed forward module.\nthat. Different options of augmenting convolutions with self-\nattention are studied in Sec 3.4.2. We found that convolution\nmodule stacked after the self-attention module works best for\nspeech recognition.\n3. Experiments\n3.1. Data\nWe evaluate the proposed model on the LibriSpeech [26]\ndataset, which consists of 970 hours of labeled speech and\nan additional 800M word token text-only corpus for building\nlanguage model. We extracted 80-channel \ufb01lterbanks features\ncomputed from a 25ms window with a stride of 10ms. We use\nSpecAugment [27, 28] with mask parameter (F = 27), and ten\ntime masks with maximum time-mask ratio (pS = 0.05), where\nthe maximum-size of the time mask is set to pS times the length\nof the utterance.\n3.2. Conformer Transducer\nWe identify three models, small, medium and large, with 10M,\n30M, and 118M params, respectively, by sweeping different\ncombinations of network depth, model dimensions, number of\nattention heads and choosing the best performing one within\nmodel parameter size constraints. We use a single-LSTM-layer\ndecoder in all our models. Table 1 describes their architecture\nhyper-parameters.\nFor regularization, we apply dropout [29] in each residual\nunit of the conformer, i.e, to the output of each module, before\nit is added to the module input. We use a rate of Pdrop = 0.1.\nVariational noise [5, 30] is introduced to the model as a regu-\nlarization. A \u21132 regularization with 1e \u22126 weight is also added\nto all the trainable weights in the network. We train the models\nwith the Adam optimizer [31] with \u03b21 = 0.9, \u03b22 = 0.98 and\n\u03f5 = 10\u22129 and a transformer learning rate schedule [6], with\n10k warm-up steps and peak learning rate 0.05/\n\u221a\nd where d is\nthe model dimension in conformer encoder.\nWe use a 3-layer LSTM language model (LM) with width\n4096 trained on the LibriSpeech langauge model corpus with\nthe LibriSpeech960h transcripts added, tokenized with the 1k\nWPM built from LibriSpeech 960h. The LM has word-level\nperplexity 63.9 on the dev-set transcripts. The LM weight \u03bb\nfor shallow fusion is tuned on the dev-set via grid search. All\nmodels are implemented with Lingvo toolkit [32].\n3.3. Results on LibriSpeech\nTable 2 compares the (WER) result of our model on Lib-\nriSpeech test-clean/test-other with a few state-of-the-art mod-\nels include: ContextNet [10], Transformer transducer [7], and\nQuartzNet [9]. All our evaluation results round up to 1 digit\nafter decimal point.\nWithout a language model,\nthe performance of our\nmedium model already achieve competitive results of 2.3/5.0\non test/testother outperforming the best known Transformer,\nLSTM based model, or a similar sized convolution model. With\nthe language model added, our model achieves the lowest word\nTable 1: Model hyper-parameters for Conformer S, M, and L\nmodels, found via sweeping different combinations and choos-\ning the best performing models within the parameter limits.\nModel\nConformer\n(S)\nConformer\n(M)\nConformer\n(L)\nNum Params (M)\n10.3\n30.7\n118.8\nEncoder Layers\n16\n16\n17\nEncoder Dim\n144\n256\n512\nAttention Heads\n4\n4\n8\nConv Kernel Size\n32\n32\n32\nDecoder Layers\n1\n1\n1\nDecoder Dim\n320\n640\n640\nTable 2: Comparison of Conformer with recent published mod-\nels. Our model shows improvements consistently over various\nmodel parameter size constraints. At 10.3M parameters, our\nmodel is 0.7% better on testother when compared to contempo-\nrary work, ContextNet(S) [10]. At 30.7M model parameters our\nmodel already signi\ufb01cantly outperforms the previous published\nstate of the art results of Transformer Transducer [7] with 139M\nparameters.",
            "methodology": "",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper1.pdf",
        "sections": {
            "abstract": "Recently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs).\nTransformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies.\nOn the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother.\nWe also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1.",
            "introduction": "End-to-end automatic speech recognition (ASR) systems based\non neural networks have seen large improvements in recent\nyears. Recurrent neural networks (RNNs) have been the de-\nfacto choice for ASR [1, 2, 3, 4] as they can model the temporal\ndependencies in the audio sequences effectively [5]. Recently,\nthe Transformer architecture based on self-attention [6, 7] has\nenjoyed widespread adoption for modeling sequences due to its\nability to capture long distance interactions and the high train-\ning ef\ufb01ciency. Alternatively, convolutions have also been suc-\ncessful for ASR [8, 9, 10, 11, 12], which capture local context\nprogressively via a local receptive \ufb01eld layer by layer.\nHowever, models with self-attention or convolutions each\nhas its limitations. While Transformers are good at modeling\nlong-range global context, they are less capable to extract \ufb01ne-\ngrained local feature patterns.\nConvolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation. To combat this issue, contemporary work Con-\ntextNet [10] adopts the squeeze-and-excitation module [13] in\neach residual block to capture longer context. However, it is still\nlimited in capturing dynamic global context as it only applies a\nglobal averaging over the entire sequence.\nRecent works have shown that combining convolution and\nFeed Forward Module\nMulti-Head Self Attention\nModule\nConvolution Module\n1/2 x\n1/2 x\nFeed Forward Module\nDropout\nConformer Blocks\nLinear\nSpecAug\n10 ms rate\n10 ms rate\nConvolution\nSubsampling\n40 ms rate\nx N\n40 ms rate\n40 ms rate\n+\n+\n+\n+\nLayernorm\nFigure 1: Conformer encoder model architecture. Conformer\ncomprises of two macaron-like feed-forward layers with half-\nstep residual connections sandwiching the multi-headed self-\nattention and convolution modules. This is followed by a post\nlayernorm.\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.\n[17] proposed a multi-branch architecture with splitting the in-\nput into two branches: self-attention and convolution; and con-\ncatenating their outputs. Their work targeted mobile applica-\ntions and showed improvements in machine translation tasks.\nIn this work, we study how to organically combine con-\nvolutions with self-attention in ASR models. We hypothesize\nthat both global and local interactions are important for being\nparameter ef\ufb01cient. To achieve this, we propose a novel combi-\nnation of self-attention and convolution will achieve the best of\nboth worlds \u2013 self-attention learns the global interaction whilst\nthe convolutions ef\ufb01ciently capture the relative-offset-based lo-\ncal correlations. Inspired by Wu et al. [17, 18], we introduce\na novel combination of self-attention and convolution, sand-\nwiched between a pair feed forward modules, as illustrated in\nFig 1.\nOur proposed model, named Conformer, achieves state-of-\nthe-art results on LibriSpeech, outperforming the previous best\npublished Transformer Transducer [7] by 15% relative improve-\narXiv:2005.08100v1  [eess.AS]  16 May 2020\nLayernorm\nGlu\nActivation\nPointwise\nConv\nBatchNorm\nSwish\nActivation\n1D\nDepthwise\nConv\nPointwise\nConv\nDropout\n+\nFigure 2: Convolution module. The convolution module contains a pointwise convolution with an expansion factor of 2 projecting the\nnumber of channels with a GLU activation layer, followed by a 1-D Depthwise convolution. The 1-D depthwise conv is followed by a\nBatchnorm and then a swish activation layer.\nment on the testother dataset with an external language model.\nWe present three models based on model parameter limit con-\nstraints of 10M , 30M and 118M. Our 10M model shows an im-\nprovement when compared to similar sized contemporary work\n[10] with 2.7%/6.3% on test/testother datasets. Our medium\n30M parameters-sized model already outperforms transformer\ntransducer published in [7] which uses 139M model parameters.\nWith the big 118M parameter model, we are able to achieve\n2.1%/4.3% without using language models and 1.9%/3.9% with\nan external language model.\nWe further carefully study the effects of the number of at-\ntention heads, convolution kernel sizes, activation functions,\nplacement of feed-forward layers, and different strategies of\nadding convolution modules to a Transformer-based network,\nand shed light on how each contributes to the accuracy improve-\nments.\n2. Conformer Encoder\nOur audio encoder \ufb01rst processes the input with a convolution\nsubsampling layer and then with a number of conformer blocks,\nas illustrated in Figure 1. The distinctive feature of our model is\nthe use of Conformer blocks in the place of Transformer blocks\nas in [7, 19].\nA conformer block is composed of four modules stacked\ntogether, i.e, a feed-forward module, a self-attention module,\na convolution module, and a second feed-forward module in\nthe end. Sections 2.1, 1, and 2.3 introduce the self-attention,\nconvolution, and feed-forward modules, respectively. Finally,\n2.4 describes how these sub blocks are combined.\n2.1. Multi-Headed Self-Attention Module\nWe employ multi-headed self-attention (MHSA) while integrat-\ning an important technique from Transformer-XL [20], the rel-\native sinusoidal positional encoding scheme. The relative po-\nsitional encoding allows the self-attention module to general-\nize better on different input length and the resulting encoder is\nmore robust to the variance of the utterance length. We use pre-\nnorm residual units [21, 22] with dropout which helps training\nand regularizing deeper models. Figure 3 below illustrates the\nmulti-headed self-attention block.\nLayernorm\nMulti-Head Attention with\nRelative Positional\nEmbedding\nDropout\n+\nFigure 3: Multi-Headed self-attention module. We use multi-\nheaded self-attention with relative positional embedding in a\npre-norm residual unit.\n2.2. Convolution Module\nInspired by [17], the convolution module starts with a gating\nmechanism [23]\u2014a pointwise convolution and a gated linear\nunit (GLU). This is followed by a single 1-D depthwise convo-\nlution layer. Batchnorm is deployed just after the convolution\nto aid training deep models. Figure 2 illustrates the convolution\nblock.\n2.3. Feed Forward Module\nThe Transformer architecture as proposed in [6] deploys a feed\nforward module after the MHSA layer and is composed of two\nlinear transformations and a nonlinear activation in between. A\nresidual connection is added over the feed-forward layers, fol-\nlowed by layer normalization. This structure is also adopted by\nTransformer ASR models [7, 24].\nWe follow pre-norm residual units [21, 22] and apply layer\nnormalization within the residual unit and on the input before\nthe \ufb01rst linear layer. We also apply Swish activation [25] and\ndropout, which helps regularizing the network. Figure 4 illus-\ntrates the Feed Forward (FFN) module.\n2.4. Conformer Block\nOur proposed Conformer block contains two Feed Forward\nmodules sandwiching the Multi-Headed Self-Attention module\nand the Convolution module, as shown in Figure 1.\nThis sandwich structure is inspired by Macaron-Net [18],\nwhich proposes replacing the original feed-forward layer in the\nTransformer block into two half-step feed-forward layers, one\nbefore the attention layer and one after. As in Macron-Net, we\nemploy half-step residual weights in our feed-forward (FFN)\nmodules. The second feed-forward module is followed by a\n\ufb01nal layernorm layer. Mathematically, this means, for input xi\nto a Conformer block i, the output yi of the block is:\n\u02dcxi = xi + 1\n2FFN(xi)\nx\u2032\ni = \u02dcxi + MHSA( \u02dcxi)\nx\u2032\u2032\ni = x\u2032\ni + Conv(x\u2032\ni)\nyi = Layernorm(x\u2032\u2032\ni + 1\n2FFN(x\u2032\u2032\ni ))\n(1)\nwhere FFN refers to the Feed forward module, MHSA refers to\nthe Multi-Head Self-Attention module, and Conv refers to the\nConvolution module as described in the preceding sections.\nOur ablation study discussed in Sec 3.4.3 compares the\nMacaron-style half-step FFNs with the vanilla FFN as used in\nprevious works. We \ufb01nd that having two Macaron-net style\nfeed-forward layers with half-step residual connections sand-\nwiching the attention and convolution modules in between pro-\nvides a signi\ufb01cant improvement over having a single feed-\nforward module in our Conformer architecture.\nThe combination of convolution and self-attention has been\nstudied before and one can imagine many ways to achieve\nLayernorm\nLinear\nLayer\nDropout\nLinear\nLayer\nSwish\nActivation\nDropout\n+\nFigure 4: Feed forward module. The \ufb01rst linear layer uses an expansion factor of 4 and the second linear layer projects it back to the\nmodel dimension. We use swish activation and a pre-norm residual units in feed forward module.\nthat. Different options of augmenting convolutions with self-\nattention are studied in Sec 3.4.2. We found that convolution\nmodule stacked after the self-attention module works best for\nspeech recognition.\n3. Experiments\n3.1. Data\nWe evaluate the proposed model on the LibriSpeech [26]\ndataset, which consists of 970 hours of labeled speech and\nan additional 800M word token text-only corpus for building\nlanguage model. We extracted 80-channel \ufb01lterbanks features\ncomputed from a 25ms window with a stride of 10ms. We use\nSpecAugment [27, 28] with mask parameter (F = 27), and ten\ntime masks with maximum time-mask ratio (pS = 0.05), where\nthe maximum-size of the time mask is set to pS times the length\nof the utterance.\n3.2. Conformer Transducer\nWe identify three models, small, medium and large, with 10M,\n30M, and 118M params, respectively, by sweeping different\ncombinations of network depth, model dimensions, number of\nattention heads and choosing the best performing one within\nmodel parameter size constraints. We use a single-LSTM-layer\ndecoder in all our models. Table 1 describes their architecture\nhyper-parameters.\nFor regularization, we apply dropout [29] in each residual\nunit of the conformer, i.e, to the output of each module, before\nit is added to the module input. We use a rate of Pdrop = 0.1.\nVariational noise [5, 30] is introduced to the model as a regu-\nlarization. A \u21132 regularization with 1e \u22126 weight is also added\nto all the trainable weights in the network. We train the models\nwith the Adam optimizer [31] with \u03b21 = 0.9, \u03b22 = 0.98 and\n\u03f5 = 10\u22129 and a transformer learning rate schedule [6], with\n10k warm-up steps and peak learning rate 0.05/\n\u221a\nd where d is\nthe model dimension in conformer encoder.\nWe use a 3-layer LSTM language model (LM) with width\n4096 trained on the LibriSpeech langauge model corpus with\nthe LibriSpeech960h transcripts added, tokenized with the 1k\nWPM built from LibriSpeech 960h. The LM has word-level\nperplexity 63.9 on the dev-set transcripts. The LM weight \u03bb\nfor shallow fusion is tuned on the dev-set via grid search. All\nmodels are implemented with Lingvo toolkit [32].\n3.3. Results on LibriSpeech\nTable 2 compares the (WER) result of our model on Lib-\nriSpeech test-clean/test-other with a few state-of-the-art mod-\nels include: ContextNet [10], Transformer transducer [7], and\nQuartzNet [9]. All our evaluation results round up to 1 digit\nafter decimal point.\nWithout a language model,\nthe performance of our\nmedium model already achieve competitive results of 2.3/5.0\non test/testother outperforming the best known Transformer,\nLSTM based model, or a similar sized convolution model. With\nthe language model added, our model achieves the lowest word\nTable 1: Model hyper-parameters for Conformer S, M, and L\nmodels, found via sweeping different combinations and choos-\ning the best performing models within the parameter limits.\nModel\nConformer\n(S)\nConformer\n(M)\nConformer\n(L)\nNum Params (M)\n10.3\n30.7\n118.8\nEncoder Layers\n16\n16\n17\nEncoder Dim\n144\n256\n512\nAttention Heads\n4\n4\n8\nConv Kernel Size\n32\n32\n32\nDecoder Layers\n1\n1\n1\nDecoder Dim\n320\n640\n640\nTable 2: Comparison of Conformer with recent published mod-\nels. Our model shows improvements consistently over various\nmodel parameter size constraints. At 10.3M parameters, our\nmodel is 0.7% better on testother when compared to contempo-\nrary work, ContextNet(S) [10]. At 30.7M model parameters our\nmodel already signi\ufb01cantly outperforms the previous published\nstate of the art results of Transformer Transducer [7] with 139M\nparameters.",
            "methodology": "#Params (M)\nWER Without LM\nWER With LM\ntestclean\ntestother\ntestclean\ntestother\nHybrid\nTransformer [33]\n-\n-\n-\n2.26\n4.85\nCTC\nQuartzNet [9]\n19\n3.90\n11.28\n2.69\n7.25\nLAS\nTransformer [34]\n270\n2.89\n6.98\n2.33\n5.17\nTransformer [19]\n-\n2.2\n5.6\n2.6\n5.7\nLSTM\n360\n2.6\n6.0\n2.2\n5.2\nTransducer\nTransformer [7]\n139\n2.4\n5.6\n2.0\n4.6\nContextNet(S) [10]\n10.8\n2.9\n7.0\n2.3\n5.5\nContextNet(M) [10]\n31.4\n2.4\n5.4\n2.0\n4.5\nContextNet(L) [10]\n112.7\n2.1\n4.6\n1.9\n4.1\nConformer (Ours)\nConformer(S)\n10.3\n2.7\n6.3\n2.1\n5.0\nConformer(M)\n30.7\n2.3\n5.0\n2.0\n4.3\nConformer(L)\n118.8\n2.1\n4.3\n1.9\n3.9\nerror rate among all the existing models. This clearly demon-\nstrates the effectiveness of combining Transformer and convo-\nlution in a single neural network.\n3.4. Ablation Studies\n3.4.1. Conformer Block vs. Transformer Block\nA Conformer block differs from a Transformer block in a\nnumber of ways, in particular, the inclusion of a convolution\nblock and having a pair of FFNs surrounding the block in the\nMacaron-style. Below we study these effects of these differ-\nences by mutating a Conformer block towards a Transformer\nblock, while keeping the total number of parameters unchanged.\nTable 3 shows the impact of each change to the Conformer\nblock. Among all differences, convolution sub-block is the most\nimportant feature, while having a Macaron-style FFN pair is\nalso more effective than a single FFN of the same number of\nparameters. Using swish activations led to faster convergence\nin the Conformer models.\nTable 3: Disentangling Conformer. Starting from a Conformer\nblock, we remove its features and move towards a vanilla Trans-\nformer block: (1) replacing SWISH with ReLU; (2) remov-\ning the convolution sub-block; (3) replacing the Macaron-style\nFFN pairs with a single FFN; (4) replacing self-attention with\nrelative positional embedding [20] with a vanilla self-attention\nlayer [6]. All ablation study",
            "conclusion": "In this work, we introduced Conformer, an architecture that\nintegrates components from CNNs and Transformers for end-\nto-end speech recognition. We studied the importance of each\ncomponent, and demonstrated that the inclusion of convolution\nmodules is critical to the performance of the Conformer model.\nThe model exhibits better accuracy with fewer parameters than\nprevious work on the LibriSpeech dataset, and achieves a new\nstate-of-the-art performance at 1.9%/3.9% for test/testother."
        }
    },
    {
        "pdf_file": "paper1.pdf",
        "sections": {
            "abstract": "Recently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs).\nTransformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies.\nOn the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother.\nWe also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1.",
            "introduction": "End-to-end automatic speech recognition (ASR) systems based\non neural networks have seen large improvements in recent\nyears. Recurrent neural networks (RNNs) have been the de-\nfacto choice for ASR [1, 2, 3, 4] as they can model the temporal\ndependencies in the audio sequences effectively [5]. Recently,\nthe Transformer architecture based on self-attention [6, 7] has\nenjoyed widespread adoption for modeling sequences due to its\nability to capture long distance interactions and the high train-\ning ef\ufb01ciency. Alternatively, convolutions have also been suc-\ncessful for ASR [8, 9, 10, 11, 12], which capture local context\nprogressively via a local receptive \ufb01eld layer by layer.\nHowever, models with self-attention or convolutions each\nhas its limitations. While Transformers are good at modeling\nlong-range global context, they are less capable to extract \ufb01ne-\ngrained local feature patterns.\nConvolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation. To combat this issue, contemporary work Con-\ntextNet [10] adopts the squeeze-and-excitation module [13] in\neach residual block to capture longer context. However, it is still\nlimited in capturing dynamic global context as it only applies a\nglobal averaging over the entire sequence.\nRecent works have shown that combining convolution and\nFeed Forward Module\nMulti-Head Self Attention\nModule\nConvolution Module\n1/2 x\n1/2 x\nFeed Forward Module\nDropout\nConformer Blocks\nLinear\nSpecAug\n10 ms rate\n10 ms rate\nConvolution\nSubsampling\n40 ms rate\nx N\n40 ms rate\n40 ms rate\n+\n+\n+\n+\nLayernorm\nFigure 1: Conformer encoder model architecture. Conformer\ncomprises of two macaron-like feed-forward layers with half-\nstep residual connections sandwiching the multi-headed self-\nattention and convolution modules. This is followed by a post\nlayernorm.\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.\n[17] proposed a multi-branch architecture with splitting the in-\nput into two branches: self-attention and convolution; and con-\ncatenating their outputs. Their work targeted mobile applica-\ntions and showed improvements in machine translation tasks.\nIn this work, we study how to organically combine con-\nvolutions with self-attention in ASR models. We hypothesize\nthat both global and local interactions are important for being\nparameter ef\ufb01cient. To achieve this, we propose a novel combi-\nnation of self-attention and convolution will achieve the best of\nboth worlds \u2013 self-attention learns the global interaction whilst\nthe convolutions ef\ufb01ciently capture the relative-offset-based lo-\ncal correlations. Inspired by Wu et al. [17, 18], we introduce\na novel combination of self-attention and convolution, sand-\nwiched between a pair feed forward modules, as illustrated in\nFig 1.\nOur proposed model, named Conformer, achieves state-of-\nthe-art results on LibriSpeech, outperforming the previous best\npublished Transformer Transducer [7] by 15% relative improve-\narXiv:2005.08100v1  [eess.AS]  16 May 2020\nLayernorm\nGlu\nActivation\nPointwise\nConv\nBatchNorm\nSwish\nActivation\n1D\nDepthwise\nConv\nPointwise\nConv\nDropout\n+\nFigure 2: Convolution module. The convolution module contains a pointwise convolution with an expansion factor of 2 projecting the\nnumber of channels with a GLU activation layer, followed by a 1-D Depthwise convolution. The 1-D depthwise conv is followed by a\nBatchnorm and then a swish activation layer.\nment on the testother dataset with an external language model.\nWe present three models based on model parameter limit con-\nstraints of 10M , 30M and 118M. Our 10M model shows an im-\nprovement when compared to similar sized contemporary work\n[10] with 2.7%/6.3% on test/testother datasets. Our medium\n30M parameters-sized model already outperforms transformer\ntransducer published in [7] which uses 139M model parameters.\nWith the big 118M parameter model, we are able to achieve\n2.1%/4.3% without using language models and 1.9%/3.9% with\nan external language model.\nWe further carefully study the effects of the number of at-\ntention heads, convolution kernel sizes, activation functions,\nplacement of feed-forward layers, and different strategies of\nadding convolution modules to a Transformer-based network,\nand shed light on how each contributes to the accuracy improve-\nments.\n2. Conformer Encoder\nOur audio encoder \ufb01rst processes the input with a convolution\nsubsampling layer and then with a number of conformer blocks,\nas illustrated in Figure 1. The distinctive feature of our model is\nthe use of Conformer blocks in the place of Transformer blocks\nas in [7, 19].\nA conformer block is composed of four modules stacked\ntogether, i.e, a feed-forward module, a self-attention module,\na convolution module, and a second feed-forward module in\nthe end. Sections 2.1, 1, and 2.3 introduce the self-attention,\nconvolution, and feed-forward modules, respectively. Finally,\n2.4 describes how these sub blocks are combined.\n2.1. Multi-Headed Self-Attention Module\nWe employ multi-headed self-attention (MHSA) while integrat-\ning an important technique from Transformer-XL [20], the rel-\native sinusoidal positional encoding scheme. The relative po-\nsitional encoding allows the self-attention module to general-\nize better on different input length and the resulting encoder is\nmore robust to the variance of the utterance length. We use pre-\nnorm residual units [21, 22] with dropout which helps training\nand regularizing deeper models. Figure 3 below illustrates the\nmulti-headed self-attention block.\nLayernorm\nMulti-Head Attention with\nRelative Positional\nEmbedding\nDropout\n+\nFigure 3: Multi-Headed self-attention module. We use multi-\nheaded self-attention with relative positional embedding in a\npre-norm residual unit.\n2.2. Convolution Module\nInspired by [17], the convolution module starts with a gating\nmechanism [23]\u2014a pointwise convolution and a gated linear\nunit (GLU). This is followed by a single 1-D depthwise convo-\nlution layer. Batchnorm is deployed just after the convolution\nto aid training deep models. Figure 2 illustrates the convolution\nblock.\n2.3. Feed Forward Module\nThe Transformer architecture as proposed in [6] deploys a feed\nforward module after the MHSA layer and is composed of two\nlinear transformations and a nonlinear activation in between. A\nresidual connection is added over the feed-forward layers, fol-\nlowed by layer normalization. This structure is also adopted by\nTransformer ASR models [7, 24].\nWe follow pre-norm residual units [21, 22] and apply layer\nnormalization within the residual unit and on the input before\nthe \ufb01rst linear layer. We also apply Swish activation [25] and\ndropout, which helps regularizing the network. Figure 4 illus-\ntrates the Feed Forward (FFN) module.\n2.4. Conformer Block\nOur proposed Conformer block contains two Feed Forward\nmodules sandwiching the Multi-Headed Self-Attention module\nand the Convolution module, as shown in Figure 1.\nThis sandwich structure is inspired by Macaron-Net [18],\nwhich proposes replacing the original feed-forward layer in the\nTransformer block into two half-step feed-forward layers, one\nbefore the attention layer and one after. As in Macron-Net, we\nemploy half-step residual weights in our feed-forward (FFN)\nmodules. The second feed-forward module is followed by a\n\ufb01nal layernorm layer. Mathematically, this means, for input xi\nto a Conformer block i, the output yi of the block is:\n\u02dcxi = xi + 1\n2FFN(xi)\nx\u2032\ni = \u02dcxi + MHSA( \u02dcxi)\nx\u2032\u2032\ni = x\u2032\ni + Conv(x\u2032\ni)\nyi = Layernorm(x\u2032\u2032\ni + 1\n2FFN(x\u2032\u2032\ni ))\n(1)\nwhere FFN refers to the Feed forward module, MHSA refers to\nthe Multi-Head Self-Attention module, and Conv refers to the\nConvolution module as described in the preceding sections.\nOur ablation study discussed in Sec 3.4.3 compares the\nMacaron-style half-step FFNs with the vanilla FFN as used in\nprevious works. We \ufb01nd that having two Macaron-net style\nfeed-forward layers with half-step residual connections sand-\nwiching the attention and convolution modules in between pro-\nvides a signi\ufb01cant improvement over having a single feed-\nforward module in our Conformer architecture.\nThe combination of convolution and self-attention has been\nstudied before and one can imagine many ways to achieve\nLayernorm\nLinear\nLayer\nDropout\nLinear\nLayer\nSwish\nActivation\nDropout\n+\nFigure 4: Feed forward module. The \ufb01rst linear layer uses an expansion factor of 4 and the second linear layer projects it back to the\nmodel dimension. We use swish activation and a pre-norm residual units in feed forward module.\nthat. Different options of augmenting convolutions with self-\nattention are studied in Sec 3.4.2. We found that convolution\nmodule stacked after the self-attention module works best for\nspeech recognition.\n3. Experiments\n3.1. Data\nWe evaluate the proposed model on the LibriSpeech [26]\ndataset, which consists of 970 hours of labeled speech and\nan additional 800M word token text-only corpus for building\nlanguage model. We extracted 80-channel \ufb01lterbanks features\ncomputed from a 25ms window with a stride of 10ms. We use\nSpecAugment [27, 28] with mask parameter (F = 27), and ten\ntime masks with maximum time-mask ratio (pS = 0.05), where\nthe maximum-size of the time mask is set to pS times the length\nof the utterance.\n3.2. Conformer Transducer\nWe identify three models, small, medium and large, with 10M,\n30M, and 118M params, respectively, by sweeping different\ncombinations of network depth, model dimensions, number of\nattention heads and choosing the best performing one within\nmodel parameter size constraints. We use a single-LSTM-layer\ndecoder in all our models. Table 1 describes their architecture\nhyper-parameters.\nFor regularization, we apply dropout [29] in each residual\nunit of the conformer, i.e, to the output of each module, before\nit is added to the module input. We use a rate of Pdrop = 0.1.\nVariational noise [5, 30] is introduced to the model as a regu-\nlarization. A \u21132 regularization with 1e \u22126 weight is also added\nto all the trainable weights in the network. We train the models\nwith the Adam optimizer [31] with \u03b21 = 0.9, \u03b22 = 0.98 and\n\u03f5 = 10\u22129 and a transformer learning rate schedule [6], with\n10k warm-up steps and peak learning rate 0.05/\n\u221a\nd where d is\nthe model dimension in conformer encoder.\nWe use a 3-layer LSTM language model (LM) with width\n4096 trained on the LibriSpeech langauge model corpus with\nthe LibriSpeech960h transcripts added, tokenized with the 1k\nWPM built from LibriSpeech 960h. The LM has word-level\nperplexity 63.9 on the dev-set transcripts. The LM weight \u03bb\nfor shallow fusion is tuned on the dev-set via grid search. All\nmodels are implemented with Lingvo toolkit [32].\n3.3. Results on LibriSpeech\nTable 2 compares the (WER) result of our model on Lib-\nriSpeech test-clean/test-other with a few state-of-the-art mod-\nels include: ContextNet [10], Transformer transducer [7], and\nQuartzNet [9]. All our evaluation results round up to 1 digit\nafter decimal point.\nWithout a language model,\nthe performance of our\nmedium model already achieve competitive results of 2.3/5.0\non test/testother outperforming the best known Transformer,\nLSTM based model, or a similar sized convolution model. With\nthe language model added, our model achieves the lowest word\nTable 1: Model hyper-parameters for Conformer S, M, and L\nmodels, found via sweeping different combinations and choos-\ning the best performing models within the parameter limits.\nModel\nConformer\n(S)\nConformer\n(M)\nConformer\n(L)\nNum Params (M)\n10.3\n30.7\n118.8\nEncoder Layers\n16\n16\n17\nEncoder Dim\n144\n256\n512\nAttention Heads\n4\n4\n8\nConv Kernel Size\n32\n32\n32\nDecoder Layers\n1\n1\n1\nDecoder Dim\n320\n640\n640\nTable 2: Comparison of Conformer with recent published mod-\nels. Our model shows improvements consistently over various\nmodel parameter size constraints. At 10.3M parameters, our\nmodel is 0.7% better on testother when compared to contempo-\nrary work, ContextNet(S) [10]. At 30.7M model parameters our\nmodel already signi\ufb01cantly outperforms the previous published\nstate of the art results of Transformer Transducer [7] with 139M\nparameters.",
            "methodology": "#Params (M)\nWER Without LM\nWER With LM\ntestclean\ntestother\ntestclean\ntestother\nHybrid\nTransformer [33]\n-\n-\n-\n2.26\n4.85\nCTC\nQuartzNet [9]\n19\n3.90\n11.28\n2.69\n7.25\nLAS\nTransformer [34]\n270\n2.89\n6.98\n2.33\n5.17\nTransformer [19]\n-\n2.2\n5.6\n2.6\n5.7\nLSTM\n360\n2.6\n6.0\n2.2\n5.2\nTransducer\nTransformer [7]\n139\n2.4\n5.6\n2.0\n4.6\nContextNet(S) [10]\n10.8\n2.9\n7.0\n2.3\n5.5\nContextNet(M) [10]\n31.4\n2.4\n5.4\n2.0\n4.5\nContextNet(L) [10]\n112.7\n2.1\n4.6\n1.9\n4.1\nConformer (Ours)\nConformer(S)\n10.3\n2.7\n6.3\n2.1\n5.0\nConformer(M)\n30.7\n2.3\n5.0\n2.0\n4.3\nConformer(L)\n118.8\n2.1\n4.3\n1.9\n3.9\nerror rate among all the existing models. This clearly demon-\nstrates the effectiveness of combining Transformer and convo-\nlution in a single neural network.\n3.4. Ablation Studies\n3.4.1. Conformer Block vs. Transformer Block\nA Conformer block differs from a Transformer block in a\nnumber of ways, in particular, the inclusion of a convolution\nblock and having a pair of FFNs surrounding the block in the\nMacaron-style. Below we study these effects of these differ-\nences by mutating a Conformer block towards a Transformer\nblock, while keeping the total number of parameters unchanged.\nTable 3 shows the impact of each change to the Conformer\nblock. Among all differences, convolution sub-block is the most\nimportant feature, while having a Macaron-style FFN pair is\nalso more effective than a single FFN of the same number of\nparameters. Using swish activations led to faster convergence\nin the Conformer models.\nTable 3: Disentangling Conformer. Starting from a Conformer\nblock, we remove its features and move towards a vanilla Trans-\nformer block: (1) replacing SWISH with ReLU; (2) remov-\ning the convolution sub-block; (3) replacing the Macaron-style\nFFN pairs with a single FFN; (4) replacing self-attention with\nrelative positional embedding [20] with a vanilla self-attention\nlayer [6]. All ablation study",
            "conclusion": "In this work, we introduced Conformer, an architecture that\nintegrates components from CNNs and Transformers for end-\nto-end speech recognition. We studied the importance of each\ncomponent, and demonstrated that the inclusion of convolution\nmodules is critical to the performance of the Conformer model.\nThe model exhibits better accuracy with fewer parameters than\nprevious work on the LibriSpeech dataset, and achieves a new\nstate-of-the-art performance at 1.9%/3.9% for test/testother.\n5. References\n[1] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al., \u201cState-\nof-the-art speech recognition with sequence-to-sequence models,\u201d\nin 2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2018, pp. 4774\u20134778.\n[2] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures,\ndata and units for streaming end-to-end speech recognition with\nrnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU).\nIEEE, 2017, pp. 193\u2013199.\n[3] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez,\nD. Zhao, D. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang,\nD. Bhatia, Y. Shangguan, B. Li, G. Pundak, K. C. Sim, T. Bagby,\nS.-Y. Chang, K. Rao, and A. Gruenstein, \u201cStreaming End-to-end\nSpeech Recognition For Mobile Devices,\u201d in Proc. ICASSP, 2019.\n[4] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen, and et al., \u201cA streaming\non-device end-to-end model surpassing server-side conventional\nmodel quality and latency,\u201d in ICASSP, 2020.\n[5] A. Graves, \u201cSequence transduction with recurrent neural net-\nworks,\u201d arXiv preprint arXiv:1211.3711, 2012.\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\n2017.\n[7] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, \u201cTransformer transducer: A streamable speech recog-\nnition model with transformer encoders and rnn-t loss,\u201d in ICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nIEEE, 2020, pp. 7829\u20137833.\n[8] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Co-\nhen, H. Nguyen, and R. T. Gadde, \u201cJasper: An end-to-end convo-\nlutional neural acoustic model,\u201d arXiv preprint arXiv:1904.03288,\n2019.\n[9] S. Kriman, S. Beliaev, B. Ginsburg, J. Huang, O. Kuchaiev,\nV. Lavrukhin, R. Leary, J. Li, and Y. Zhang, \u201cQuartznet: Deep\nautomatic speech recognition with 1d time-channel separable con-\nvolutions,\u201d arXiv preprint arXiv:1910.10261, 2019.\n[10] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati,\nR. Pang, and Y. Wu, \u201cContextnet: Improving convolutional neural\nnetworks for automatic speech recognition with global context,\u201d\narXiv preprint arXiv:2005.03191, 2020.\n[11] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhad-\nran, \u201cDeep convolutional neural networks for lvcsr,\u201d in 2013 IEEE\ninternational conference on acoustics, speech and signal process-\ning.\nIEEE, 2013, pp. 8614\u20138618.\n[12] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn,\nand D. Yu, \u201cConvolutional neural networks for speech recogni-\ntion,\u201d IEEE/ACM Transactions on audio, speech, and language\nprocessing, vol. 22, no. 10, pp. 1533\u20131545, 2014.\n[13] J. Hu, L. Shen, and G. Sun, \u201cSqueeze-and-excitation networks,\u201d\nin Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7132\u20137141.\n[14] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, \u201cAttention\naugmented convolutional networks,\u201d in Proceedings of the IEEE\nInternational Conference on Computer Vision, 2019, pp. 3286\u2013\n3295.\n[15] B. Yang, L. Wang, D. Wong, L. S. Chao, and Z. Tu, \u201cConvolu-\ntional self-attention networks,\u201d arXiv preprint arXiv:1904.03107,\n2019.\n[16] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi,\nand Q. V. Le, \u201cQanet:\nCombining local convolution with\nglobal self-attention for reading comprehension,\u201d arXiv preprint\narXiv:1804.09541, 2018.\n[17] Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, \u201cLite transformer with\nlong-short range attention,\u201d arXiv preprint arXiv:2004.11886,\n2020.\n[18] Y. Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and\nT.-Y. Liu, \u201cUnderstanding and improving transformer from a\nmulti-particle dynamic system point of view,\u201d arXiv preprint\narXiv:1906.02762, 2019.\n[19] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., \u201cA\ncomparative study on transformer vs rnn in speech applications,\u201d\narXiv preprint arXiv:1909.06317, 2019.\n[20] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhut-\ndinov, \u201cTransformer-xl:\nAttentive language models beyond a\n\ufb01xed-length context,\u201d 2019.\n[21] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao,\n\u201cLearning deep transformer models for machine translation,\u201d in\nProceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics.\nAssociation for Computational Lin-\nguistics, Jul. 2019, pp. 1810\u20131822.\n[22] T. Q. Nguyen and J. Salazar, \u201cTransformers without tears:\nImproving the normalization of self-attention,\u201d arXiv preprint\narXiv:1910.05895, 2019.\n[23] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, \u201cLanguage\nmodeling with gated convolutional networks,\u201d in Proceedings of\nthe 34th International Conference on Machine Learning-Volume\n70.\nJMLR. org, 2017, pp. 933\u2013941.\n[24] L. Dong, S. Xu, and B. Xu, \u201cSpeech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,\u201d in 2018\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2018, pp. 5884\u20135888.\n[25] P. Ramachandran, B. Zoph, and Q. V. Le, \u201cSearching for activa-\ntion functions,\u201d arXiv preprint arXiv:1710.05941, 2017.\n[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech: an asr corpus based on public domain audio books,\u201d\nin 2015 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2015, pp. 5206\u20135210.\n[27] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V. Le, \u201cSpecaugment: A simple data augmen-\ntation method for automatic speech recognition,\u201d arXiv preprint\narXiv:1904.08779, 2019.\n[28] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V.\nLe, and Y. Wu, \u201cSpecaugment on large scale datasets,\u201d arXiv\npreprint arXiv:1912.05533, 2019.\n[29] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, \u201cDropout: A simple way to prevent neural net-\nworks from over\ufb01tting,\u201d Journal of Machine Learning Research,\nvol. 15, no. 56, pp. 1929\u20131958, 2014.\n[30] K.-C. Jim, C. L. Giles, and B. G. Horne, \u201cAn analysis of noise\nin recurrent neural networks: convergence and generalization,\u201d\nIEEE Transactions on neural networks, vol. 7, no. 6, pp. 1424\u2013\n1438, 1996.\n[31] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic opti-\nmization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[32] J. Shen, P. Nguyen, Y. Wu, Z. Chen, and et al., \u201cLingvo: a modu-\nlar and scalable framework for sequence-to-sequence modeling,\u201d\n2019.\n[33] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang et al., \u201cTransformer-\nbased acoustic modeling for hybrid speech recognition,\u201d arXiv\npreprint arXiv:1910.09799, 2019.\n[34] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave,\nV. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert, \u201cEnd-to-\nend asr: from supervised to semi-supervised learning with modern\narchitectures,\u201d 2019.\n[35] F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli, \u201cPay\nless attention with lightweight and dynamic convolutions,\u201d arXiv\npreprint arXiv:1901.10430, 2019."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1.",
            "introduction": "Deep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a",
            "methodology": "for Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1.",
            "introduction": "Deep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a",
            "methodology": "for Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1.",
            "introduction": "Deep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a",
            "methodology": "for Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1.",
            "introduction": "Deep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a",
            "methodology": "for Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art",
            "conclusion": "s\nSpecAugment greatly improves the performance of ASR net-\nworks. We are able to obtain state-of-the-art results on the Lib-\nriSpeech 960h and Switchboard 300h tasks on end-to-end LAS"
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1.",
            "introduction": "Deep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a",
            "methodology": "for Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art",
            "conclusion": "s\nSpecAugment greatly improves the performance of ASR net-\nworks. We are able to obtain state-of-the-art results on the Lib-\nriSpeech 960h and Switchboard 300h tasks on end-to-end LAS\nnetworks by augmenting the training set using simple hand-\ncrafted policies, surpassing the performance of hybrid systems\neven without the aid of a language model. SpecAugment con-\nverts ASR from an over-\ufb01tting to an under-\ufb01tting problem, and\nwe were able to gain performance by using bigger networks and\ntraining longer.\nAcknowledgements:\nWe would like to thank Yuan Cao,\nCiprian Chelba, Kazuki Irie, Ye Jia, Anjuli Kannan, Patrick\nNguyen, Vijay Peddinti, Rohit Prabhavalkar, Yonghui Wu and\nShuyuan Zhang for useful discussions. We also thank Gy\u00a8orgy\nKov\u00b4acs for introducing us to the works [49, 50].\n7. References\n[1] G. Hinton, L. Deng, D. Yu, G. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V. Vanhoucke, P. Nguyen, B. Kingsbury et al., \u201cDeep\nneural networks for acoustic modeling in speech recognition,\u201d\nIEEE Signal processing magazine, vol. 29, 2012.\n[2] G. Dahl, D. Yu, L. Deng, and A. Acero, \u201cContext-Dependent\nPre-Trained Deep Neural Networks for Large-Vocabulary Speech\nRecognition,\u201d IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, vol. 20, Jan 2012.\n[3] T. Sainath, A. rahman Mohamed, B. Kingsbury, and B. Ramab-\nhadran, \u201cDeep Convolutional Neural Networks for LVCSR,\u201d in\nICASSP, 2013.\n[4] A. Graves, A. rahman Mohamed, and G. Hinton, \u201cSpeech Recog-\nnition with Deep Recurrent Neural Networks,\u201d in ICASSP, 2013.\n[5] A. Graves and N. Jaitly, \u201cTowards End-to-End Speech Recogni-\ntion with Recurrent Neural Networks,\u201d in ICML, 2014.\n[6] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, \u201cListen, Attend and\nSpell: A Neural Network for Large Vocabulary Conversational\nSpeech Recognition,\u201d in ICASSP, 2016.\n[7] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,\n\u201cEnd-to-End Attention-based Large Vocabulary Speech Recogni-\ntion,\u201d in ICASSP, 2016.\n[8] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, N. Jaitly,\nB. Li, J. Chorowski, and M. Bacchiani, \u201cState-of-the-art Speech\nRecognition With Sequence-to-Sequence Models,\u201d in ICASSP,\n2018.\n[9] N. Kanda, R. Takeda, and Y. Obuchi, \u201cElastic spectral distortion\nfor low resource speech recognition with deep neural networks,\u201d\nin ASRU, 2013.\n[10] A. Ragni, K. M. Knill, S. P. Rath, and M. J. F. Gales, \u201cData aug-\nmentation for low resource languages,\u201d in INTERSPEECH, 2014.\n[11] N. Jaitly and G. Hinton, \u201cVocal Tract Length Perturbation (VTLP)\nimproves speech recognition,\u201d in ICML Workshop on Deep Learn-\ning for Audio, Speech and Language Processing, 2013.\n[12] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos,\nE. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and\nA. Ng, \u201cDeep Speech: Scaling up end-to-end speech recognition,\u201d\nin arXiv, 2014.\n[13] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, \u201cAudio Augmen-\ntation for Speech Recognition,\u201d in INTERSPEECH, 2015.\n[14] C. Kim, A. Misra, K. Chin, T. Hughes, A. Narayanan, T. Sainath,\nand M. Bacchiani, \u201cGeneration of large-scale simulated utterances\nin virtual rooms to train deep-neural networks for far-\ufb01eld speech\nrecognition in Google Home,\u201d in INTERSPEECH, 2017.\n[15] R. Prabhavalkar, R. Alvarez, C. Parada, P. Nakkiran, and T. N.\nSainath, \u201cAutomatic gain control and multi-style training for ro-\nbust small-footprint keyword spotting with deep neural networks,\u201d\nin ICASSP, 2015.\n[16] A. Raju, S. Panchapagesan, X. Liu, A. Mandal, and N. Strom,\n\u201cData Augmentation for Robust Keyword Spotting under Play-\nback Interference,\u201d in arXiv, 2018.\n[17] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based\nfusion for Multistream ASR,\u201d in ICASSP, 2016.\n[18] E. D. Cubuk, B. Zoph, D. Man\u00b4e, V. Vasudevan, and Q. V. Le, \u201cAu-\ntoaugment: Learning augmentation policies from data,\u201d in CVPR,\n2019.\n[19] T. DeVries and G. Taylor, \u201cImproved Regularization of Convolu-\ntional Neural Networks with Cutout,\u201d in arXiv, 2017.\n[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech: An ASR corpus based on public domain audio books,\u201d\nin ICASSP, 2015.\n[21] C\u00b8 . G\u00a8ulc\u00b8ehre, O. Firat, K. Xu, K. Cho, L. Barrault, H. Lin,\nF. Bougares, H. Schwenk, and Y. Bengio, \u201cOn using monolingual\ncorpora in neural machine translation,\u201d in arxiv, 2015.\n[22] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSWITCHBOARD:\ntelephone speech corpus for research and development,\u201d in\nICASSP, 1992.\n[23] C. Cieri, D. Miller, and K. Walker, \u201cThe \ufb01sher corpus: a resource\nfor the next generations of speech-to-text,\u201d in LREC, 2004.\n[24] A. Zeyer, K. Irie, R. Schl\u00a8uter, and H. Ney, \u201cImproved training of\nend-to-end attention models for speech recognition,\u201d in INTER-\nSPEECH, 2018.\n[25] K. Irie, R. Prabhavalkar, A. Kannan, A. Bruguier, D. Rybach, and\nP. Nguyen, \u201cModel Unit Exploration for Sequence-to-Sequence\nSpeech Recognition,\u201d in arXiv, 2019.\n[26] M. Schuster and K. Nakajima, \u201cJapanese and korean voice\nsearch,\u201d in ICASSP, 2012.\n[27] A. Graves, \u201cPractical Variational Inference for Neural Networks,\u201d\nin NIPS, 2011.\n[28] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRe-\nthinking the inception architecture for computer vision,\u201d in CVPR,\n2016.\n[29] J. Chorowski and N. Jaitly, \u201cTowards better decoding and lan-\nguage model integration in sequence to sequence models,\u201d in IN-\nTERSPEECH, 2017.\n[30] D. Povey, V. Peddinti, D. Galvez, P. Ghahrmani, V. Manohar,\nX. Na, Y. Wang, and S. Khudanpur, \u201cPurely sequence-trained\nneural networks for ASR based on lattice-free MMI,\u201d in INTER-\nSPEECH, 2016.\n[31] K. J. Han, A. Chandrashekaran, J. Kim, and I. Lane, \u201cThe CA-\nPIO 2017 Conversational Speech Recognition System,\u201d in arXiv,\n2017.\n[32] X. Yang, J. Li, and X. Zhou, \u201cA novel pyramidal-FSMN archi-\ntecture with lattice-free MMI for speech recognition,\u201d in arXiv,\n2018.\n[33] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2Letter: an End-\nto-End ConvNet-based Speech Recognition System,\u201d in arXiv,\n2016.\n[34] V. Liptchinsky, G. Synnaeve, and R. Collobert, \u201cLetter-Based\nSpeech Recognition with Gated ConvNets,\u201d in arXiv, 2017.\n[35] Y. Zhou, C. Xiong, and R. Socher, \u201cImproving End-to-End\nSpeech Recognition with Policy Learning,\u201d in ICASSP, 2018.\n[36] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve,\nand R. Collobert, \u201cFully Convolutional Speech Recognition,\u201d in\narXiv, 2018.\n[37] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M.\nCohen, H. Nguyen, and R. T. Gadde, \u201cJasper: An End-to-End\nConvolutional Neural Acoustic Model,\u201d in arXiv, 2019.\n[38] A. Zeyer, A. Merboldt, R. Schl\u00a8uter, and H. Ney, \u201cA comprehen-\nsive analysis on attention models,\u201d in NIPS: Workshop IRASL,\n2018.\n[39] S. Sabour, W. Chan, and M. Norouzi, \u201cOptimal Completion Dis-\ntillation for Sequence Learning,\u201d in ICLR, 2019.\n[40] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz,\nJ. Silovsky, G. Stemmer, and K. Vesely, \u201cThe Kaldi Speech\nRecognition Toolkit,\u201d in ASRU, 2011."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "We present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1.",
            "introduction": "Deep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a",
            "methodology": "for Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art",
            "conclusion": "s\nSpecAugment greatly improves the performance of ASR net-\nworks. We are able to obtain state-of-the-art results on the Lib-\nriSpeech 960h and Switchboard 300h tasks on end-to-end LAS\nnetworks by augmenting the training set using simple hand-\ncrafted policies, surpassing the performance of hybrid systems\neven without the aid of a language model. SpecAugment con-\nverts ASR from an over-\ufb01tting to an under-\ufb01tting problem, and\nwe were able to gain performance by using bigger networks and\ntraining longer.\nAcknowledgements:\nWe would like to thank Yuan Cao,\nCiprian Chelba, Kazuki Irie, Ye Jia, Anjuli Kannan, Patrick\nNguyen, Vijay Peddinti, Rohit Prabhavalkar, Yonghui Wu and\nShuyuan Zhang for useful discussions. We also thank Gy\u00a8orgy\nKov\u00b4acs for introducing us to the works [49, 50].\n7. References\n[1] G. Hinton, L. Deng, D. Yu, G. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V. Vanhoucke, P. Nguyen, B. Kingsbury et al., \u201cDeep\nneural networks for acoustic modeling in speech recognition,\u201d\nIEEE Signal processing magazine, vol. 29, 2012.\n[2] G. Dahl, D. Yu, L. Deng, and A. Acero, \u201cContext-Dependent\nPre-Trained Deep Neural Networks for Large-Vocabulary Speech\nRecognition,\u201d IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, vol. 20, Jan 2012.\n[3] T. Sainath, A. rahman Mohamed, B. Kingsbury, and B. Ramab-\nhadran, \u201cDeep Convolutional Neural Networks for LVCSR,\u201d in\nICASSP, 2013.\n[4] A. Graves, A. rahman Mohamed, and G. Hinton, \u201cSpeech Recog-\nnition with Deep Recurrent Neural Networks,\u201d in ICASSP, 2013.\n[5] A. Graves and N. Jaitly, \u201cTowards End-to-End Speech Recogni-\ntion with Recurrent Neural Networks,\u201d in ICML, 2014.\n[6] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, \u201cListen, Attend and\nSpell: A Neural Network for Large Vocabulary Conversational\nSpeech Recognition,\u201d in ICASSP, 2016.\n[7] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,\n\u201cEnd-to-End Attention-based Large Vocabulary Speech Recogni-\ntion,\u201d in ICASSP, 2016.\n[8] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, N. Jaitly,\nB. Li, J. Chorowski, and M. Bacchiani, \u201cState-of-the-art Speech\nRecognition With Sequence-to-Sequence Models,\u201d in ICASSP,\n2018.\n[9] N. Kanda, R. Takeda, and Y. Obuchi, \u201cElastic spectral distortion\nfor low resource speech recognition with deep neural networks,\u201d\nin ASRU, 2013.\n[10] A. Ragni, K. M. Knill, S. P. Rath, and M. J. F. Gales, \u201cData aug-\nmentation for low resource languages,\u201d in INTERSPEECH, 2014.\n[11] N. Jaitly and G. Hinton, \u201cVocal Tract Length Perturbation (VTLP)\nimproves speech recognition,\u201d in ICML Workshop on Deep Learn-\ning for Audio, Speech and Language Processing, 2013.\n[12] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos,\nE. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and\nA. Ng, \u201cDeep Speech: Scaling up end-to-end speech recognition,\u201d\nin arXiv, 2014.\n[13] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, \u201cAudio Augmen-\ntation for Speech Recognition,\u201d in INTERSPEECH, 2015.\n[14] C. Kim, A. Misra, K. Chin, T. Hughes, A. Narayanan, T. Sainath,\nand M. Bacchiani, \u201cGeneration of large-scale simulated utterances\nin virtual rooms to train deep-neural networks for far-\ufb01eld speech\nrecognition in Google Home,\u201d in INTERSPEECH, 2017.\n[15] R. Prabhavalkar, R. Alvarez, C. Parada, P. Nakkiran, and T. N.\nSainath, \u201cAutomatic gain control and multi-style training for ro-\nbust small-footprint keyword spotting with deep neural networks,\u201d\nin ICASSP, 2015.\n[16] A. Raju, S. Panchapagesan, X. Liu, A. Mandal, and N. Strom,\n\u201cData Augmentation for Robust Keyword Spotting under Play-\nback Interference,\u201d in arXiv, 2018.\n[17] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based\nfusion for Multistream ASR,\u201d in ICASSP, 2016.\n[18] E. D. Cubuk, B. Zoph, D. Man\u00b4e, V. Vasudevan, and Q. V. Le, \u201cAu-\ntoaugment: Learning augmentation policies from data,\u201d in CVPR,\n2019.\n[19] T. DeVries and G. Taylor, \u201cImproved Regularization of Convolu-\ntional Neural Networks with Cutout,\u201d in arXiv, 2017.\n[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech: An ASR corpus based on public domain audio books,\u201d\nin ICASSP, 2015.\n[21] C\u00b8 . G\u00a8ulc\u00b8ehre, O. Firat, K. Xu, K. Cho, L. Barrault, H. Lin,\nF. Bougares, H. Schwenk, and Y. Bengio, \u201cOn using monolingual\ncorpora in neural machine translation,\u201d in arxiv, 2015.\n[22] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSWITCHBOARD:\ntelephone speech corpus for research and development,\u201d in\nICASSP, 1992.\n[23] C. Cieri, D. Miller, and K. Walker, \u201cThe \ufb01sher corpus: a resource\nfor the next generations of speech-to-text,\u201d in LREC, 2004.\n[24] A. Zeyer, K. Irie, R. Schl\u00a8uter, and H. Ney, \u201cImproved training of\nend-to-end attention models for speech recognition,\u201d in INTER-\nSPEECH, 2018.\n[25] K. Irie, R. Prabhavalkar, A. Kannan, A. Bruguier, D. Rybach, and\nP. Nguyen, \u201cModel Unit Exploration for Sequence-to-Sequence\nSpeech Recognition,\u201d in arXiv, 2019.\n[26] M. Schuster and K. Nakajima, \u201cJapanese and korean voice\nsearch,\u201d in ICASSP, 2012.\n[27] A. Graves, \u201cPractical Variational Inference for Neural Networks,\u201d\nin NIPS, 2011.\n[28] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRe-\nthinking the inception architecture for computer vision,\u201d in CVPR,\n2016.\n[29] J. Chorowski and N. Jaitly, \u201cTowards better decoding and lan-\nguage model integration in sequence to sequence models,\u201d in IN-\nTERSPEECH, 2017.\n[30] D. Povey, V. Peddinti, D. Galvez, P. Ghahrmani, V. Manohar,\nX. Na, Y. Wang, and S. Khudanpur, \u201cPurely sequence-trained\nneural networks for ASR based on lattice-free MMI,\u201d in INTER-\nSPEECH, 2016.\n[31] K. J. Han, A. Chandrashekaran, J. Kim, and I. Lane, \u201cThe CA-\nPIO 2017 Conversational Speech Recognition System,\u201d in arXiv,\n2017.\n[32] X. Yang, J. Li, and X. Zhou, \u201cA novel pyramidal-FSMN archi-\ntecture with lattice-free MMI for speech recognition,\u201d in arXiv,\n2018.\n[33] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2Letter: an End-\nto-End ConvNet-based Speech Recognition System,\u201d in arXiv,\n2016.\n[34] V. Liptchinsky, G. Synnaeve, and R. Collobert, \u201cLetter-Based\nSpeech Recognition with Gated ConvNets,\u201d in arXiv, 2017.\n[35] Y. Zhou, C. Xiong, and R. Socher, \u201cImproving End-to-End\nSpeech Recognition with Policy Learning,\u201d in ICASSP, 2018.\n[36] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve,\nand R. Collobert, \u201cFully Convolutional Speech Recognition,\u201d in\narXiv, 2018.\n[37] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M.\nCohen, H. Nguyen, and R. T. Gadde, \u201cJasper: An End-to-End\nConvolutional Neural Acoustic Model,\u201d in arXiv, 2019.\n[38] A. Zeyer, A. Merboldt, R. Schl\u00a8uter, and H. Ney, \u201cA comprehen-\nsive analysis on attention models,\u201d in NIPS: Workshop IRASL,\n2018.\n[39] S. Sabour, W. Chan, and M. Norouzi, \u201cOptimal Completion Dis-\ntillation for Sequence Learning,\u201d in ICLR, 2019.\n[40] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz,\nJ. Silovsky, G. Stemmer, and K. Vesely, \u201cThe Kaldi Speech\nRecognition Toolkit,\u201d in ASRU, 2011.\n[41] K. Vesely, A. Ghoshal, L. Burger, and D. Povey, \u201cSequence-\ndiscriminative training of deep neural networks,\u201d in INTER-\nSPEECH, 2013.\n[42] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, \u201cEnd-to-end\nspeech recognition using lattice-free MMI,\u201d in INTERSPEECH,\n2018.\n[43] G. Zweig, C. Yu, J. Droppo, and A. Stolcke, \u201cAdvances in All-\nNeural Speech Recognition,\u201d in ICASSP, 2017.\n[44] K. Audhkhasi, B. Ramabhadran, G. Saon, M. Picheny, and D. Na-\nhamoo, \u201cDirect Acoustics-to-Word Models for English Conversa-\ntional Speech Recognition,\u201d in INTERSPEECH, 2018.\n[45] K. Audhkhasi, B. Kingsbury, B. Ramabhadran, G. Saon, and\nM. Picheny, \u201cBuilding competitive direct acoustics-to-word mod-\nels for english conversational speech recognition,\u201d in ICASSP,\n2018.\n[46] L. Lu, X. Zhang, and S. Renals, \u201cOn training the recurrent neural\nnetwork encoder-decoder for large vocabulary end-to-end speech\nrecognition,\u201d in ICASSP, 2016.\n[47] S. Toshniwal, H. Tang, L. Lu, and K. Livescu, \u201cMultitask Learn-\ning with Low-Level Auxiliary Tasks for Encoder-Decoder Based\nSpeech Recognition,\u201d in INTERSPEECH, 2017.\n[48] C. Weng, J. Cui, G. Wang, J. Wang, C. Yu, D. Su, and D. Yu, \u201cIm-\nproving Attention Based Sequence-to-Sequence Models for End-\nto-End English Conversational Speech Recognition,\u201d in INTER-\nSPEECH, 2018.\n[49] G. Kov\u00b4acs, L. T\u00b4oth, D. Van Compernolle, and S. Ganapathy, \u201cIn-\ncreasing the robustness of cnn acoustic models using autoregres-\nsive moving average spectrogram features and channel dropout,\u201d\nPattern Recognition Letters, vol. 100, pp. 44\u201350, 2017.\n[50] L. T\u00b4oth, G. Kov\u00b4acs, and D. Van Compernolle, \u201cA perceptually\ninspired data augmentation method for noise robust cnn acoustic\nmodels,\u201d in SPECOM, 2018."
        }
    },
    {
        "pdf_file": "paper3.pdf",
        "sections": {
            "abstract": "Recurrent neural networks (RNNs) are a powerful model for\nsequential data. End-to-end training methods such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art results in cur-\nsive handwriting recognition. However RNN performance in\nspeech recognition has so far been disappointing, with better\nresults returned by deep feedforward networks. This paper in-\nvestigates deep recurrent neural networks, which combine the\nmultiple levels of representation that have proved so effective\nin deep networks with the \ufb02exible use of long range context\nthat empowers RNNs. When trained end-to-end with suit-\nable regularisation, we \ufb01nd that deep Long Short-term Mem-\nory RNNs achieve a test set error of 17.7% on the TIMIT\nphoneme recognition benchmark, which to our knowledge is\nthe best recorded score.\nIndex Terms\u2014 recurrent neural networks, deep neural\nnetworks, speech recognition\n1.",
            "introduction": "Neural networks have a long history in speech recognition,\nusually in combination with hidden Markov models [1, 2].\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-\nral networks (RNNs) as an alternative model. HMM-RNN\nsystems [5] have also seen a recent revival [6, 7], but do not\ncurrently perform as well as deep networks.\nInstead of combining RNNs with HMMs, it is possible\nto train RNNs \u2018end-to-end\u2019 for speech recognition [8, 9, 10].\nThis approach exploits the larger state-space and richer dy-\nnamics of RNNs compared to HMMs, and avoids the prob-\nlem of using potentially incorrect alignments as training tar-\ngets. The combination of Long Short-term Memory [11], an\nRNN architecture with an improved memory, with end-to-end\ntraining has proved especially effective for cursive handwrit-\ning recognition [12, 13]. However it has so far made little\nimpact on speech recognition.\nRNNs are inherently deep in time, since their hidden state\nis a function of all previous hidden states. The question that\ninspired this paper was whether RNNs could also bene\ufb01t from\ndepth in space; that is from stacking multiple recurrent hid-\nden layers on top of each other, just as feedforward layers are\nstacked in conventional deep networks. To answer this ques-\ntion we introduce deep Long Short-term Memory RNNs and\nassess their potential for speech recognition. We also present\nan enhancement to a recently introduced end-to-end learning",
            "methodology": "s such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper3.pdf",
        "sections": {
            "abstract": "Recurrent neural networks (RNNs) are a powerful model for\nsequential data. End-to-end training methods such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art results in cur-\nsive handwriting recognition. However RNN performance in\nspeech recognition has so far been disappointing, with better\nresults returned by deep feedforward networks. This paper in-\nvestigates deep recurrent neural networks, which combine the\nmultiple levels of representation that have proved so effective\nin deep networks with the \ufb02exible use of long range context\nthat empowers RNNs. When trained end-to-end with suit-\nable regularisation, we \ufb01nd that deep Long Short-term Mem-\nory RNNs achieve a test set error of 17.7% on the TIMIT\nphoneme recognition benchmark, which to our knowledge is\nthe best recorded score.\nIndex Terms\u2014 recurrent neural networks, deep neural\nnetworks, speech recognition\n1.",
            "introduction": "Neural networks have a long history in speech recognition,\nusually in combination with hidden Markov models [1, 2].\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-\nral networks (RNNs) as an alternative model. HMM-RNN\nsystems [5] have also seen a recent revival [6, 7], but do not\ncurrently perform as well as deep networks.\nInstead of combining RNNs with HMMs, it is possible\nto train RNNs \u2018end-to-end\u2019 for speech recognition [8, 9, 10].\nThis approach exploits the larger state-space and richer dy-\nnamics of RNNs compared to HMMs, and avoids the prob-\nlem of using potentially incorrect alignments as training tar-\ngets. The combination of Long Short-term Memory [11], an\nRNN architecture with an improved memory, with end-to-end\ntraining has proved especially effective for cursive handwrit-\ning recognition [12, 13]. However it has so far made little\nimpact on speech recognition.\nRNNs are inherently deep in time, since their hidden state\nis a function of all previous hidden states. The question that\ninspired this paper was whether RNNs could also bene\ufb01t from\ndepth in space; that is from stacking multiple recurrent hid-\nden layers on top of each other, just as feedforward layers are\nstacked in conventional deep networks. To answer this ques-\ntion we introduce deep Long Short-term Memory RNNs and\nassess their potential for speech recognition. We also present\nan enhancement to a recently introduced end-to-end learning",
            "methodology": "s such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper3.pdf",
        "sections": {
            "abstract": "Recurrent neural networks (RNNs) are a powerful model for\nsequential data. End-to-end training methods such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art results in cur-\nsive handwriting recognition. However RNN performance in\nspeech recognition has so far been disappointing, with better\nresults returned by deep feedforward networks. This paper in-\nvestigates deep recurrent neural networks, which combine the\nmultiple levels of representation that have proved so effective\nin deep networks with the \ufb02exible use of long range context\nthat empowers RNNs. When trained end-to-end with suit-\nable regularisation, we \ufb01nd that deep Long Short-term Mem-\nory RNNs achieve a test set error of 17.7% on the TIMIT\nphoneme recognition benchmark, which to our knowledge is\nthe best recorded score.\nIndex Terms\u2014 recurrent neural networks, deep neural\nnetworks, speech recognition\n1.",
            "introduction": "Neural networks have a long history in speech recognition,\nusually in combination with hidden Markov models [1, 2].\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-\nral networks (RNNs) as an alternative model. HMM-RNN\nsystems [5] have also seen a recent revival [6, 7], but do not\ncurrently perform as well as deep networks.\nInstead of combining RNNs with HMMs, it is possible\nto train RNNs \u2018end-to-end\u2019 for speech recognition [8, 9, 10].\nThis approach exploits the larger state-space and richer dy-\nnamics of RNNs compared to HMMs, and avoids the prob-\nlem of using potentially incorrect alignments as training tar-\ngets. The combination of Long Short-term Memory [11], an\nRNN architecture with an improved memory, with end-to-end\ntraining has proved especially effective for cursive handwrit-\ning recognition [12, 13]. However it has so far made little\nimpact on speech recognition.\nRNNs are inherently deep in time, since their hidden state\nis a function of all previous hidden states. The question that\ninspired this paper was whether RNNs could also bene\ufb01t from\ndepth in space; that is from stacking multiple recurrent hid-\nden layers on top of each other, just as feedforward layers are\nstacked in conventional deep networks. To answer this ques-\ntion we introduce deep Long Short-term Memory RNNs and\nassess their potential for speech recognition. We also present\nan enhancement to a recently introduced end-to-end learning",
            "methodology": "s such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper3.pdf",
        "sections": {
            "abstract": "Recurrent neural networks (RNNs) are a powerful model for\nsequential data. End-to-end training methods such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art results in cur-\nsive handwriting recognition. However RNN performance in\nspeech recognition has so far been disappointing, with better\nresults returned by deep feedforward networks. This paper in-\nvestigates deep recurrent neural networks, which combine the\nmultiple levels of representation that have proved so effective\nin deep networks with the \ufb02exible use of long range context\nthat empowers RNNs. When trained end-to-end with suit-\nable regularisation, we \ufb01nd that deep Long Short-term Mem-\nory RNNs achieve a test set error of 17.7% on the TIMIT\nphoneme recognition benchmark, which to our knowledge is\nthe best recorded score.\nIndex Terms\u2014 recurrent neural networks, deep neural\nnetworks, speech recognition\n1.",
            "introduction": "Neural networks have a long history in speech recognition,\nusually in combination with hidden Markov models [1, 2].\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-\nral networks (RNNs) as an alternative model. HMM-RNN\nsystems [5] have also seen a recent revival [6, 7], but do not\ncurrently perform as well as deep networks.\nInstead of combining RNNs with HMMs, it is possible\nto train RNNs \u2018end-to-end\u2019 for speech recognition [8, 9, 10].\nThis approach exploits the larger state-space and richer dy-\nnamics of RNNs compared to HMMs, and avoids the prob-\nlem of using potentially incorrect alignments as training tar-\ngets. The combination of Long Short-term Memory [11], an\nRNN architecture with an improved memory, with end-to-end\ntraining has proved especially effective for cursive handwrit-\ning recognition [12, 13]. However it has so far made little\nimpact on speech recognition.\nRNNs are inherently deep in time, since their hidden state\nis a function of all previous hidden states. The question that\ninspired this paper was whether RNNs could also bene\ufb01t from\ndepth in space; that is from stacking multiple recurrent hid-\nden layers on top of each other, just as feedforward layers are\nstacked in conventional deep networks. To answer this ques-\ntion we introduce deep Long Short-term Memory RNNs and\nassess their potential for speech recognition. We also present\nan enhancement to a recently introduced end-to-end learning",
            "methodology": "s such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art",
            "conclusion": "s we can draw from this are (a) LSTM\nworks much better than tanh for this task, (b) bidirectional\nTable 1. TIMIT Phoneme Recognition Results. \u2018Epochs\u2019 is\nthe number of passes through the training set before conver-\ngence. \u2018PER\u2019 is the phoneme error rate on the core test set.\nNETWORK\nWEIGHTS\nEPOCHS\nPER\nCTC-3L-500H-TANH\n3.7M\n107\n37.6%\nCTC-1L-250H\n0.8M\n82\n23.9%\nCTC-1L-622H\n3.8M\n87\n23.0%\nCTC-2L-250H\n2.3M\n55\n21.0%\nCTC-3L-421H-UNI\n3.8M\n115\n19.6%\nCTC-3L-250H\n3.8M\n124\n18.6%\nCTC-5L-250H\n6.8M\n150\n18.4%\nTRANS-3L-250H\n4.3M\n112\n18.3%\nPRETRANS-3L-250H\n4.3M\n144\n17.7%\nFig. 3. Input Sensitivity of a deep CTC RNN. The heatmap\n(top) shows the derivatives of the \u2018ah\u2019 and \u2018p\u2019 outputs printed\nin red with respect to the \ufb01lterbank inputs (bottom).\nThe\nTIMIT ground truth segmentation is shown below. Note that\nthe sensitivity extends to surrounding segments; this may be\nbecause CTC (which lacks an explicit language model) at-\ntempts to learn linguistic dependencies from the acoustic data.\nLSTM has a slight advantage over unidirectional LSTMand\n(c) depth is more important than layer size (which supports\nprevious \ufb01ndings for deep networks [3]). Although the advan-\ntage of the transducer is slight when the weights are randomly\ninitialised, it becomes more substantial when pretraining is\nused.\n5. CONCLUSIONS AND FUTURE WORK\nWe have shown that the combination of deep, bidirectional\nLong Short-term Memory RNNs with end-to-end training and\nweight noise gives state-of-the-art results in phoneme recog-\nnition on the TIMIT database. An obvious next step is to ex-\ntend the system to large vocabulary speech recognition. An-\nother interesting direction would be to combine frequency-\ndomain convolutional neural networks [27] with deep LSTM."
        }
    },
    {
        "pdf_file": "paper3.pdf",
        "sections": {
            "abstract": "Recurrent neural networks (RNNs) are a powerful model for\nsequential data. End-to-end training methods such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art results in cur-\nsive handwriting recognition. However RNN performance in\nspeech recognition has so far been disappointing, with better\nresults returned by deep feedforward networks. This paper in-\nvestigates deep recurrent neural networks, which combine the\nmultiple levels of representation that have proved so effective\nin deep networks with the \ufb02exible use of long range context\nthat empowers RNNs. When trained end-to-end with suit-\nable regularisation, we \ufb01nd that deep Long Short-term Mem-\nory RNNs achieve a test set error of 17.7% on the TIMIT\nphoneme recognition benchmark, which to our knowledge is\nthe best recorded score.\nIndex Terms\u2014 recurrent neural networks, deep neural\nnetworks, speech recognition\n1.",
            "introduction": "Neural networks have a long history in speech recognition,\nusually in combination with hidden Markov models [1, 2].\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-\nral networks (RNNs) as an alternative model. HMM-RNN\nsystems [5] have also seen a recent revival [6, 7], but do not\ncurrently perform as well as deep networks.\nInstead of combining RNNs with HMMs, it is possible\nto train RNNs \u2018end-to-end\u2019 for speech recognition [8, 9, 10].\nThis approach exploits the larger state-space and richer dy-\nnamics of RNNs compared to HMMs, and avoids the prob-\nlem of using potentially incorrect alignments as training tar-\ngets. The combination of Long Short-term Memory [11], an\nRNN architecture with an improved memory, with end-to-end\ntraining has proved especially effective for cursive handwrit-\ning recognition [12, 13]. However it has so far made little\nimpact on speech recognition.\nRNNs are inherently deep in time, since their hidden state\nis a function of all previous hidden states. The question that\ninspired this paper was whether RNNs could also bene\ufb01t from\ndepth in space; that is from stacking multiple recurrent hid-\nden layers on top of each other, just as feedforward layers are\nstacked in conventional deep networks. To answer this ques-\ntion we introduce deep Long Short-term Memory RNNs and\nassess their potential for speech recognition. We also present\nan enhancement to a recently introduced end-to-end learning",
            "methodology": "s such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art",
            "conclusion": "s we can draw from this are (a) LSTM\nworks much better than tanh for this task, (b) bidirectional\nTable 1. TIMIT Phoneme Recognition Results. \u2018Epochs\u2019 is\nthe number of passes through the training set before conver-\ngence. \u2018PER\u2019 is the phoneme error rate on the core test set.\nNETWORK\nWEIGHTS\nEPOCHS\nPER\nCTC-3L-500H-TANH\n3.7M\n107\n37.6%\nCTC-1L-250H\n0.8M\n82\n23.9%\nCTC-1L-622H\n3.8M\n87\n23.0%\nCTC-2L-250H\n2.3M\n55\n21.0%\nCTC-3L-421H-UNI\n3.8M\n115\n19.6%\nCTC-3L-250H\n3.8M\n124\n18.6%\nCTC-5L-250H\n6.8M\n150\n18.4%\nTRANS-3L-250H\n4.3M\n112\n18.3%\nPRETRANS-3L-250H\n4.3M\n144\n17.7%\nFig. 3. Input Sensitivity of a deep CTC RNN. The heatmap\n(top) shows the derivatives of the \u2018ah\u2019 and \u2018p\u2019 outputs printed\nin red with respect to the \ufb01lterbank inputs (bottom).\nThe\nTIMIT ground truth segmentation is shown below. Note that\nthe sensitivity extends to surrounding segments; this may be\nbecause CTC (which lacks an explicit language model) at-\ntempts to learn linguistic dependencies from the acoustic data.\nLSTM has a slight advantage over unidirectional LSTMand\n(c) depth is more important than layer size (which supports\nprevious \ufb01ndings for deep networks [3]). Although the advan-\ntage of the transducer is slight when the weights are randomly\ninitialised, it becomes more substantial when pretraining is\nused.\n5. CONCLUSIONS AND FUTURE WORK\nWe have shown that the combination of deep, bidirectional\nLong Short-term Memory RNNs with end-to-end training and\nweight noise gives state-of-the-art results in phoneme recog-\nnition on the TIMIT database. An obvious next step is to ex-\ntend the system to large vocabulary speech recognition. An-\nother interesting direction would be to combine frequency-\ndomain convolutional neural networks [27] with deep LSTM.\n6. REFERENCES\n[1] H.A. Bourlard and N. Morgan, Connnectionist Speech\nRecognition: A Hybrid Approach,\nKluwer Academic\nPublishers, 1994.\n[2] Qifeng Zhu, Barry Chen, Nelson Morgan, and Andreas\nStolcke, \u201cTandem connectionist feature extraction for\nconversational speech recognition,\u201d\nin International\nConference on Machine Learning for Multimodal Inter-\naction, Berlin, Heidelberg, 2005, MLMI\u201904, pp. 223\u2013\n231, Springer-Verlag.\n[3] A. Mohamed, G.E. Dahl, and G. Hinton,\n\u201cAcoustic\nmodeling using deep belief networks,\u201d Audio, Speech,\nand Language Processing, IEEE Transactions on, vol.\n20, no. 1, pp. 14 \u201322, jan. 2012.\n[4] G. Hinton, Li Deng, Dong Yu, G.E. Dahl, A. Mohamed,\nN. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T.N.\nSainath, and B. Kingsbury, \u201cDeep neural networks for\nacoustic modeling in speech recognition,\u201d Signal Pro-\ncessing Magazine, IEEE, vol. 29, no. 6, pp. 82 \u201397, nov.\n2012.\n[5] A. J. Robinson, \u201cAn Application of Recurrent Nets to\nPhone Probability Estimation,\u201d IEEE Transactions on\nNeural Networks, vol. 5, no. 2, pp. 298\u2013305, 1994.\n[6] Oriol Vinyals, Suman Ravuri, and Daniel Povey, \u201cRe-\nvisiting Recurrent Neural Networks for Robust ASR,\u201d\nin ICASSP, 2012.\n[7] A. Maas, Q. Le, T. O\u2019Neil, O. Vinyals, P. Nguyen, and\nA. Ng, \u201cRecurrent neural networks for noise reduction\nin robust asr,\u201d in INTERSPEECH, 2012.\n[8] A. Graves, S. Fern\u00b4andez, F. Gomez, and J. Schmidhuber,\n\u201cConnectionist Temporal Classi\ufb01cation: Labelling Un-\nsegmented Sequence Data with Recurrent Neural Net-\nworks,\u201d in ICML, Pittsburgh, USA, 2006.\n[9] A. Graves, Supervised sequence labelling with recurrent\nneural networks, vol. 385, Springer, 2012.\n[10] A. Graves, \u201cSequence transduction with recurrent neu-\nral networks,\u201d in ICML Representation Learning Work-\nsop, 2012.\n[11] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term\nMemory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u2013\n1780, 1997.\n[12] A. Graves, S. Fern\u00b4andez, M. Liwicki, H. Bunke, and\nJ. Schmidhuber,\n\u201cUnconstrained Online Handwriting\nRecognition with Recurrent Neural Networks,\u201d in NIPS.\n2008.\n[13] Alex Graves and Juergen Schmidhuber, \u201cOf\ufb02ine Hand-\nwriting Recognition with Multidimensional Recurrent\nNeural Networks,\u201d in NIPS. 2009.\n[14] F. Gers, N. Schraudolph, and J. Schmidhuber, \u201cLearning\nPrecise Timing with LSTM Recurrent Networks,\u201d Jour-\nnal of Machine Learning Research, vol. 3, pp. 115\u2013143,\n2002.\n[15] M. Schuster and K. K. Paliwal, \u201cBidirectional Recur-\nrent Neural Networks,\u201d IEEE Transactions on Signal\nProcessing, vol. 45, pp. 2673\u20132681, 1997.\n[16] A. Graves and J. Schmidhuber, \u201cFramewise Phoneme\nClassi\ufb01cation with Bidirectional LSTM and Other Neu-\nral Network Architectures,\u201d Neural Networks, vol. 18,\nno. 5-6, pp. 602\u2013610, June/July 2005.\n[17] David\nE.\nRumelhart,\nGeoffrey\nE.\nHinton,\nand\nRonald J. Williams, Learning representations by back-\npropagating errors, pp. 696\u2013699, MIT Press, 1988.\n[18] Georey Zweig and Patrick Nguyen, \u201cSCARF: A seg-\nmental CRF speech recognition system,\u201d\nTech. Rep.,\nMicrosoft Research, 2009.\n[19] Andrew W. Senior and Anthony J. Robinson, \u201cForward-\nbackward retraining of recurrent neural networks,\u201d in\nNIPS, 1995, pp. 743\u2013749.\n[20] Abdel rahman Mohamed, Dong Yu, and Li Deng, \u201cIn-\nvestigation of full-sequence training of deep belief net-\nworks for speech recognition,\u201d in in Interspeech, 2010.\n[21] M. Lehr and I. Shafran,\n\u201cDiscriminatively estimated\njoint acoustic, duration, and language model for speech\nrecognition,\u201d in ICASSP, 2010, pp. 5542 \u20135545.\n[22] Kam-Chuen Jim, C.L. Giles, and B.G. Horne, \u201cAn anal-\nysis of noise in recurrent neural networks: convergence\nand generalization,\u201d Neural Networks, IEEE Transac-\ntions on, vol. 7, no. 6, pp. 1424 \u20131438, nov 1996.\n[23] Geoffrey E. Hinton and Drew van Camp, \u201cKeeping the\nneural networks simple by minimizing the description\nlength of the weights,\u201d in COLT, 1993, pp. 5\u201313.\n[24] Alex Graves, \u201cPractical variational inference for neural\nnetworks,\u201d in NIPS, pp. 2348\u20132356. 2011.\n[25] DARPA-ISTO, The DARPA TIMIT Acoustic-Phonetic\nContinuous Speech Corpus (TIMIT), speech disc cd1-\n1.1 edition, 1990.\n[26] Kai fu Lee and Hsiao wuen Hon, \u201cSpeaker-independent\nphone recognition using hidden markov models,\u201d IEEE\nTransactions on Acoustics, Speech, and Signal Process-\ning, 1989.\n[27] O. Abdel-Hamid, A. Mohamed, Hui Jiang, and G. Penn,\n\u201cApplying convolutional neural networks concepts to\nhybrid nn-hmm model for speech recognition,\u201d\nin\nICASSP, march 2012, pp. 4277 \u20134280."
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "",
            "methodology": "",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "",
            "methodology": "",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "",
            "methodology": "",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "We introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1",
            "introduction": "Recent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective",
            "methodology": "for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey",
            "conclusion": ""
        }
    }
]