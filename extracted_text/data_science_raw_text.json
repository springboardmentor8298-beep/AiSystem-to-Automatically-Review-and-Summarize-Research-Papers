[
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired DataFrame:\n                   Pearson Correlation Coefficient\nCol1 Col2                                 \n0        3                            0.373153\n1        3                            0.419219\n          4                            0.356149\n3        4                            0.389972\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired Series:\n0  3    0.373153\n1  3    0.419219\n    4    0.356149\n3  4    0.389972\ndtype: float64\nFigure 22: An example problem of semantic perturbation. The type of the desired result has been changed but the content\nstill keeps the same.\nOrigin\nProblem:\nI have a logistic regression model using Pytorch, where my input is high-dimensional and my output must be a \nscalar - 0, 1 or 2.\nI'm using a linear layer combined with a softmax layer to return a n x 3 tensor, where each column represents the \nprobability of the input falling in one of the three classes (0, 1 or 2).\nHowever, I must return a n x 1 tensor, so I need to somehow pick the highest probability for each input and create \na tensor indicating which class had the highest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.2, 0.2],\n [0.1, 0.8, 0.1]]\nAnd I must return this:\n[[2],\n [0],\n [1]]\nProblem:\n\u2026[omit for brevity]\nHowever, I must return a 1 x n tensor, and I want to somehow pick the lowest probability for each input and \ncreate a tensor indicating which class had the lowest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.3, 0.1],\n [0.15, 0.8, 0.05]]\nAnd I must return this:\n[1, 2, 2], which has the type torch.LongTensor\nFigure 23: An example problem that is dif\ufb01cult re-written with a combination of surface and semantic perturbations\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired DataFrame:\n                   Pearson Correlation Coefficient\nCol1 Col2                                 \n0        3                            0.373153\n1        3                            0.419219\n          4                            0.356149\n3        4                            0.389972\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired Series:\n0  3    0.373153\n1  3    0.419219\n    4    0.356149\n3  4    0.389972\ndtype: float64\nFigure 22: An example problem of semantic perturbation. The type of the desired result has been changed but the content\nstill keeps the same.\nOrigin\nProblem:\nI have a logistic regression model using Pytorch, where my input is high-dimensional and my output must be a \nscalar - 0, 1 or 2.\nI'm using a linear layer combined with a softmax layer to return a n x 3 tensor, where each column represents the \nprobability of the input falling in one of the three classes (0, 1 or 2).\nHowever, I must return a n x 1 tensor, so I need to somehow pick the highest probability for each input and create \na tensor indicating which class had the highest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.2, 0.2],\n [0.1, 0.8, 0.1]]\nAnd I must return this:\n[[2],\n [0],\n [1]]\nProblem:\n\u2026[omit for brevity]\nHowever, I must return a 1 x n tensor, and I want to somehow pick the lowest probability for each input and \ncreate a tensor indicating which class had the lowest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.3, 0.1],\n [0.15, 0.8, 0.05]]\nAnd I must return this:\n[1, 2, 2], which has the type torch.LongTensor\nFigure 23: An example problem that is dif\ufb01cult re-written with a combination of surface and semantic perturbations\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI can't figure out how to do a Two-sample KS test in Scipy.\n\u2026[omit for brevity]\ntest_stat = kstest(x, 'norm')\n#>>> test_stat\n#(0.021080234718821145, 0.76584491300591395)\nWhich means that at p-value of 0.76 we can not reject the null hypothesis that the two distributions are identical.\nHowever, I want to compare two distributions and see if I can reject the null hypothesis that they are identical.\n\u2026[omit for brevity]\nI tried the naive:\ntest_stat = kstest(x, z)\nand got the following error:\nTypeError: 'numpy.ndarray' object is not callable\nIs there a way to do a two-sample KS test in Python? If so, how should I do it?\nThank You in Advance\nProblem:\n\u2026[omit for brevity]\nIs there a way to do a two-sample KS test in Python, then test whether I can reject the null hypothesis that the \ntwo distributions are identical(result=True means able to reject, and the vice versa) based on alpha? If so, how \nshould I do it?\nThank You in Advance\nFigure 24: An example problem that is dif\ufb01cult re-written for more complexity\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. inv_A is an inverse of column A and so on.\n\u2026 [omitted for brevity]\nObviously there are redundant methods like doing this in a loop, but there should exist \nmuch more pythonic ways of doing it \u2026 [omitted for brevity]\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nresult = ...# put solution in this variable\nBEGIN SOLUTION\n<code>\nFigure 25: Completion prompt corresponding to Figure 1.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired DataFrame:\n                   Pearson Correlation Coefficient\nCol1 Col2                                 \n0        3                            0.373153\n1        3                            0.419219\n          4                            0.356149\n3        4                            0.389972\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired Series:\n0  3    0.373153\n1  3    0.419219\n    4    0.356149\n3  4    0.389972\ndtype: float64\nFigure 22: An example problem of semantic perturbation. The type of the desired result has been changed but the content\nstill keeps the same.\nOrigin\nProblem:\nI have a logistic regression model using Pytorch, where my input is high-dimensional and my output must be a \nscalar - 0, 1 or 2.\nI'm using a linear layer combined with a softmax layer to return a n x 3 tensor, where each column represents the \nprobability of the input falling in one of the three classes (0, 1 or 2).\nHowever, I must return a n x 1 tensor, so I need to somehow pick the highest probability for each input and create \na tensor indicating which class had the highest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.2, 0.2],\n [0.1, 0.8, 0.1]]\nAnd I must return this:\n[[2],\n [0],\n [1]]\nProblem:\n\u2026[omit for brevity]\nHowever, I must return a 1 x n tensor, and I want to somehow pick the lowest probability for each input and \ncreate a tensor indicating which class had the lowest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.3, 0.1],\n [0.15, 0.8, 0.05]]\nAnd I must return this:\n[1, 2, 2], which has the type torch.LongTensor\nFigure 23: An example problem that is dif\ufb01cult re-written with a combination of surface and semantic perturbations\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI can't figure out how to do a Two-sample KS test in Scipy.\n\u2026[omit for brevity]\ntest_stat = kstest(x, 'norm')\n#>>> test_stat\n#(0.021080234718821145, 0.76584491300591395)\nWhich means that at p-value of 0.76 we can not reject the null hypothesis that the two distributions are identical.\nHowever, I want to compare two distributions and see if I can reject the null hypothesis that they are identical.\n\u2026[omit for brevity]\nI tried the naive:\ntest_stat = kstest(x, z)\nand got the following error:\nTypeError: 'numpy.ndarray' object is not callable\nIs there a way to do a two-sample KS test in Python? If so, how should I do it?\nThank You in Advance\nProblem:\n\u2026[omit for brevity]\nIs there a way to do a two-sample KS test in Python, then test whether I can reject the null hypothesis that the \ntwo distributions are identical(result=True means able to reject, and the vice versa) based on alpha? If so, how \nshould I do it?\nThank You in Advance\nFigure 24: An example problem that is dif\ufb01cult re-written for more complexity\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. inv_A is an inverse of column A and so on.\n\u2026 [omitted for brevity]\nObviously there are redundant methods like doing this in a loop, but there should exist \nmuch more pythonic ways of doing it \u2026 [omitted for brevity]\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nresult = ...# put solution in this variable\nBEGIN SOLUTION\n<code>\nFigure 25: Completion prompt corresponding to Figure 1.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nsolve this question with example variable `clf` and \nput result in `proba`\nBEGIN SOLUTION\n<code>\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nproba = clf.predict_proba(X_test)\nprint(proba)\n</code>\nProblem:\nSay that I want to train BaggingClassifier that uses DecisionTreeClassifier:\ndt = DecisionTreeClassifier(max_depth = 1)\nbc = BaggingClassifier(dt, n_estimators = 20, max_samples = 0.5, max_features = 0.5)\nbc = bc.fit(X_train, y_train)\nI would like to use GridSearchCV to find the best parameters for both BaggingClassifier and \nDecisionTreeClassifier(e.g. max_depth from DecisionTreeClassifier and max_samples from \nBaggingClassifier), what is the syntax for this? Besides, you can just use the default arguments of GridSearchCV.\nFigure 26: More complex Completion (on the right) prompt that requires additional information for a solution.\nProblem:\nI am using Pandas to get a dataframe like this:\n    name  a  b   c\n0  Aaron 3  5   7\n1  Aaron 3  6   9\n2  Aaron 3  6  10\n3  Brave  4  6   0\n4  Brave  3  6   1\nI want to replace each name with a unique ID so output looks like:\n  name  a  b   c\n0    1     3   5   7\n1    1     3   6   9\n2    1     3   6  10\n3    2     4   6   0\n4    2     3   6   1\nHow can I do that?\nReference Solution\n# df: pd.DataFrame as input\nresult = df.replace(df['name'].unique(),\n range(1, len(df['name'].unique()) + 1))\nWrong Solution\n# create a column named \"ID\"\ndf['ID'] = df.groupby(['name']).ngroup()\nresult = df\nFigure 27: An example wrong solution that misunderstands the requirements and modi\ufb01es on the wrong column.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired DataFrame:\n                   Pearson Correlation Coefficient\nCol1 Col2                                 \n0        3                            0.373153\n1        3                            0.419219\n          4                            0.356149\n3        4                            0.389972\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired Series:\n0  3    0.373153\n1  3    0.419219\n    4    0.356149\n3  4    0.389972\ndtype: float64\nFigure 22: An example problem of semantic perturbation. The type of the desired result has been changed but the content\nstill keeps the same.\nOrigin\nProblem:\nI have a logistic regression model using Pytorch, where my input is high-dimensional and my output must be a \nscalar - 0, 1 or 2.\nI'm using a linear layer combined with a softmax layer to return a n x 3 tensor, where each column represents the \nprobability of the input falling in one of the three classes (0, 1 or 2).\nHowever, I must return a n x 1 tensor, so I need to somehow pick the highest probability for each input and create \na tensor indicating which class had the highest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.2, 0.2],\n [0.1, 0.8, 0.1]]\nAnd I must return this:\n[[2],\n [0],\n [1]]\nProblem:\n\u2026[omit for brevity]\nHowever, I must return a 1 x n tensor, and I want to somehow pick the lowest probability for each input and \ncreate a tensor indicating which class had the lowest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.3, 0.1],\n [0.15, 0.8, 0.05]]\nAnd I must return this:\n[1, 2, 2], which has the type torch.LongTensor\nFigure 23: An example problem that is dif\ufb01cult re-written with a combination of surface and semantic perturbations\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI can't figure out how to do a Two-sample KS test in Scipy.\n\u2026[omit for brevity]\ntest_stat = kstest(x, 'norm')\n#>>> test_stat\n#(0.021080234718821145, 0.76584491300591395)\nWhich means that at p-value of 0.76 we can not reject the null hypothesis that the two distributions are identical.\nHowever, I want to compare two distributions and see if I can reject the null hypothesis that they are identical.\n\u2026[omit for brevity]\nI tried the naive:\ntest_stat = kstest(x, z)\nand got the following error:\nTypeError: 'numpy.ndarray' object is not callable\nIs there a way to do a two-sample KS test in Python? If so, how should I do it?\nThank You in Advance\nProblem:\n\u2026[omit for brevity]\nIs there a way to do a two-sample KS test in Python, then test whether I can reject the null hypothesis that the \ntwo distributions are identical(result=True means able to reject, and the vice versa) based on alpha? If so, how \nshould I do it?\nThank You in Advance\nFigure 24: An example problem that is dif\ufb01cult re-written for more complexity\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. inv_A is an inverse of column A and so on.\n\u2026 [omitted for brevity]\nObviously there are redundant methods like doing this in a loop, but there should exist \nmuch more pythonic ways of doing it \u2026 [omitted for brevity]\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nresult = ...# put solution in this variable\nBEGIN SOLUTION\n<code>\nFigure 25: Completion prompt corresponding to Figure 1.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nsolve this question with example variable `clf` and \nput result in `proba`\nBEGIN SOLUTION\n<code>\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nproba = clf.predict_proba(X_test)\nprint(proba)\n</code>\nProblem:\nSay that I want to train BaggingClassifier that uses DecisionTreeClassifier:\ndt = DecisionTreeClassifier(max_depth = 1)\nbc = BaggingClassifier(dt, n_estimators = 20, max_samples = 0.5, max_features = 0.5)\nbc = bc.fit(X_train, y_train)\nI would like to use GridSearchCV to find the best parameters for both BaggingClassifier and \nDecisionTreeClassifier(e.g. max_depth from DecisionTreeClassifier and max_samples from \nBaggingClassifier), what is the syntax for this? Besides, you can just use the default arguments of GridSearchCV.\nFigure 26: More complex Completion (on the right) prompt that requires additional information for a solution.\nProblem:\nI am using Pandas to get a dataframe like this:\n    name  a  b   c\n0  Aaron 3  5   7\n1  Aaron 3  6   9\n2  Aaron 3  6  10\n3  Brave  4  6   0\n4  Brave  3  6   1\nI want to replace each name with a unique ID so output looks like:\n  name  a  b   c\n0    1     3   5   7\n1    1     3   6   9\n2    1     3   6  10\n3    2     4   6   0\n4    2     3   6   1\nHow can I do that?\nReference Solution\n# df: pd.DataFrame as input\nresult = df.replace(df['name'].unique(),\n range(1, len(df['name'].unique()) + 1))\nWrong Solution\n# create a column named \"ID\"\ndf['ID'] = df.groupby(['name']).ngroup()\nresult = df\nFigure 27: An example wrong solution that misunderstands the requirements and modi\ufb01es on the wrong column.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI'm using tensorflow 2.10.0.\nI have a list of bytes and I want to convert it to a list of strings:\nx=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd\n8\\xa9',\n\u00a0\u00a0\u00a0b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n\u00a0\u00a0\u00a0\u00a0b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n\u00a0\u00a0\u00a0\u00a0b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n\u00a0\u00a0\u00a0\u00a0b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\u00a0\nHow can I get the string result list in Tensorflow? \nReference Solution\n# x: list of bytes as input\nresult = [tf.compat.as_str_any(a) for a in x]\nWrong Solution\n# Not using method in Tensorflow\nresult = [item.decode('utf-8') for item in x]\nFigure 28: An example wrong solution that uses a common function instead of a function of TensorFlow.\n102\nI think it's a pretty common message for PyTorch users with low GPU memory:\nRuntimeError: CUDA out of memory. Tried to allocate \ud83d\ude0a MiB (GPU \ud83d\ude0a; \ud83d\ude0a GiB total\ncapacity; \ud83d\ude0a GiB already allocated; \ud83d\ude0a MiB free; \ud83d\ude0a cached)\nI tried to process an image by loading each layer to GPU and then loading it back:\n m \n self.children():\n    m.cuda()\n    x = m(x)\n    m.cpu()\n    torch.cuda.empty_cache()\nfor\nin\nBut it doesn't seem to be very effective. I'm wondering is there any tips and tricks to train\nlarge deep learning models while using little GPU memory.\npython\ndeep-learning\npytorch\nobject-detection\nlow-memory\nFigure 29: An example untestable problem involving hardware problems.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "DS-1000: A Natural and Reliable Benchmark for Data Science Code\nGeneration\nYuhang Lai\u22171\nChengxi Li\u22171\nYiming Wang\u22171 2\nTianyi Zhang\u22173\nRuiqi Zhong\u22174\nLuke Zettlemoyer 5 6\nScott Wen-tau Yih 6\nDaniel Fried 7\nSida Wang 6\nTao Yu 1 5\nAbstract\nWe introduce DS-1000, a code generation bench-\nmark with a thousand data science problems\nspanning seven Python libraries, such as NumPy\nand Pandas.\nCompared to prior works, DS-\n1000 incorporates three core features. First, our\nproblems re\ufb02ect diverse, realistic, and practical\nuse cases since we collected them from Stack-\nOver\ufb02ow. Second, our automatic evaluation is\nhighly speci\ufb01c (reliable) \u2013 across all Codex-002-\npredicted solutions that our evaluation accept,\nonly 1.8% of them are incorrect; we achieve\nthis with multi-criteria metrics, checking both\nfunctional correctness by running test cases and\nsurface-form constraints by restricting API us-\nages or keywords. Finally, we proactively defend\nagainst memorization by slightly modifying our\nproblems to be different from the original Stack-\nOver\ufb02ow source; consequently, models cannot\nanswer them correctly by memorizing the solu-\ntions from pre-training. The current best pub-\nlic system (Codex-002) achieves 43.3% accuracy,\nleaving ample room for improvement. We release\nour benchmark at https://ds1000-code-gen.\ngithub.io.\n1. Introduction\nData science is important in many areas (Romero & Ventura,\n2013; Bolyen et al., 2019; Faghmous & Kumar, 2014), but\nrequires programming pro\ufb01ciency in specialized libraries,\nthus posing substantial barriers to lay users. Fortunately,\nthese barriers could potentially be reduced by pre-trained\ncode generation models: for example, Codex (Chen et al.,\n2021a) can complete small Python snippets with non-trivial\n*Equal contribution. Author ordering determined by alphabet-\nical order.\n1The University of Hong Kong 2Peking University\n3Stanford University 4UC Berkeley 5University of Washington\n6Meta AI 7Carnegie Mellon University. Correspondence to: Tao\nYu <tyu@cs.hku.hk>.\naccuracy and AlphaCode (Li et al., 2022) can tackle dif\ufb01cult\ncompetitive programming problems. We anticipate that\nthese barriers will diminish if the community can make solid\nprogress in applying these models to data science problems.\nHowever, we currently lack a benchmark that 1) focuses on\neveryday data science applications, 2) includes naturalistic\nintents and contexts, and 3) has a reliable execution-based\nevaluation metric. Most of the existing datasets with reliable\ntest cases (Hendrycks et al., 2021; Chen et al., 2021a) focus\non competition or interview-style programming problems;\nthey measure algorithmic understanding but do not target\nreal-world usage. Also, as represented by e.g., user prob-\nlems on StackOver\ufb02ow, users\u2019 data science coding problems\nusually have diverse contexts including their incorrect code,\nerror messages, and input-output examples, which cannot\nbe found in most prior data science relevant code generation\nbenchmarks (Yin et al., 2018; Hendrycks et al., 2021; Chan-\ndel et al., 2022b; Chen et al., 2021a). Moreover, most of\nthese benchmarks solely rely on surface-form metrics such\nas BLEU or CodeBLEU (Yin et al., 2018; Agashe et al.,\n2019; Chen et al., 2021b). These metrics diverge from the\nprogrammer\u2019s intent, increasingly so as model capability\nimproves (Zhong et al., 2020). To our knowledge, no exist-\ning benchmarks contain both naturally occurring problems\nwith diverse contexts and reliable evaluation metrics.\nTo \ufb01ll this gap, we introduce DS-1000, a benchmark with a\nthousand problems covering seven widely-used Python data\nscience libraries: NumPy, Pandas, TensorFlow, PyTorch,\nSciPy, Scikit-learn, and Matplotlib.\nWe highlight\nthree core features of DS-1000: 1) it contains realistic\nproblems with diverse contexts, 2) it implements reliable\nmulti-criteria execution-based evaluation metrics, and 3) it\nproactively defends against memorization. We outline how\nwe achieved each of them below.\nFirst, we collected naturally occurring problems from Stack-\nOver\ufb02ow, manually scored their representativeness and use-\nfulness, and curated a subset of them to create our bench-\nmark. While inputs in existing code generation datasets\nare either highly structured (problems or code context) or\nrestricted in scope, our natural problems are diverse in con-\ntent and format. For example, users might search for more\narXiv:2211.11501v1  [cs.SE]  18 Nov 2022\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nresult = df.div(1).add_prefix(\"inv_\")\nPrompt\nReference Solution\nresult = df.join(df.apply(lambda x: 1/x).add_prefix(\u201cinv_\"))\nTest case 1\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nans = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6],\n            \"inv_A\": [1/1, 1/2, 1/3], \n             \"inv_B\": [1/4, 1/5, 1/6]})\nTest case 2\n df,ans = ...[omit for brevity]\n pd.testing.assert_frame_equal(result, ans)\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHere is a sample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name \nthem based on existing column names with a prefix, e.g. inv_A is an inverse of \ncolumn A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/1, \n1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\nObviously there are redundant methods like doing this in a loop, but there \nshould exist much more pythonic ways of doing it \u2026 [omitted for brevity]\nPredict\nCorrect/wrong?\nLanguage Models (GPT-3 Codex)\nReplace [insert] in the code context with \nfollowing predicted code snippets\nProblem\nCode Context\nExecute to evaluate\nMulti-criteria Execution-based Evaluation\nFigure 1: An example problem in DS-1000. The model needs to \ufb01ll in the code into \u201c[insert]\u201d in the prompt on the left; the\ncode will then be executed to pass the multi-criteria automatic evaluation, which includes the test cases and the surface-form\nconstraints; a reference solution is provided at the bottom left.\nef\ufb01cient code implementations (Figure 1), provide incorrect\ncode with an error message and ask for bug \ufb01xes (Figure 13),\ninquire about speci\ufb01c API usage (Figure 14), or ask for code\nthat implements functionality they specify with input-output\nexamples (Figure 1). These problems better re\ufb02ect real-\nworld applications and open up new modeling challenges,\nwhich have been understudied in existing code generation\nbenchmarks.\nSecond, it is challenging to evaluate program solutions to\nnatural and diverse problems reliably. Unlike competition-\nstyle problems, natural problems might lack executable con-\ntexts and test cases, allow multiple solutions, depend on\nexternal libraries, etc. To address these challenges, \ufb01ve of\nthe authors of this paper, all pro\ufb01cient in data science and\nexperts in Python, hand-adapted the original problems by\nwriting executable code contexts, rewriting problems to be\nspeci\ufb01c enough to be testable, and implementing automatic\nmulti-criteria execution-based evaluation using carefully\nwritten and reviewed test cases and constraints that check\nfunctional correctness and surface-form constraints. On\nprogram solutions predicted by Codex-002, we \ufb01nd that\nonly 1.8% of the predicted programs passing our evalua-\ntion are incorrect (false discovery rate), indicating that our\nevaluation is reliable.\nThird, one potential concern for adapting public problems\nis that the models might simply memorize the correspond-\ning solution during pre-training time (Carlini et al., 2021a).\nWe show in Section 2.4 that this can indeed happen: while\nCodex achieves 72.5% accuracy on the popular numpy-100\ndataset, the accuracy drastically drops to 23.6% after per-\nturbing them without increasing their dif\ufb01culty. Therefore,\nwhile building DS-1000, we proactively took measures\nagainst memorization by perturbing each problem.\nFigure 1 shows an example DS-1000 problem, its reference\nsolution, and an expert-written automatic multi-criteria eval-\nuation. To answer the problem, the model needs to \ufb01ll in\nthe solution; to pass our automatic evaluation, it needs to 1)\nreturn the correct output and 2) avoid inef\ufb01cient implemen-\ntations that use for-loops.\nWe use DS-1000 to evaluate several popular code generation\nmodels, including Codex (Chen et al., 2021a), CodeGen\n(Nijkamp et al., 2022), and InCoder (Fried et al., 2022).\nWe found model performance ranges from 7.4% to 43.3%,\nwith Codex-002 model being the best. This implies that\nthese models have the potential to reduce the barrier for data\nanalysis, yet still have large room for improvement.\n2. Benchmark Construction\nOur pipeline for building DS-1000 contains \ufb01ve stages, il-\nlustrated in Figure 2 and described below. 1) We scraped\nand selected high-quality problems from StackOver\ufb02ow\n(Section 2.1). 2) We rewrote the problem and the refer-\nence solution so that the problem is unambiguous and the\nreference solution is executable.(Section 2.2) 3) We im-\nplemented a multi-criteria automatic evaluation for each\nproblem, which includes test cases and surface-form con-\nstraints (Section 2.3). 4) We performed a pilot study which\nshows that Codex can answer problems by memorizing the\npre-training corpus, and proactively took measures to pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n inv_df = df.join(df.apply(lambda x: 1/x).add_prefix(\"inv_\"))\ndf = pd.DataFrame({\"A\": [1, 2, 3], \n  \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n# A known WRONG SOLUTION\nresult = df.join(df.apply(lambda \nx:math.e**x).add_prefix('exp_'))\n### END SOLUTION\nprint(result)\n\u2026 I'd like to apply the exponential function to each \nexisting column \u2026 The resulting dataframe should \nlook like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \n\"B\": [4, 5, 6],\n\"exp_A\": [e^1, e^2, e^3], \n\"exp_B\": [e^4, e^5, e^6]})\n \u2026 [omitted for brevity]\n\u2777 Adding Code Context\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],     \n \"B\": [4, 5, 6]})\n### BEGIN SOLUTION\n[insert]\n### END SOLUTION\nprint(result)\nTest cases\n\u2026[omit for brevity]\npd.testing.assert_frame_equal(result, \nans)\nSurface-form constraints\nfor and while should not appear in Syntax \nTree\n\u2778 Implementing Automatic Tests\n\u277a Red Teaming\n\u2779 Perturbing Original Problem \n\u2776 Manually Selecting and Modifying StackOverflow Problems\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nHere is a sample dataframe:\nI'd like to add inverses of each existing column to the dataframe \nand \u2026 [omitted for brevity]\ntry:\n High vote\n Testable\nRepresentative\n Useful\nFigure 2: The pipeline for building DS-1000. See the start of Section 2 for a detailed description.\nvent this by perturbing the problems and their reference\nsolutions in DS-1000 (Section 2.4). 5) We improved our\nmulti-criteria evaluation by requiring it to reject a small\nset of sample predictions that we considered incorrect via\nmanual review (Section 2.5), and then calculated the false\ndiscovery rate of our metric on a larger set of sample predic-\ntions. To reliably carry out this data collection procedure,\n\ufb01ve authors who are computer science students and familiar\nwith data science spent a total of about 1200 hours con-\nstructing DS-1000 (including steps from problem selection\nto quality review).\n2.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nTo obtain\nnatural and high-quality problems, we scraped data from\nStackOver\ufb02ow under each library tag (e.g., \u201cNumPy\u201d). To\nselect popular problems, we \ufb01rst removed duplicates and se-\nlected problems with at least 1 vote, 1000 views, that had an\naccepted answer. Next, we ranked problems based on votes\nand views and calibrated these statistics based on the time\na problem was created since older problems naturally have\nmore views and votes. We refer readers to Appendix A.1 for\nmore details. Among the \ufb01ltered problems, we randomly\nsampled an initial pool containing 4500 problems (1000 for\nNumPy, Pandas, and Matplotlib, 500 for Scikit-learn\nand SciPy, 250 for TensorFlow, and 250 for PyTorch).\nFiltering Suitable Problems.\nTo select problems from\nthe above pool for our benchmark, our annotators scored\neach problem according to the following rubric: whether a\nproblem a) contains input-output examples in the problem,\nb) is dif\ufb01cult to predict the solution for models according\nto the annotators\u2019 judgment, c) is practically useful, d) has\na clear description, and e) is feasible to evaluate the solu-\ntion. We aggregated these scores, reranked the candidate\nproblems, and incorporated the top-ranked ones to create\nDS-1000. We ended up with 451 unique StackOver\ufb02ow\nproblems. More than half of the original StackOver\ufb02ow\nproblems were \ufb01ltered out because they ask for an explana-\ntion for an algorithm or general content (see Appendix A.1).\nControlling Library Version.\nData science libraries\nare continuously evolving.\nAs a result, the semantics\nof the problem is determined not only by the language\ndescription but also by the software environment (e.g.,\nlibrary version).\nFor example, the same code snippet,\ntf.math.reciprocal(A), is only valid in the newer ver-\nsion of TensorFlow. We \ufb01xed the evaluation environment\nto include the latest versions of libraries that can be installed\nwith Python 3.7.10 and present the detailed documentation\nin Appendix A.1.\n2.2. Rewriting Problems and Reference Solutions\nCreating Executable Context.\nTo implement an\nexecution-based evaluation for each natural language prob-\nlem, we needed to write an executable context. We \ufb01rst\nadded package imports and de\ufb01ned the variables described\nin the problem. For example, in Figure 2, we imported the\nPandas package and created the dataframe described in the\nproblem as part of the context. Second, we needed to specify\nthe desired behavior of the target program to be predicted.\nFor example, in Figure 2, a code generation model can in-\nfer from the context that the resulting dataframe should be\nnamed as result, rather than output.\nRewriting Matplotlib Problems.\nMany Matplotlib\nproblems on StackOver\ufb02ow clarify their problems with\nexample \ufb01gures, which, however, cannot be encoded by\ncurrent pre-trained code models. Therefore, we rewrote the\nStackOver\ufb02ow problems in symbols (i.e., code and text)\nand adopted a different format from other libraries (see\nFigure 15).\nCollecting Reference Solutions. Finally, we obtained the\nreference solution for each problem from multiple high-\nvote replies, edited all reference solutions to be executable\ngiven the context we provided, and \ufb01xed errors whenever\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPerturbation\nCategories\nExample\nSurface\nConvert to completing function\nFigure 16, change format of code context\nParaphrase the description of the problem\nFigure 17, express the same problem in different words\nChange the example input and output\nFigure 18, replace this example with a longer one\nSemantic\nReplace keywords with analogy words\nFigure 19, replace \u201cinv\u201d with \u201cexp\u201d\nChange the required index\nFigure 20, need the speci\ufb01ed rows and columns\nReverse the order of the list, string or dataframe\nFigure 21, reverse the needed string\nChange the type of the required result\nFigure 22, change the DataFrame to a Series\nDif\ufb01cult Rewrite\nCombining several surface and semantic perturbations\nFigure 23, change examples and replace \u201chighest\u201d with \u201clowest\u201d\nDigging more perturbations that increase the dif\ufb01culty\nFigure 24, hypothesis testing\nTable 1: The perturbation categories along with examples. \u201cSurface\u201d perturbations do not change the reference solution,\nwhile \u201cSemantic\u201d perturbations do.\nwe noticed them (e.g., Figure 11). Even though we did not\nuse the reference solution in DS-1000 for evaluation, we\nprovided them in DS-1000 to facilitate future research.\n2.3. Implementing Multi-Criteria Evaluations\nOur automatic evaluation is multi-criteria, checking both\nfunctional correctness and surface-form constraints.\nFunctional Correctness.\nTo evaluate functional correct-\nness, we constructed test cases by converting the input-\noutput examples provided in the StackOver\ufb02ow problem;\nthen the expert annotators manually wrote additional test\ncases to improve the evaluation. To evaluate a predicted\nprogram, we execute it on the test inputs and compare the\noutputs with the ground truth.\nHowever, checking the exact equivalence of outputs can in-\nadvertently reject correct programs. Many problems involve\n\ufb02oating point arithmetic, and many return values are accept-\nable since they are close to the ground truth answer, but\nthey are not exactly equal. Some problems require random\noutputs, e.g., generating 100 samples from a distribution,\nand even executing the reference solution twice can lead to\ndifferent results. Many problems do not fully specify all the\nparameters, e.g., the color scheme for the output \ufb01gure in\nthe Matplotlib library, or the hyper-parameters of a learn-\ning algorithm in Scikit-learn; therefore, programs with\ndifferent parameters can satisfy the requirement, returning\nvalues that are different. In all these cases, we relied on the\nbest judgment of our expert annotators to implement the\nmetric for each problem, which sometimes involves com-\nplicated techniques, such as using statistical tests to handle\nrandomness. See more examples in Appendix A.2.\nSurface-Form Constraints. Functional correctness alone\nis insuf\ufb01cient. For example, vectorized operations can be\nexpanded using for-loops, which, however, are inef\ufb01cient\nand do not meet the requirement of the problem. Therefore,\nwe introduced additional surface-form constraints that re-\nquire the presence/absence of speci\ufb01c APIs for keywords.\nNotably, such a check is different from the standard surface-\nform metrics such as CodeBLEU (Ren et al., 2020), which\nrequires the whole model prediction to be uniformly similar\nto a reference solution; instead, DS-1000 precisely targets\nsmall but important parts of surface form.\n2.4. Perturbation to Defend Against Memorization\nMany models are pre-trained on web text and hence memo-\nrize its content (Elangovan et al., 2021; Carlini et al., 2021b);\ntherefore, they might answer our problems correctly by\nsimply recalling the solutions seen during pre-training if\nthey were trained on StackOver\ufb02ow or derivative sites. We\ndemonstrate this effect on numpy-100, 1 a problem set of\n100 NumPy problems with solutions that are copied several\nthousand times on GitHub. When prompted to answer a\nselected subset of 20 problems, Codex-002 achieves 72.5%\npass@1 accuracy.2\nHowever, if the model truly knows how to solve those prob-\nlems, it should be able to solve similar problems at the\nsame level of dif\ufb01culty. This motivates us to perturb the\nproblems in two ways: surface perturbations and semantic\nperturbations. For surface perturbations, we paraphrased\nthe problem or modi\ufb01ed the code context in the problem,\nbut the reference solution should stay the same after the\nperturbation; for example, changing from \u201cCreate a 5x5\nmatrix . . . \u201d to \u201cI need a matrix sized 5x5 . . . \u201d. For semantic\nperturbations, we changed the semantics of the reference\nsolution without changing its dif\ufb01culty ; for example, ask-\ning for \u201cmin\u201d instead of \u201cmax\u201d in the problem. We provide\nmore detailed categories in Table 1. In all of these cases, the\ndif\ufb01culty of the problem does not change for humans.\nOrigin\nSurface\nSemantic\nAvg. Perturbation\n72.5\n50.8\n23.6\n40.6\nTable 2: The performance of Codex-002 on numpy-100.\n1https://github.com/rougier/numpy-100\n2The fraction of Codex-002 samples that are correct.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nPandas\nNumPy\nMatplotlib\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nTotal/Avg.\nProblem\n291\n220\n155\n115\n106\n45\n68\n1000\nOrigin\n100\n97\n111\n46\n58\n17\n22\n451\nSurface Perturbation\n24\n22\n0\n57\n11\n11\n27\n152\nSemantic Perturbation\n88\n51\n44\n9\n20\n12\n11\n235\nDif\ufb01cult Rewrite\n79\n50\n0\n3\n17\n5\n8\n162\n% Surface-Form Constraints\n12.0\n36.4\n0\n27.8\n17.9\n20.0\n27.9\n19.4\nAvg. Test Cases\n1.7\n2.0\n1.0\n1.5\n1.6\n1.6\n1.7\n1.6\nAvg. Problem Words\n184.8\n137.5\n21.1\n147.3\n192.4\n133.3\n133.4\n140.0\nAvg. Lines of Code Context\n9.0\n8.3\n6.9\n11.0\n10.2\n9.2\n9.0\n8.9\nAvg. Lines of Code Solution\n5.4\n2.5\n3.0\n3.3\n3.1\n4.1\n2.1\n3.6\nTable 3: Detailed statistics of DS-1000.\nWe manually applied these perturbations to numpy-100 and\nshow the result on Table 2. Although the dif\ufb01culty level\nremains the same to human users, the performance of Codex-\n002 drops to 40.6% after perturbation (50.8% on surface per-\nturbations and 23.6% on semantic perturbations). Further-\nmore, in 36% of the cases, the model still predicted the orig-\ninal answer of the problem after the semantic perturbation,\nimplying that the model is solving the original problems by\nmemorizing their corresponding solutions. Therefore, we\ncould signi\ufb01cantly overestimate model performance if we\ntest them on problems directly taken from the web. (See\nAppendix B for more details)\nTherefore, to proactively prevent memorization, we applied\nthe above two perturbations to DS-1000 problems. Per-\nturbation is a labor-intensive process. Even for a simple\nperturbation from min to max, our annotators needed to edit\nall mentions of min, smallest, minimum to make the problem\ncoherent, and updated the code context, reference solution,\nand our evaluation metric accordingly.\nFinally, to make DS-1000 more challenging, we additionally\nintroduced several semantic perturbations that increase the\ndif\ufb01culty on purpose (\u201cDif\ufb01cult Rewrite\u201d in Table 1).\n2.5. Quality Assurance\nTo ensure the quality of our benchmark, each problem, refer-\nence solution, and automatic multi-criteria evaluation were\nreviewed by at least three expert annotators familiar with the\nlibrary. Additionally, we \u201cred teamed\u201d our automatic evalua-\ntion by requiring it to reject all programs known to be incor-\nrect, e.g., solutions to semantically perturbed problems (see\nFigure 2). After the quality review, we also quantitatively\nmeasured the evaluation quality by examining whether our\nmulti-criteria automatic metric can reject incorrect Codex-\n002 predictions (more details in Section 3).\n3. Dataset Statistics\nWe provide detailed dataset statistics in Table 3. DS-1000\ncontains 1000 problems originating from 451 unique Stack-\nOver\ufb02ow problems. To defend against potential memoriza-\ntion, more than half of the DS-1000 problems are modi\ufb01ed\nfrom the original StackOver\ufb02ow problems (Section 2.4);\nthey include 152 surface perturbations, 235 semantic pertur-\nbations, and 162 dif\ufb01cult rewrites.\nDS-1000 has carefully designed testing methods, checking\nboth execution semantics and surface-form constraints. For\neach problem, there are 1.6 test cases (manually annotated\ncorner test cases) on average, and 19.4% of them are accom-\npanied by surface-form constraints. The average of problem\nwords in DS-1000 is 140. On average, the reference solution\ncontains 3.6 lines of code. Table 3 shows the library break-\ndown statistics: Most libraries have a similar distribution\nexcept Matplotlib because we adopted a different problem\nformat due to its multimodal nature.\nTable 4 compares DS-1000 to other datasets.\nNotably,\nthe average number of words per problem in DS-1000 is\nmuch larger than other data science related datasets (e.g.,\nDSP, Chandel et al. 2022a and CoNaLa, Yin et al. 2018).\nMore importantly, the problems in DS-1000 represent more\ndiverse and naturalistic intent and context formats that can-\nnot be seen in any other datasets. Unlike generic Python\ncode generation benchmarks (MBPP, Austin et al. 2021 and\nHumanEval, Chen et al. 2021a), we note that data science\ncode generation benchmarks have fewer test cases since\nthe annotators need to de\ufb01ne program inputs with complex\nobjects such as square matrices, classi\ufb01ers, or dataframes\nrather than simple primitives, such as \ufb02oats or lists. Never-\ntheless, as we will show next, even a few test cases suf\ufb01ce\nfor DS-1000.\nWe evaluate our multi-criteria automatic metric by check-\ning whether it can reject incorrect solutions. We randomly\nsampled 10 problems from each library and sampled 40 pre-\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nDataset\nProblems\nEvaluation\nAvg. Test Cases\nAvg. P Words\nAvg. Lines of Code Solution\nData Source\nHumanEval\n164\nTest Cases\n7.7\n23.0\n6.3\nHand-Written\nMBPP\n974\nTest Cases\n3.0\n15.7\n6.7\nHand-Written\nAPPS\n10000\nTest Cases\n13.2\n293.2\n18.0\nCompetitions\nJuICe\n1981\nExact Match + BLEU\n-\n57.2\n3.3\nNotebooks\nDSP\n1119\nTest Cases\n2.1\n71.9\n4.5\nNotebooks\nCoNaLa\n2879\nBLEU\n-\n13.8\n1.1\nStackOver\ufb02ow\nDS-1000\n1000\nTest Cases +\nSurface-Form Constraints\n1.6\n140.0\n3.6\nStackOver\ufb02ow\nTable 4: Comparison of DS-1000 to other benchmarks. The \ufb01rst three benchmarks target general Python usage and the\nnext three involve data science code generation. DS-1000 adapts realistic problems from StackOver\ufb02ow and checks both\nexecution semantics and surface-form constraints.\ndictions from Codex-002 for each problem (2800 problem-\ncode examples in total).3 We run our automatic metric on\nall the sample predictions, review the predictions manually,\ncalculate how often they disagree, and report the following\nfour quantities:\n\u2022 Sample Level False Discovery Rate: among all pre-\ndicted samples that pass our automatic evaluation,\n1.8% of them are incorrect according to our annotator.\n\u2022 Sample Level False Omission Rate: among all pre-\ndicted samples that do not pass our automatic eval-\nuation, 0.5% of them are correct according to our\nannotator.\n\u2022 Problem Level False Positive Percentage: among all\nproblems, 5.7% of the problems contain at least one\nincorrect sample prediction that pass our automatic\nmetric.\n\u2022 Problem Level False Negative Percentage: among all\nproblems, 5.7% (it happens to be the same as the above)\nproblems contain at least one correct sample prediction\nthat fails to pass our automatic metric.\nGenerally, problem-level measures are especially stringent\nsince they require correctly judging all predictions among\nthe 40 sample predictions. While an apple-to-apple com-\nparison with other datasets is not possible due to the dif-\nference in the underlying model and benchmark construc-\ntion method (as a point of reference, Li et al. (2022) \ufb01nd\nthe problem Level False Positive Percentage to be 60% on\nAPPS (Hendrycks et al., 2021)), these measures re\ufb02ect that\nDS-1000 is reliable.4\n3We use a higher temperature of 0.7 compared with 0.2 in\nSection 4.2 to get more diverse predictions.\n4Some problems in APPS might apply quite similar tests, and\nsome problems may have even as few as 2 or 3 test cases in the\ntest split. Thus, insuf\ufb01cient test coverage probably happens though\nthere are more test cases in average (Li et al., 2022).\n4. Benchmarking State-of-the-Art Models\nWe used DS-1000 to benchmark \ufb01ve pre-trained code mod-\nels from three different families. The best model Codex-002\nInsertion achieves 43.3% accuracy, indicating room for im-\nprovement. We also show the results on the perturbed and\nunperturbed examples in Section 4.4.\n4.1. Prompt Format\nWe provide an of\ufb01cial prompt format in DS-1000 because\nit signi\ufb01cantly impacts the performance of pre-trained lan-\nguage models (Zhao et al., 2021). Figure 1 shows an exam-\nple: each prompt starts with a natural language description\nand then provides a code context; the code context uses\nHTML-like markers to indicate the location of missing code\nthat a model needs to \ufb01ll in and provides both left and the\nright context to the missing code pieces.\nWe decide to use in\ufb01lling as our of\ufb01cial format because\nthe right context is important to specify the behavior of\nthe program predictions (e.g., the variable name for the re-\nsult). More broadly, given that 1) in\ufb01lling is an important\nfunctionality for real-world programming and 2) there is a\ngrowing trend in pre-training with the right context (Agha-\njanyan et al., 2022; Fried et al., 2022; Bavarian et al., 2022;\nTay et al., 2022), we expect more future pre-trained models\nto perform in\ufb01lling.\nOn the other hand, given that many current language mod-\nels trained on code are not yet capable of in\ufb01lling, we also\nprovide an of\ufb01cial prompt that transfers the right context\ninformation into the left context (Figure 25 and 26). Nev-\nertheless, despite our best effort to design the prompts for\nleft-to-right models, they still lag behind models with in\ufb01ll-\ning capabilities (Section 4.3). We conjecture that in\ufb01lling\nmodels are inherently more effective at utilizing the right\ncontext information. Finally, we only have Completion\nformat for Matplotlib problems because Matplotlib pro-\nvides global access to the current \ufb01gure so the right context\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nis not necessary.\nFrom now on, we refer to the in\ufb01lling prompt format as\nInsertion format and the left-context-only format as Com-\npletion format.\n4.2. Experimental Setup\nModels. We experiment with three families of pre-trained\nmodels: Codex, InCoder (Fried et al., 2022), and Code-\nGen (Nijkamp et al., 2022). For the Codex models, we\nexperiment with codex-davinci-002 (Codex-002), codex-\ndavinci-001 (Codex-001), and codex-cushman-001 (Codex-\nCushman). For InCoder and CodeGen, we experiment with\nthe 6B parameters models. Among these models, Codex\nand CodeGen models are trained to predict the right context\nwhile InCoder models are trained for both left-to-right gen-\neration and in\ufb01lling. In addition, Codex-002 also supports\nin\ufb01lling, although the exact model training details are not\ndisclosed.\nImplementation Details. We generate 40 samples for each\nDS-1000 problem with temperature set to 0.2, top-p cutoff\nset to 0.95, and max generation length set to 1024. We set\nthe stop sequence tokens to \u201c</code>\u201d and \u201c# SOLUTION\nEND\u201d. These samples are used in the unbiased estimator of\npass@1. For DS-1000, evaluating generated codes does not\nrequire special computational resources like GPUs.\n4.3. Main Results\nTable 5 displays the pass@1 accuracy on DS-1000. We \ufb01nd\nthat DS-1000 can differentiate models with different capa-\nbilities. The best model Codex-002 achieves a nontrivial\nbut far-from-perfect average accuracy of 43.3%, indicating\nsubstantial room for improvement. In contrast, other models\nlike CodeGen-6B or InCoder-6B have much worse overall\nperformance, with accuracy lower than 5% on some libraries.\nQualitatively, these smaller models often cannot correctly\nfollow the prompt instruction, generating additional com-\nments instead of the required code. Future ablation is needed\nto understand the underlying cause for this performance gap,\nwhich could be the difference in model size, lack of instruc-\ntion tuning, or the difference in pre-training data.\nIn addition, we observe that model accuracy varies across\ndifferent libraries. This speaks to the importance of a holis-\ntic evaluation of multiple data science libraries because\nperformance in a speci\ufb01c library may not directly generalize\nto other libraries.\nMoreover, we \ufb01nd that Insertion format often leads to bet-\nter performance. The same Codex-002 model has a 4.1%\naverage accuracy improvement when used with Insertion\nformat than used with Completion format. This shows the\nimportance of the in\ufb01lling capability for data science code\ncompletion.\n4.4. Results by Perturbation\nIn Section 2.4, we demonstrated the risk of memorizing\nthe solutions on the numpy-100 problem set; do we observe\nthe same effect on DS-1000? To investigate this, we ap-\nplied surface perturbations (i.e., the problem changes but\nthe reference solution does not change) and semantic pertur-\nbations (the reference solution will change) to the problems\nin DS-1000.\nTable 6 shows the results.5 The performance of Codex-002\ndrops after perturbation (3.4% on surface perturbations and\n9.0% on semantic perturbations) but the drop is much less\nsevere than what we observed on numpy-100. This indi-\nrectly suggests that Codex-002 might have memorized the\nsolution for some StackOver\ufb02ow problems, but the effect is\nless severe because they have not been repeated as often as\nnumpy-100 on the internet. Still, we believe problem pertur-\nbation to be a useful strategy to defend against memorization\nby future models proactively.\nAdditionally, we rewrote some problems to create more DS-\n1000 problems by intentionally making them more dif\ufb01cult\neven for human programmers. As expected, Codex-002\nperforms much worse after the rewrite, and we plan to use\nthese problems as a challenge for future models.\nWe give a preliminary error analysis in Appendix C.\n5. Related Work\nNatural Language to Code. Research on translating natu-\nral language to executable forms dates back several decades.\nThe models have become increasingly capable of producing\ncomplex and general programs while requiring fewer human\nannotations. Zelle & Mooney (1996) and Zettlemoyer &\nCollins (2007) translate natural language queries to domain-\nspeci\ufb01c database queries. Liang et al. (2013) and Berant\net al. (2013) parse natural language into \ufb01rst-order logic to\nanswer generic knowledge-based questions. Yu et al. (2018);\nScholak et al. (2021) translate natural language problems to\ngeneral SQL programs and develop models that can general-\nize across domains. While all the works above still need to\ntrain their models on the task they evaluate, recently Li et al.\n(2022); Chen et al. (2021a) show that generative models pre-\ntrained on code can produce Python snippets to tackle com-\npetitive programming challenges, without any additional\nhuman annotations. Many other recent works corroborated\nthis \ufb01nding (Nijkamp et al., 2022; Fried et al., 2022; Xu\net al., 2022; Black et al., 2022), and additional techniques\nat inference time further improve the performance (Poesia\net al., 2022; Shi et al., 2022).\n5Note that the results are not comparable to Table 5 since for\neach kind of perturbation, we only selected a subset of problems\nto perturb.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nFormat\nModel\nPandas NumPy Matplotlib Scikit-learn SciPy TensorFlow PyTorch\nOverall\nLeft-to-right\nCompletion\nCodex-002\n26.5\n43.1\n57.0\n44.8\n31.8\n39.3\n41.8\n39.2\nCodex-001\n9.4\n26.6\n41.8\n18.5\n15.0\n17.2\n9.7\n20.2\nCodex-Cushman\n7.9\n21.8\n40.7\n18.0\n11.3\n12.2\n12.4\n18.1\nCodeGen-6B\n1.9\n12.1\n18.6\n5.8\n7.4\n12.8\n3.4\n8.4\nInCoder-6B\n3.1\n4.4\n28.3\n2.8\n2.8\n3.8\n4.4\n7.4\nInsertion\nCodex-002\n30.1\n46.5\n57.0*\n53.7\n34.8\n53.4\n47.7\n43.3\nInCoder-6B\n2.9\n4.6\n28.3*\n3.1\n3.1\n7.8\n3.2\n7.5\nTable 5: pass@1 accuracy with 40 samples generated for each problem. The upper part shows accuracy on the left-to-right\nCompletion format, while the lower part shows the results of Insertion format. The rightmost \u201cOverall\u201d columns show the\naverage accuracy on 1000 problems from all libraries. DS-1000 is able to differentiate the capabilities of different models\nand there is substantial room for improvement even for the best Codex-002 model. \u2217: Matplotlib problems do not have the\nright context so Completion and Insertion formats are the same.\nPandas\nNumPy\nScikit-learn\nSciPy\nTensorFlow\nPyTorch\nOverall\nOriginsurface\n37.3\n61.2\n52.6\n33.0\n64.9\n64.8\n53.2\nSurface\n31.9 \u22125.4\n58.4 \u22122.8\n55.7 +3.1\n32.1 \u22120.9\n58.0 \u22128.9\n50.0 \u221214.8\n49.8 \u22123.4\nOriginsemantic\n36.8\n56.7\n60.6*\n40.3\n71.3\n65.1\n47.2\nSemantic\n33.2 \u22123.6\n49.0 \u22127.7\n38.9*\u221221.7\n34.3 \u22126.0\n42.5 \u221225.8\n30.5 \u221234.6\n38.2 \u22129.0\nOrigindif\ufb01cult\n39.9\n52.7\n5.0*\n58.1\n73.0*\n53.8*\n46.8\nDif\ufb01cult Rewrite\n17.7 \u221222.2\n27.1 \u221225.6\n0.0*\u22125.0\n13.8 \u221244.3\n38.0*\u221235.0\n28.8*\u221225.0\n21.0 \u221225.8\nTable 6: Effect of three different types of problem perturbation. In each subsection, we compare the accuracy of the\nperturbed problems to that of the original problems. We observe that although Surface and Semantic perturbations also\ncause a performance drop on DS-1000 the performance drop is much smaller compared to that on numpy-100. \u2217: Numbers\nare averaged from less than 10 problems.\nCode Generation Benchmarks.\nAs models become in-\ncreasingly capable, researchers start to build increasingly\ndif\ufb01cult and general code generation benchmarks. While\nZelle & Mooney (1996) focused only on domain-speci\ufb01c\nlanguages, Yu et al. (2018) builds a Text-to-SQL benchmark\nthat evaluates the capability to write broad-domain SQL\nprograms. Yin et al. (2018) evaluates the capability to write\nshort but general Python snippets, while more recent papers\nHendrycks et al. (2021); Li et al. (2022) evaluate models\u2019\ncapability to solve competitive programming problems in\nPython. If code generation models continue to improve, we\nexpect future researchers to focus on more complex tasks.\nAt the same time, however, it becomes more dif\ufb01cult to build\nreliable benchmarks aligned with real-world applications.\nPrograms are most useful when they are executed; therefore,\nwe need to evaluate their execution semantics, and the best\ngeneral method so far is still to ask experts to manually write\ntest cases. Consequently, most benchmarks with test cases\nfocus on competition/interview/ programming challenges\n(Hendrycks et al., 2021; Li et al., 2022), because these are\nthe only applications where a lot of test cases are already\navailable. Therefore, most recent papers that evaluate on\nreal-world programs have to rely on unreliable surface-form\nmetrics (Ren et al., 2020; Chen et al., 2021b; Xu et al., 2022).\nThis streetlight effect might incentivize the community to\nwork on problems that are easy to evaluate but not useful\nin practice. In response to this challenge, our paper man-\nually implements a reliable metric for naturally occurring\nproblems. Future works can consider using models to help\nhumans write useful tests (Tufano et al., 2020), or formally\nverify the correctness of a predicted solution (Chu et al.,\n2017).\n6. Conclusion\nWe propose DS-1000, a benchmark for generating code for\ndata analysis. Our benchmark 1) contains realistic problems,\n2) implements reliable automatic metrics, and 3) proactively\ndefends against memorization strategies. We hope DS-1000\ncan track the progress of this research area and facilitate fair\ncomparisons between models, and our methods to construct\nit can inspire other areas where the task is complicated and\nthe ground truth is challenging to evaluate.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAcknowledgements\nWe thank Noah A. Smith, Tianbao Xie, Shuyang Jiang for\ntheir helpful feedback on this work.\nReferences\nAgashe, R., Iyer, S., and Zettlemoyer, L.\nJuICe: A\nlarge scale distantly supervised dataset for open domain\ncontext-based code generation. In Empirical Methods in\nNatural Language Processing (EMNLP), 2019.\nAghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu,\nH., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis,\nM., et al. Cm3: A causal masked multimodal model of\nthe internet. arXiv preprint arXiv:2201.07520, 2022.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski,\nH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey,\nC., Tworek, J., and Chen, M.\nEf\ufb01cient training of\nlanguage models to \ufb01ll in the middle. arXiv preprint\narXiv:2207.14255, 2022.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic\nparsing on freebase from question-answer pairs. In Pro-\nceedings of the 2013 conference on empirical methods in\nnatural language processing, pp. 1533\u20131544, 2013.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B: An\nopen-source autoregressive language model. In Proceed-\nings of BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models, vir-\ntual+Dublin, May 2022. Association for Computational\nLinguistics.\nBolyen, E., Rideout, J. R., Dillon, M. R., Bokulich, N. A.,\nAbnet, C. C., Al-Ghalith, G. A., Alexander, H., Alm, E. J.,\nArumugam, M., et al. Reproducible, interactive, scalable\nand extensible microbiome data science using qiime 2\n(vol 37, pg 852, 2019). Nature biotechnology, 2019.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M.,\nHerbert-Voss, A., Lee, K., Roberts, A., Brown, T.,\nSong, D., Erlingsson, \u00da., Oprea, A., and Raffel, C.\nExtracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pp. 2633\u20132650. USENIX Association, August\n2021a.\nISBN 978-1-939133-24-3.\nURL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nCarlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, \u00da., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In Bailey, M.\nand Greenstadt, R. (eds.), 30th USENIX Security Sympo-\nsium, USENIX Security 2021, August 11-13, 2021, pp.\n2633\u20132650. USENIX Association, 2021b. URL https:\n//www.usenix.org/conference/usenixsecurity21/\npresentation/carlini-extracting.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. CoRR, abs/2201.12901, 2022a. URL https:\n//arxiv.org/abs/2201.12901.\nChandel, S., Clement, C. B., Serrato, G., and Sundaresan, N.\nTraining and evaluating a jupyter notebook data science\nassistant. arXiv preprint arXiv:2201.12901, 2022b.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021a.\nChen, X., Gong, L., Cheung, A., and Song, D. Plotcoder:\nHierarchical decoding for synthesizing visualization code\nin programmatic context. In Association for Computa-\ntional Linguistics (ACL), 2021b.\nChu, S., Wang, C., Weitz, K., and Cheung, A. Cosette: An\nautomated prover for sql. In CIDR, 2017.\nElangovan, A., He, J., and Verspoor, K. Memorization\nvs. generalization : Quantifying data leakage in NLP\nperformance evaluation. In Merlo, P., Tiedemann, J.,\nand Tsarfaty, R. (eds.), Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, EACL\n2021, Online, April 19 - 23, 2021, pp. 1325\u20131335. As-\nsociation for Computational Linguistics, 2021.\ndoi:\n10.18653/v1/2021.eacl-main.113. URL https://doi.\norg/10.18653/v1/2021.eacl-main.113.\nFaghmous, J. H. and Kumar, V. A big data guide to under-\nstanding climate change: The case for theory-guided data\nscience. Big data, 2(3):155\u2013163, 2014.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E.,\nShi, F., Zhong, R., Yih, W., Zettlemoyer, L., and Lewis,\nM. Incoder: A generative model for code in\ufb01lling and\nsynthesis. CoRR, abs/2204.05999, 2022.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,\nA., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and\nSteinhardt, J. Measuring coding challenge competence\nwith apps. NeurIPS, 2021.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nLi, Y., Choi, D. H., Chung, J., Kushman, N., Schrit-\ntwieser, J., Leblond, R., Eccles, T., Keeling, J., Gi-\nmeno, F., Lago, A. D., Hubert, T., Choy, P., de Mas-\nson d\u2019Autume, C., Babuschkin, I., Chen, X., Huang,\nP., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J.,\nMankowitz, D. J., Robson, E. S., Kohli, P., de Freitas,\nN., Kavukcuoglu, K., and Vinyals, O. Competition-level\ncode generation with alphacode. CoRR, abs/2203.07814,\n2022. doi: 10.48550/arXiv.2203.07814. URL https:\n//doi.org/10.48550/arXiv.2203.07814.\nLiang, P., Jordan, M. I., and Klein, D. Learning Dependency-\nBased Compositional Semantics. Computational Linguis-\ntics, 39(2):389\u2013446, 06 2013. ISSN 0891-2017. doi:\n10.1162/COLI_a_00127. URL https://doi.org/10.\n1162/COLI_a_00127.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou,\nY., Savarese, S., and Xiong, C. A conversational paradigm\nfor program synthesis. CoRR, abs/2203.13474, 2022.\nPoesia, G., Polozov, A., Le, V., Tiwari, A., Soares, G., Meek,\nC., and Gulwani, S. Synchromesh: Reliable code genera-\ntion from pre-trained language models. In International\nConference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=KmtVD97J43e.\nRen, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sun-\ndaresan, N., Zhou, M., Blanco, A., and Ma, S. Code-\nbleu: a method for automatic evaluation of code syn-\nthesis.\nCoRR, abs/2009.10297, 2020.\nURL https:\n//arxiv.org/abs/2009.10297.\nRomero, C. and Ventura, S. Data mining in education. Wiley\nInterdisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 3(1):12\u201327, 2013.\nScholak, T., Schucher, N., and Bahdanau, D. PICARD:\nParsing incrementally for constrained auto-regressive de-\ncoding from language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, pp. 9895\u20139901, Online and Punta Cana, Do-\nminican Republic, November 2021. Association for Com-\nputational Linguistics.\nShi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and\nWang, S. I. Natural language to code translation with\nexecution. arXiv preprint arXiv:2204.11454, 2022.\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\nUnifying language learning paradigms. arXiv preprint\narXiv:2205.05131, 2022.\nTufano, M., Drain, D., Svyatkovskiy, A., Deng, S. K., and\nSundaresan, N. Unit test case generation with transform-\ners and focal context. arXiv preprint arXiv:2009.05617,\n2020.\nXu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A\nsystematic evaluation of large language models of code.\nIn Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pp. 1\u201310, 2022.\nYin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig,\nG. Learning to mine aligned code and natural language\npairs from stack over\ufb02ow. In International Conference on\nMining Software Repositories, MSR, pp. 476\u2013486. ACM,\n2018. doi: https://doi.org/10.1145/3196398.3196408.\nYu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z.,\nMa, J., Li, I., Yao, Q., Roman, S., Zhang, Z., and Radev,\nD. R. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-\nSQL task. In Empirical Methods in Natural Language\nProcessing (EMNLP), 2018.\nZelle, M. and Mooney, R. J. Learning to parse database\nqueries using inductive logic programming. In Associa-\ntion for the Advancement of Arti\ufb01cial Intelligence (AAAI),\npp. 1050\u20131055, 1996.\nZettlemoyer, L. and Collins, M. Online learning of relaxed\nccg grammars for parsing to logical form. In Proceedings\nof the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natu-\nral Language Learning (EMNLP-CoNLL), pp. 678\u2013687,\n2007.\nZhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.\nCalibrate before use: Improving few-shot performance\nof language models.\nIn International Conference on\nMachine Learning, pp. 12697\u201312706. PMLR, 2021.\nZhong, R., Yu, T., and Klein, D. Semantic evaluation for\ntext-to-SQL with distilled test suites. In Proceedings\nof the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 396\u2013411, On-\nline, November 2020. Association for Computational Lin-\nguistics. doi: 10.18653/v1/2020.emnlp-main.29. URL\nhttps://aclanthology.org/2020.emnlp-main.29.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nAppendices\nA. Details on Data Collection\nA.1. Problem Selection\nSourcing Popular StackOver\ufb02ow Problems.\nWe lever-\nage StackOver\ufb02ow to collect representative data science\ncode generation problems on each library. To select popular\nproblems, we \ufb01rst removed duplicates and selected prob-\nlems with at least 1 vote, 1000 views, and accepted answers.\nAfter this initial \ufb01ltering, we obtain 15881 NumPy problems,\n26248 Pandas problems, 1965 PyTorch problems, 8258\nTensorFlow problems, 4141 SciPy problems, and 4499\nScikit-learn problems. Next, we performed a strati\ufb01ed\nsampling on problems from each year to further subsample\nthe problems from Pandas and TensorFlow. We designed a\nthreshold for each year\u2019s problems differently because older\nproblems naturally have higher votes. Table 8 displays the\ncriteria we used to \ufb01lter each year\u2019s problem on Pandas and\nTensorFlow.\nFiltering Suitable Problems.\nFrom the initial pool of\npopular problems, our annotators selected problems that\nare suitable for building DS-1000. Besides the considera-\ntions mentioned in Section 2, we discuss those problems\nthat are not selected here. In general, we consider a prob-\nlem to be unsuitable if our multi-criteria evaluation is not\napplicable (untestable problems). For example, we leaved\nStackOver\ufb02ow problems involving hardware problems (See\nFigure 29), software errors (See Figure 30), concrete exe-\ncution time analysis, etc. out of DS-1000. See Figure 31\nfor a concrete example where the problem asks for a natural\nlanguage explanation of a method in TensorFlow. We leave\nincorporating more unsuitable StackOver\ufb02ow problems for\nfuture work.\nControlling Library Version. Table 7 details the software\nversions that we build DS-1000 with.\nPackage\nVersion\nSeaborn\n0.11.2\nMatplotlib\n3.5.2\nNumPy\n1.21.6\nPandas\n1.3.5\nScikit-learn\n1.0.2\nSciPy\n1.7.3\nTensorFlow\n2.10.0\nPyTorch\n1.12.1\nTable 7: The versions of software in DS-1000\nA.2. Example Problems\nHere we present an example problem from each of the seven\nlibraries in DS-1000 to illustrate the challenges we encoun-\ntered in creating DS-1000.\nFigure 9 shows a NumPy problem asking how to generate\nsamples that suit log-uniform distribution. Since the result\nvaries with different solutions and different settings, it\u2019s\nunreasonable to test the equivalence. Instead, we apply the\nKolmogorov-Smirnov test that judges whether two groups\nof samples suit the identical or rather similar population.\nFigure 10 gives a SciPy problem that describes some trou-\nble with the number of stored elements in a sparse matrix\nand asks for a solution without repetitive type conversion.\nSince our self-made assertion that checks the equivalence\nof two matrices cannot distinguish the difference between\nstored numbers, we need a special design for this problem.\nFor functional correctness, we check the type of b, match the\nelements, and check the number of non-zero elements(nnz),\nwhich is the core of the problem. For surface-form con-\nstraints, we reject the use of .toarray(), .A, .todense(),\nand .array(), which might attempt to transform a sparse\nmatrix into a dense one.\nFigure 11 shows a Pandas problem. We found that the\nsolution with the highest votes ignores the requirement \u201cbut\ndoes not exactly match it\u201d in the description of the problem,\nand thus we had to \ufb01x the bug in our reference solution.\nBesides, we enhanced the test case to check the point.\nFigure 12 shows a TensorFlow problem. Since there is no\nbuilt-in testing function de\ufb01ned in TensorFlow 2.10.0, we\nhad to design it by ourselves.\nFigure 13 demonstrates a PyTorch problem. Here we use\nload_data() to hide the input and let the models learn from\nthe description. The correct solution is not a regular type\nconversion, as indicated in the error message.\nFigure 14 shows a Scikit-learn problem. It requires ap-\nplying the preprocessing method de\ufb01ned in Scikit-learn\nto a Pandas dataframe, and it tests whether the models\nlearn Scikit-learn, Pandas, and their interaction well.\nActually, these data science libraries are not independent of\nothers, and this problem exempli\ufb01es the interactions.\nFigure 15 shows a Matplotlib problem. Here the origi-\nnal problem on StackOver\ufb02ow contains an example \ufb01gure,\nwhich cannot be processed by current code models. We\nrewrite the original problem into a standalone problem, that\nis, \u201cPlot y over x and show blue dashed grid lines\u201d. The\nautomatic evaluation comes in two parts. First, it compares\nthe image produced by the generated program with the im-\nage produced by the reference program. If two images\nmatch exactly, then the generated program is considered\ncorrect. Otherwise, the automatic evaluation examines the\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nYear\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\nPandas\nvote\n50\n50\n14\n14\n14\n4\n4\n4\n2\n2\n1\n1\nview\n5k\n5k\n5k\n5k\n5k\n1k\n1k\n1k\n1.1k\n1.1k\n1k\n1k\nproblems\n2\n8\n467\n494\n554\n2139\n2483\n1894\n1985\n809\n225\n8\nTensorFlow\nvote\n-\n-\n-\n-\n10\n5\n4\n2\n2\n1\n1\n1\nview\n-\n-\n-\n-\n3k\n2k\n1k\n1.6k\n1.2k\n1.3k\n1k\n1k\nproblems\n-\n-\n-\n-\n100\n632\n1136\n1167\n1004\n776\n185\n6\nTable 8: The problem selection parameters and the number of result problems of Pandas and TensorFlow.\nMatplotlib axis object and asserts the conditions relevant\nto the problem speci\ufb01cation. In this example, the assertions\nare testing the existence of grid lines and the color of the\ngrid lines.\nA.3. Problem Perturbation\nHere, we give an example for each type of perturbation,\nas shown in Table 1. We highlight the changes we made\nthrough perturbations.\nFigure 16, Figure 17 and Figure 18 give examples of surface\nperturbations, showing code context perturbation, paraphras-\ning, and changes in example respectively. The original task\nhasn\u2019t changed.\nFigure 19 shows how we replace keywords with analogy\nwords in a Pandas problem. The perturbed problem asks\nfor applying an exponential function to column A and B.\nThe problem in Figure 20 concentrates on changing the re-\nquired index. Here we specify the target index on which\nto operate using ordinal numbers. Figure 21 gives an ex-\nample of reversing the order. The desired output string is\nreversed(from \u201cabc,def,ghi,jkl\u201d to \u201cjkl,ghi,def,abc\u201d). We\nexpect the models to capture the information and handle the\nperturbation. Figure 22 shows an example of changing the\ntype of the required result. Here we change the type from\npd.DataFrame to pd.Series.\nFigure 23 and Figure 24 demonstrate how we get dif\ufb01cult\nrewrites. The example in Figure 23 replaces \u201chighest\u201d with\n\u201clowest\u201d and changes the shape of the desired output (from n\n\u00d7 1 to 1 \u00d7 n). The example in Figure 24, on the other hand,\nfocuses on digging more perturbations that could increase\nthe dif\ufb01culty. The models should not only learn how to use a\ntwo-sample KS test but also learn how to interpret the result\nof the KS test.\nA.4. Prompt Format\nAs we\u2019ve mentioned in Section 4.1, we also provide a\nprompt of Completion format. Here are two examples (Fig-\nure 25 and Figure 26) showing that we have to translate the\ncode in the right context into natural language instructions\nas complementary information.\nB. Details of Experiments on numpy-100\nnumpy-100 is a collection of 100 NumPy exercises from\nNumPy mailing list, StackOver\ufb02ow, and NumPy documen-\ntation, which has been forked over 4.7k times on GitHub.\nAs shown in Figure 3, in the numpy-100 problem set, each\nproblem is given a short, one-sentence description with no\ncode context, followed by a reference solution.\n#### 28. Consider a (6,7,8) shape array, what is the index (x,y,z) \nof the 100th element?\n```python\nprint(np.unravel_index(99, (6,7,8)))\n```\nFigure 3: A numpy-100 example.\nFirst, we wrote a code context for each problem and applied\nInsertion prompt, as shown in Figure 4.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 4: A numpy-100 example prompt.\nThen we paraphrased the problems and modi\ufb01ed the code\ncontexts as surface perturbations, as shown in Figure 5 and\nFigure 6. We changed the description from \u201cConsider a\n(6,7,8) shape array, what is the index (x,y,z) of the 100th\nelement?\u201d to \u201cI have an array with shape (6,7,8). I need to\n\ufb01nd the index of the 100th element.\u201d. In another way, we\nchanged the code context to require models to complete a\ngiven function.\nFor semantic perturbation, we changed the requirements\nof the problems and also the semantics of the reference\nsolutions without changing their dif\ufb01culty. As shown in\nFigure 7, we changed \u201c100\u201d to \u201c99\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI have a array with shape (6,7,8). I need to find the index of the 100th element.\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 5: A numpy-100 example of surface perturbation.\nWe expressed the same description in different words.\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?\n<code>\nimport numpy as np\ndef f():\n    [insert]\n    return result\n</code>\nFigure 6: A numpy-100 example of surface perturbation.\nWe changed the code context.\nAt last, we equipped each problem and its perturbation with\none test case and an automatic evaluation. Then we tested\nthe performance of Codex-002 on them. We sampled 20\nproblems from numpy-100 and generated 10 samples for\neach problem with temperature set to 0.7, and top-p cutoff\nset to 0.95.\nC. Error Analysis\nWe provide a preliminary error analysis by showing an exam-\nple model error in Figure 8 and provide additional examples\nin Figure 27 and 28. In this example, the problem asks for\nremoving adjacent duplicated non-zero values in a given\narray, which cannot be satis\ufb01ed by a single NumPy opera-\ntion. The reference implements this problem by creating a\nbinary array representing the selection and performing two\noperations to meet the problem requirement. However, we\nsee Codex-002 fails on the composite request and attempts\nto answer the problem with a single method, np.unique,\npointed out as incorrect in the problem already.. This exam-\nple error demonstrates the challenges in DS-1000 problems,\nProblem:\nConsider a (6,7,8) shape array, what is the index (x,y,z) of the 99th element?\n<code>\nimport numpy as np\n[insert]\nprint(result)\n</code>\nFigure 7: A numpy-100 example of semantic perturbation.\nWe only changed the required index.\nwhich require both natural language understanding and code\ngeneration abilities.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nGiven a numpy array, I wish to remove the adjacent \n(before removing) duplicate non-zero value and all the \nzero value.\nFor instance, for an array like that: \n[0,0,1,1,1,2,2,0,1,3,3,3], I'd like to transform it to: \n[1,2,1,3]. Do you know how to do it?\nI just know np.unique(arr) but it would remove all the \nduplicate value and keep the zero value. Thank you in \nadvance!\nReference Solution\n# a: 1-d np.array as input\nselection = np.ones(len(a), dtype = bool)\nselection[1:] = a[1:] != a[:-1]\nselection &= a != 0\nresult = a[selection]\nWrong Solution\n# Just mimic mentioned wrong solution\nresult = np.unique(a)\nFigure 8: An example model mistake. The problem speci\ufb01es a composite requirement, removing adjacent non-zero\nduplicates, which cannot be solved by a single operation. The model mistakenly generates a single operation that removes\nall duplicates.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\nimport spicy.stats\nresult = scipy.stats.loguniform.rvs(a = min, b = max, size = n)\nAutomatic Evaluation\nTest code\n np.testing.assert_array_equal(result.shape, ans.shape)\n from scipy.stats import ks_2samp\n # Kolmogorov-Smirnov Test judges whether the two sampled   \n # from similar distribution\n assert ks_2samp(result, ans)[0] <= 0.1\nSurface-form constraints\nfor and while should not appear in Syntax Tree\nTest case 1\n min = 1\n max = np.e\n n = 10000\n ans = ... # generated by Reference solution\nProblem:\nI could not find a built-in function in Python to generate a log uniform \ndistribution given a min and max value (the R equivalent is here), \nsomething like: loguni[n, min, max, base] that returns n log uniformly \ndistributed in the range min and max.\nThe closest I found though was numpy.random.uniform . \nThat is, given range of x, I want to get samples of given size (n) that suit log-\nuniform distribution. \nAny help would be appreciated!\nA:\n<code>\nimport numpy as np\nmin = 1\nmax = np.e\nn = 10000\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 9: NumPy example problem involving randomness, requiring the use of a specialist knowledge test.\nReference Solution\n  b.setdiag(0)\n  b.eliminate_zeros()\nAutomatic Evaluation\nTest code\nassert type(b) == type(ans)\n# Matching elements\nassert len(sparse.find(b != ans)[0]) == 0\n# Checking number of nonzero elements\nassert b.nnz == ans.nnz\nSurface-form constraints\n.toarray(), .A, .todense(), .array() should \nnot appear in Syntax Tree\nTest case 1\n a = np.ones((2, 2))\nTest case 2\n a = []\n ans = sparse.csr_matrix(a)\n ans.setdiag(0)\n ans.eliminate zeros() \nProblem:\nI want to remove diagonal elements from a sparse matrix. Since the matrix is sparse, \nthese elements shouldn't be stored once removed.\nScipy provides a method to set diagonal elements values: setdiag\n\u2026[omit for brevity]\nHowever with csr_matrix, it seems diagonal elements are not removed from storage:\n\u2026[omit for brevity]\n>>> b.setdiag(0)\n>>> b\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 4 stored elements in Compressed Sparse Row format>\n>>> b.toarray()\narray([[ 0.,  1.],\n       [ 1.,  0.]])\nThrough a dense array, we have of course:\n>>> csr_matrix(b.toarray())\n<2x2 sparse matrix of type '<type 'numpy.float64'>'\n    with 2 stored elements in Compressed Sparse Row format>\nIs that intended? If so, is it due to the compressed format of csr matrices? Is there any \nworkaround else than going from sparse to dense to sparse again?\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\na = np.ones((2, 2))\nb = sparse.csr_matrix(a)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(b)\n</code>\nFigure 10: An example problem of SciPy. Speci\ufb01c checking on conversion between dense matrix and sparse matrix.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nJust iterate over  DataFrame.columns , now this is an example in \nwhich you will end up with a list of column names that match: \nspike_cols = [col for col in df.columns if 'spike' in col] \nReference Solution\nresult = [col for col in df.columns \nif s in col and col != s]\nAutomatic Evaluation\nTest code\n assert result == ans\nTest case 1\n data = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n  'spiked-in': [7,8,9], 'no': [10,11,12], \n  'spike': [13,14,15]}\n df = pd.DataFrame(data)\n s = 'spike'\n ans = [col for col in df.columns \nif s in col and col != s]\nProblem:\nI have a dataframe with column names, and I want to find the one \nthat contains a certain string, but does not exactly match it. I'm \nsearching for 'spike' in column names like 'spike-2', 'hey spike', \n'spiked-in' (the 'spike' part is always continuous).\nI want the column name to be returned as a string or a variable, so \nI access the column later with df['name'] or df[name] as \nnormal. I want to get a list like['spike-2', 'spiked-in']. \nI've tried to find ways to do this, to no avail. Any tips? \nA:\n<code>\nimport pandas as pd\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], \n 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nHighest-vote Solution\nFigure 11: An example problem of Pandas. We need to write reference solutions by ourselves because high-vote replies\nfrom StackOver\ufb02ow ignore the requirement \u201cbut does not exactly match it\u201d.\nlengths_transposed = tf.expand_dims(lengths, 1)\nrange = tf.range(0, 8, 1)\nrange_row = tf.expand_dims(range, 0)\nmask = tf.less(range_row, lengths_transposed)\nresult = tf.where(mask, tf.ones([4, 8]), tf.zeros([4, 8]))\nReference Solution\nTest code\ndef tensor_equal(a, b): # self-made test function\n    if type(a) != type(b):\n        return False\n    if isinstance(a, type(tf.constant([]))) is not True:\n        if isinstance(a, type(tf.Variable([]))) is not True:\n            return False\n    if a.shape != b.shape:\n        return False\n    if a.dtype != tf.float32:\n        a = tf.cast(a, tf.float32)\n    if b.dtype != tf.float32:\n        b = tf.cast(b, tf.float32)\n    if not tf.reduce_min(tf.cast(a == b, dtype=tf.int32)):\n        return False\n    return True\nassert tensor_equal(result, ans)\nTest case 1\n lengths = [4, 3, 5, 2]\n ans = ... # generated by Reference solution\nTest case 2\n lengths, ans = ...[omitted for brevity]\nProblem:\nI'm using tensorflow 2.10.0.\nI have a tensor of lengths in tensorflow, let's say it looks like \nthis:\n[4, 3, 5, 2]\nI wish to create a mask of 1s and 0s whose number of 0s \ncorrespond to the entries to this tensor, padded in front by \n1s to a total length of 8. I.e. I want to create this tensor:\n[[1,1,1,1,0,0,0,0],\n [1,1,1,0,0,0,0,0],\n [1,1,1,1,1,0,0,0],\n [1,1,0,0,0,0,0,0]\n]\nHow might I do this?\nA:\n<code>\nimport tensorflow as tf\nlengths = [4, 3, 5, 2]\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nFigure 12: An example problem of TensorFlow. We implemented well-designed test function for tensor comparison.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nReference Solution\ntensor_of_tensors = torch.stack((list_of_tensors))\nAutomatic Evaluation\nTest code\n torch.testing.assert_close(tensor_of_tensors, ans, \n           check_dtype = False)\nTest case 1\n torch.random.manual_seed(42)\n list_of_tensors = [torch.randn(3), torch.randn(3),   \n torch.randn(3)]\n ans = ... # generated by Reference solution\nProblem:\nI have this code:\nimport torch\nlist_of_tensors = [ torch.randn(3), torch.randn(3), \ntorch.randn(3)]\ntensor_of_tensors = torch.tensor(list_of_tensors)\nI am getting the error:\nValueError: only one element tensors can be converted \nto Python scalars\nHow can I convert the list of tensors to a tensor of tensors in pytorch?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nlist_of_tensors = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(tensor_of_tensors)\n</code>\nFigure 13: An example problem of PyTorch, with failed attempt and error message given in the description.\nReference Solution\ndf_out = pd.DataFrame(preprocessing.scale(data),  \n                 index=data.index, columns=data.columns)\nAutomatic Evaluation\nTest code\n # tolerate rounding error\n pd.testing.assert_frame_equal(df_out, ans,   \n                     check_dtype=False, check_exact=False)\nTest case 1\n np.random.seed(42)\n data = pd.DataFrame(np.random.rand(3, 3),   \n  index=['first', 'second', 'third'], \n  columns=['c1', 'c2', \u2018c3\u2019])\n ans = ... # generated by Reference Solution\nProblem:\nI'm using the excellent read_csv()function from pandas, which gives:\nIn [31]: data = pandas.read_csv(\"lala.csv\", \ndelimiter=\",\")\nIn [32]: data\nOut[32]:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12083 entries, 0 to 12082\nColumns: 569 entries, REGIONC to SCALEKER\ndtypes: float64(51), int64(518)\nbut when i apply a function from scikit-learn i loose the informations about \ncolumns:\nfrom sklearn import preprocessing\npreprocessing.scale(data)\ngives numpy array.\nIs there a way to apply preprocessing.scale to DataFrames without loosing \nthe information(index, columns)?\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\ndata = load_data()\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(df_out)\n</code>\nFigure 14: An example problem of Scikit-learn, requiring applying sklearn preprocessing method to Pandas dataframe.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nConsider the \ufb01gure below: \nThis image has been set up with the following code. \nplt.rc('text', usetex=True) \nplt.rc('font', family='serif') \nfig, ax = plt.subplots() \nax.set_xlabel(\"Run Number\", fontsize=25) \nplt.grid(True, linestyle=\u2018--') \n... \nReference Solution\nplt.plot(y, x)\nplt.grid(color=\"blue\", linestyle=\"dashed\")\nAutomatic Evaluation\nTest code\n# Precisely matching images with np.array\nfrom PIL import Image\ncode_img, oracle_img = ... # load images\nsample_image_stat = (\n    code_img.shape == oracle_img.shape\n    and np.allclose(code_img, oracle_img)\n)\ntry:\n    assert sample_image_stat\n# IF Failed, matching image components\nax = plt.gca()\nassert ax.xaxis._major_tick_kw[\"gridOn\"]\nassert \"grid_color\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_color\"] in \n[\"blue\", \u201cb\"]\nassert \"grid_linestyle\" in ax.xaxis._major_tick_kw\nassert ax.xaxis._major_tick_kw[\"grid_linestyle\"] in \n[\"dashed\", \"--\", \"-.\", \":\"]\nTest case 1\n x,y = ...[shown in prompt]\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.arange(10)\ny = np.arange(10)\n# Plot y over x and show blue dashed grid lines\n# SOLUTION START\nRewrite prompt\nFigure 15: An example problem of Matplotlib. Matplotlib original problems often contain example \ufb01gures which cannot\nbe processed by current code models. We rewrite original problems into standalone problems in the form of comments.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nsa = sparse.csr_matrix(np.array([[1,2,3],[4,5,6],\n[7,8,9]]))\nsb = sparse.csr_matrix(np.array([0,1,2]))\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nprint(result)\n</code>\nA:\n<code>\nfrom scipy import sparse\nimport numpy as np\nexample_sA = sparse.csr_matrix(np.array([[1,2,3],\n[4,5,6],[7,8,9]]))\nexample_sB = sparse.csr_matrix(np.array([0,1,2]))\ndef f(sA = example_sA, sB = example_sB):\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\n    return result\n</code>\nProblem:\nI have this example of matrix by matrix multiplication using numpy arrays:\nimport numpy as np\nm = np.array([[1,2,3],[4,5,6],[7,8,9]])\nc = np.array([0,1,2])\nm * c\narray([[ 0,  2,  6],\n       [ 0,  5, 12],\n       [ 0,  8, 18]])\nHow can i do the same thing if m is scipy sparse CSR matrix? The result should be csr_matrix as well.\nThis gives dimension mismatch:\nsp.sparse.csr_matrix(m)*sp.sparse.csr_matrix(c)\nFigure 16: An example problem of surface perturbation. We expect model complete the function(on the right).\nOrigin\nProblem:\nHow do I convert data from a Scikit-learn Bunch object (from sklearn.datasets) to \na Pandas DataFrame?\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\nProblem:\nCan you give me any suggestion that transforms a sklearn Bunch object (from \nsklearn.datasets) to a dataframe? I'd like to do it to iris dataset.\nThanks!\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # May be you can give me a Pandas method?\nFigure 17: An example problem of surface perturbation. The description in the prompt has been paraphrased.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\narray([\n   array([0.5, 1.0, 2.0], dtype=float16),\n   array([4.0, 6.0, 8.0], dtype=float16)\n], dtype=object)\nProblem:\nHow to convert a numpy array of dtype=object to torch Tensor?\nx = np.array([\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n    np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n    np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n], dtype=object)\nFigure 18: An example problem of surface perturbation. The example input in the prompt has been replaced with another\none.\nOrigin\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based on \nexisting column names with a prefix, e.g. inv_A is an inverse of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"inv_A\": [1/\n1, 1/2, 1/3], \"inv_B\": [1/4, 1/5, 1/6]})\n\u2026[omitted for brevity]\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add exponentials of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. exp_A is an exponential of column A and so on.\nThe resulting dataframe should look like so:\nresult = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"exp_A \n\": [e^1, e^2, e^3], \"exp_B\": [e^4, e^5, e^6]})\nNotice that e is the natural constant.\n\u2026[omitted for brevity]\nFigure 19: An example problem of semantic perturbation. \u201cinverse\u201d has been replaced with an analogy word \u201cexponential\u201d.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out rows and column entries corresponding to a particular \nindex (e.g. zero_rows = 0, zero_cols = 0 corresponds to the 1st row/column) in this array?\nProblem:\nI have a 2D array `a` to represent a many-many mapping :\n0   3   1   3\n3   0   0   0\n1   0   0   0\n3   0   0   0\nWhat is the quickest way to 'zero' out the second row and the first column?\nFigure 20: An example problem of semantic perturbation. The required index of rows and columns has been changed.\nOrigin\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"abc, def, ghi, jkl\"\nProblem:\nI have the following dataframe:\n     text\n1 \"abc\" \n2 \"def\" \n3 \"ghi\"\n4 \"jkl\" \nHow can I merge these rows into a dataframe with a single row like the following one?\n     text \n1 \"jkl, ghi, def, abc\"\nFigure 21: An example problem of semantic perturbation. The order of the desired string has been reversed.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired DataFrame:\n                   Pearson Correlation Coefficient\nCol1 Col2                                 \n0        3                            0.373153\n1        3                            0.419219\n          4                            0.356149\n3        4                            0.389972\nProblem:\nI have a square correlation matrix in pandas, and am trying to divine the most efficient way to return all values \nwhere the value (always a float -1 <= x <= 1) is above 0.3.\nThe pandas.DataFrame.filter method asks for a list of columns or a RegEx, but I always want to pass all \ncolumns in. Is there a best practice on this?\ndesired Series:\n0  3    0.373153\n1  3    0.419219\n    4    0.356149\n3  4    0.389972\ndtype: float64\nFigure 22: An example problem of semantic perturbation. The type of the desired result has been changed but the content\nstill keeps the same.\nOrigin\nProblem:\nI have a logistic regression model using Pytorch, where my input is high-dimensional and my output must be a \nscalar - 0, 1 or 2.\nI'm using a linear layer combined with a softmax layer to return a n x 3 tensor, where each column represents the \nprobability of the input falling in one of the three classes (0, 1 or 2).\nHowever, I must return a n x 1 tensor, so I need to somehow pick the highest probability for each input and create \na tensor indicating which class had the highest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.2, 0.2],\n [0.1, 0.8, 0.1]]\nAnd I must return this:\n[[2],\n [0],\n [1]]\nProblem:\n\u2026[omit for brevity]\nHowever, I must return a 1 x n tensor, and I want to somehow pick the lowest probability for each input and \ncreate a tensor indicating which class had the lowest probability. How can I achieve this using Pytorch?\nTo illustrate, my Softmax outputs this:\n[[0.2, 0.1, 0.7],\n [0.6, 0.3, 0.1],\n [0.15, 0.8, 0.05]]\nAnd I must return this:\n[1, 2, 2], which has the type torch.LongTensor\nFigure 23: An example problem that is dif\ufb01cult re-written with a combination of surface and semantic perturbations\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nOrigin\nProblem:\nI can't figure out how to do a Two-sample KS test in Scipy.\n\u2026[omit for brevity]\ntest_stat = kstest(x, 'norm')\n#>>> test_stat\n#(0.021080234718821145, 0.76584491300591395)\nWhich means that at p-value of 0.76 we can not reject the null hypothesis that the two distributions are identical.\nHowever, I want to compare two distributions and see if I can reject the null hypothesis that they are identical.\n\u2026[omit for brevity]\nI tried the naive:\ntest_stat = kstest(x, z)\nand got the following error:\nTypeError: 'numpy.ndarray' object is not callable\nIs there a way to do a two-sample KS test in Python? If so, how should I do it?\nThank You in Advance\nProblem:\n\u2026[omit for brevity]\nIs there a way to do a two-sample KS test in Python, then test whether I can reject the null hypothesis that the \ntwo distributions are identical(result=True means able to reject, and the vice versa) based on alpha? If so, how \nshould I do it?\nThank You in Advance\nFigure 24: An example problem that is dif\ufb01cult re-written for more complexity\nProblem:\nSample dataframe:\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\nI'd like to add inverses of each existing column to the dataframe and name them based \non existing column names with a prefix, e.g. inv_A is an inverse of column A and so on.\n\u2026 [omitted for brevity]\nObviously there are redundant methods like doing this in a loop, but there should exist \nmuch more pythonic ways of doing it \u2026 [omitted for brevity]\nA:\n<code>\nimport pandas as pd\ndf = pd.DataFrame({\"A\": [1, 2, 3],\"B\": [4, 5, 6]})\n</code>\nresult = ...# put solution in this variable\nBEGIN SOLUTION\n<code>\nFigure 25: Completion prompt corresponding to Figure 1.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nsolve this question with example variable `clf` and \nput result in `proba`\nBEGIN SOLUTION\n<code>\nA:\n<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, y_train = load_data()\nassert type(X_train) == np.ndarray\nassert type(y_train) == np.ndarray\nX_test = X_train\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],\n    'max_samples': [0.05, 0.1, 0.2, 0.5]\n}\ndt = DecisionTreeClassifier(max_depth=1)\nbc = BaggingClassifier(dt, n_estimators=20, \nmax_samples=0.5, max_features=0.5)\n</code>\nBEGIN SOLUTION\n<code>\n[insert]\n</code>\nEND SOLUTION\n<code>\nproba = clf.predict_proba(X_test)\nprint(proba)\n</code>\nProblem:\nSay that I want to train BaggingClassifier that uses DecisionTreeClassifier:\ndt = DecisionTreeClassifier(max_depth = 1)\nbc = BaggingClassifier(dt, n_estimators = 20, max_samples = 0.5, max_features = 0.5)\nbc = bc.fit(X_train, y_train)\nI would like to use GridSearchCV to find the best parameters for both BaggingClassifier and \nDecisionTreeClassifier(e.g. max_depth from DecisionTreeClassifier and max_samples from \nBaggingClassifier), what is the syntax for this? Besides, you can just use the default arguments of GridSearchCV.\nFigure 26: More complex Completion (on the right) prompt that requires additional information for a solution.\nProblem:\nI am using Pandas to get a dataframe like this:\n    name  a  b   c\n0  Aaron 3  5   7\n1  Aaron 3  6   9\n2  Aaron 3  6  10\n3  Brave  4  6   0\n4  Brave  3  6   1\nI want to replace each name with a unique ID so output looks like:\n  name  a  b   c\n0    1     3   5   7\n1    1     3   6   9\n2    1     3   6  10\n3    2     4   6   0\n4    2     3   6   1\nHow can I do that?\nReference Solution\n# df: pd.DataFrame as input\nresult = df.replace(df['name'].unique(),\n range(1, len(df['name'].unique()) + 1))\nWrong Solution\n# create a column named \"ID\"\ndf['ID'] = df.groupby(['name']).ngroup()\nresult = df\nFigure 27: An example wrong solution that misunderstands the requirements and modi\ufb01es on the wrong column.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nProblem:\nI'm using tensorflow 2.10.0.\nI have a list of bytes and I want to convert it to a list of strings:\nx=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd\n8\\xa9',\n\u00a0\u00a0\u00a0b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n\u00a0\u00a0\u00a0\u00a0b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n\u00a0\u00a0\u00a0\u00a0b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n\u00a0\u00a0\u00a0\u00a0b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\u00a0\nHow can I get the string result list in Tensorflow? \nReference Solution\n# x: list of bytes as input\nresult = [tf.compat.as_str_any(a) for a in x]\nWrong Solution\n# Not using method in Tensorflow\nresult = [item.decode('utf-8') for item in x]\nFigure 28: An example wrong solution that uses a common function instead of a function of TensorFlow.\n102\nI think it's a pretty common message for PyTorch users with low GPU memory:\nRuntimeError: CUDA out of memory. Tried to allocate \ud83d\ude0a MiB (GPU \ud83d\ude0a; \ud83d\ude0a GiB total\ncapacity; \ud83d\ude0a GiB already allocated; \ud83d\ude0a MiB free; \ud83d\ude0a cached)\nI tried to process an image by loading each layer to GPU and then loading it back:\n m \n self.children():\n    m.cuda()\n    x = m(x)\n    m.cpu()\n    torch.cuda.empty_cache()\nfor\nin\nBut it doesn't seem to be very effective. I'm wondering is there any tips and tricks to train\nlarge deep learning models while using little GPU memory.\npython\ndeep-learning\npytorch\nobject-detection\nlow-memory\nFigure 29: An example untestable problem involving hardware problems.\nDS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\n247\nI am using python 2.7 in Ubuntu 14.04. I installed scikit-learn, numpy and matplotlib with\nthese commands:\nsudo apt-get install build-essential python-dev python-numpy \\\npython-numpy-dev python-scipy libatlas-dev g++ python-matplotlib \\\nipython\nBut when I import these packages:\n sklearn.cross_validation \n train_test_split\nfrom\nimport\nIt returns me this error:\nImportError: No module named sklearn.cross_validation\nWhat I need to do?\npython\nscikit-learn\nFigure 30: An example untestable problem involving software errors.\n55\nI would like to understand what \n does in a bit more detail.\nA\n:\ntf.global_variables_initializer\nsparse description is given here\nReturns an Op that initializes global variables.\nBut that doesn't really help me. I know that the op is necessary to initialize the graph, but what\ndoes that actually mean? Is this the step where the graph is complied?\ntensorflow\ndeep-learning\nFigure 31: An example untestable problem involving explanations.\n"
    }
]