[
    {
        "pdf_file": "paper1.pdf",
        "text": "Conformer: Convolution-augmented Transformer for Speech Recognition\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang\nGoogle Inc.\n{anmolgulati, jamesqin, chungchengc, nikip, ngyuzh, jiahuiyu, weihan, shibow, zhangzd,\nyonghui, rpang}@google.com\nAbstract\nRecently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs).\nTransformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies.\nOn the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother.\nWe also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1. Introduction\nEnd-to-end automatic speech recognition (ASR) systems based\non neural networks have seen large improvements in recent\nyears. Recurrent neural networks (RNNs) have been the de-\nfacto choice for ASR [1, 2, 3, 4] as they can model the temporal\ndependencies in the audio sequences effectively [5]. Recently,\nthe Transformer architecture based on self-attention [6, 7] has\nenjoyed widespread adoption for modeling sequences due to its\nability to capture long distance interactions and the high train-\ning ef\ufb01ciency. Alternatively, convolutions have also been suc-\ncessful for ASR [8, 9, 10, 11, 12], which capture local context\nprogressively via a local receptive \ufb01eld layer by layer.\nHowever, models with self-attention or convolutions each\nhas its limitations. While Transformers are good at modeling\nlong-range global context, they are less capable to extract \ufb01ne-\ngrained local feature patterns.\nConvolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation. To combat this issue, contemporary work Con-\ntextNet [10] adopts the squeeze-and-excitation module [13] in\neach residual block to capture longer context. However, it is still\nlimited in capturing dynamic global context as it only applies a\nglobal averaging over the entire sequence.\nRecent works have shown that combining convolution and\nFeed Forward Module\nMulti-Head Self Attention\nModule\nConvolution Module\n1/2 x\n1/2 x\nFeed Forward Module\nDropout\nConformer Blocks\nLinear\nSpecAug\n10 ms rate\n10 ms rate\nConvolution\nSubsampling\n40 ms rate\nx N\n40 ms rate\n40 ms rate\n+\n+\n+\n+\nLayernorm\nFigure 1: Conformer encoder model architecture. Conformer\ncomprises of two macaron-like feed-forward layers with half-\nstep residual connections sandwiching the multi-headed self-\nattention and convolution modules. This is followed by a post\nlayernorm.\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.\n[17] proposed a multi-branch architecture with splitting the in-\nput into two branches: self-attention and convolution; and con-\ncatenating their outputs. Their work targeted mobile applica-\ntions and showed improvements in machine translation tasks.\nIn this work, we study how to organically combine con-\nvolutions with self-attention in ASR models. We hypothesize\nthat both global and local interactions are important for being\nparameter ef\ufb01cient. To achieve this, we propose a novel combi-\nnation of self-attention and convolution will achieve the best of\nboth worlds \u2013 self-attention learns the global interaction whilst\nthe convolutions ef\ufb01ciently capture the relative-offset-based lo-\ncal correlations. Inspired by Wu et al. [17, 18], we introduce\na novel combination of self-attention and convolution, sand-\nwiched between a pair feed forward modules, as illustrated in\nFig 1.\nOur proposed model, named Conformer, achieves state-of-\nthe-art results on LibriSpeech, outperforming the previous best\npublished Transformer Transducer [7] by 15% relative improve-\narXiv:2005.08100v1  [eess.AS]  16 May 2020\n"
    },
    {
        "pdf_file": "paper1.pdf",
        "text": "Conformer: Convolution-augmented Transformer for Speech Recognition\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang\nGoogle Inc.\n{anmolgulati, jamesqin, chungchengc, nikip, ngyuzh, jiahuiyu, weihan, shibow, zhangzd,\nyonghui, rpang}@google.com\nAbstract\nRecently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs).\nTransformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies.\nOn the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother.\nWe also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1. Introduction\nEnd-to-end automatic speech recognition (ASR) systems based\non neural networks have seen large improvements in recent\nyears. Recurrent neural networks (RNNs) have been the de-\nfacto choice for ASR [1, 2, 3, 4] as they can model the temporal\ndependencies in the audio sequences effectively [5]. Recently,\nthe Transformer architecture based on self-attention [6, 7] has\nenjoyed widespread adoption for modeling sequences due to its\nability to capture long distance interactions and the high train-\ning ef\ufb01ciency. Alternatively, convolutions have also been suc-\ncessful for ASR [8, 9, 10, 11, 12], which capture local context\nprogressively via a local receptive \ufb01eld layer by layer.\nHowever, models with self-attention or convolutions each\nhas its limitations. While Transformers are good at modeling\nlong-range global context, they are less capable to extract \ufb01ne-\ngrained local feature patterns.\nConvolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation. To combat this issue, contemporary work Con-\ntextNet [10] adopts the squeeze-and-excitation module [13] in\neach residual block to capture longer context. However, it is still\nlimited in capturing dynamic global context as it only applies a\nglobal averaging over the entire sequence.\nRecent works have shown that combining convolution and\nFeed Forward Module\nMulti-Head Self Attention\nModule\nConvolution Module\n1/2 x\n1/2 x\nFeed Forward Module\nDropout\nConformer Blocks\nLinear\nSpecAug\n10 ms rate\n10 ms rate\nConvolution\nSubsampling\n40 ms rate\nx N\n40 ms rate\n40 ms rate\n+\n+\n+\n+\nLayernorm\nFigure 1: Conformer encoder model architecture. Conformer\ncomprises of two macaron-like feed-forward layers with half-\nstep residual connections sandwiching the multi-headed self-\nattention and convolution modules. This is followed by a post\nlayernorm.\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.\n[17] proposed a multi-branch architecture with splitting the in-\nput into two branches: self-attention and convolution; and con-\ncatenating their outputs. Their work targeted mobile applica-\ntions and showed improvements in machine translation tasks.\nIn this work, we study how to organically combine con-\nvolutions with self-attention in ASR models. We hypothesize\nthat both global and local interactions are important for being\nparameter ef\ufb01cient. To achieve this, we propose a novel combi-\nnation of self-attention and convolution will achieve the best of\nboth worlds \u2013 self-attention learns the global interaction whilst\nthe convolutions ef\ufb01ciently capture the relative-offset-based lo-\ncal correlations. Inspired by Wu et al. [17, 18], we introduce\na novel combination of self-attention and convolution, sand-\nwiched between a pair feed forward modules, as illustrated in\nFig 1.\nOur proposed model, named Conformer, achieves state-of-\nthe-art results on LibriSpeech, outperforming the previous best\npublished Transformer Transducer [7] by 15% relative improve-\narXiv:2005.08100v1  [eess.AS]  16 May 2020\nLayernorm\nGlu\nActivation\nPointwise\nConv\nBatchNorm\nSwish\nActivation\n1D\nDepthwise\nConv\nPointwise\nConv\nDropout\n+\nFigure 2: Convolution module. The convolution module contains a pointwise convolution with an expansion factor of 2 projecting the\nnumber of channels with a GLU activation layer, followed by a 1-D Depthwise convolution. The 1-D depthwise conv is followed by a\nBatchnorm and then a swish activation layer.\nment on the testother dataset with an external language model.\nWe present three models based on model parameter limit con-\nstraints of 10M , 30M and 118M. Our 10M model shows an im-\nprovement when compared to similar sized contemporary work\n[10] with 2.7%/6.3% on test/testother datasets. Our medium\n30M parameters-sized model already outperforms transformer\ntransducer published in [7] which uses 139M model parameters.\nWith the big 118M parameter model, we are able to achieve\n2.1%/4.3% without using language models and 1.9%/3.9% with\nan external language model.\nWe further carefully study the effects of the number of at-\ntention heads, convolution kernel sizes, activation functions,\nplacement of feed-forward layers, and different strategies of\nadding convolution modules to a Transformer-based network,\nand shed light on how each contributes to the accuracy improve-\nments.\n2. Conformer Encoder\nOur audio encoder \ufb01rst processes the input with a convolution\nsubsampling layer and then with a number of conformer blocks,\nas illustrated in Figure 1. The distinctive feature of our model is\nthe use of Conformer blocks in the place of Transformer blocks\nas in [7, 19].\nA conformer block is composed of four modules stacked\ntogether, i.e, a feed-forward module, a self-attention module,\na convolution module, and a second feed-forward module in\nthe end. Sections 2.1, 1, and 2.3 introduce the self-attention,\nconvolution, and feed-forward modules, respectively. Finally,\n2.4 describes how these sub blocks are combined.\n2.1. Multi-Headed Self-Attention Module\nWe employ multi-headed self-attention (MHSA) while integrat-\ning an important technique from Transformer-XL [20], the rel-\native sinusoidal positional encoding scheme. The relative po-\nsitional encoding allows the self-attention module to general-\nize better on different input length and the resulting encoder is\nmore robust to the variance of the utterance length. We use pre-\nnorm residual units [21, 22] with dropout which helps training\nand regularizing deeper models. Figure 3 below illustrates the\nmulti-headed self-attention block.\nLayernorm\nMulti-Head Attention with\nRelative Positional\nEmbedding\nDropout\n+\nFigure 3: Multi-Headed self-attention module. We use multi-\nheaded self-attention with relative positional embedding in a\npre-norm residual unit.\n2.2. Convolution Module\nInspired by [17], the convolution module starts with a gating\nmechanism [23]\u2014a pointwise convolution and a gated linear\nunit (GLU). This is followed by a single 1-D depthwise convo-\nlution layer. Batchnorm is deployed just after the convolution\nto aid training deep models. Figure 2 illustrates the convolution\nblock.\n2.3. Feed Forward Module\nThe Transformer architecture as proposed in [6] deploys a feed\nforward module after the MHSA layer and is composed of two\nlinear transformations and a nonlinear activation in between. A\nresidual connection is added over the feed-forward layers, fol-\nlowed by layer normalization. This structure is also adopted by\nTransformer ASR models [7, 24].\nWe follow pre-norm residual units [21, 22] and apply layer\nnormalization within the residual unit and on the input before\nthe \ufb01rst linear layer. We also apply Swish activation [25] and\ndropout, which helps regularizing the network. Figure 4 illus-\ntrates the Feed Forward (FFN) module.\n2.4. Conformer Block\nOur proposed Conformer block contains two Feed Forward\nmodules sandwiching the Multi-Headed Self-Attention module\nand the Convolution module, as shown in Figure 1.\nThis sandwich structure is inspired by Macaron-Net [18],\nwhich proposes replacing the original feed-forward layer in the\nTransformer block into two half-step feed-forward layers, one\nbefore the attention layer and one after. As in Macron-Net, we\nemploy half-step residual weights in our feed-forward (FFN)\nmodules. The second feed-forward module is followed by a\n\ufb01nal layernorm layer. Mathematically, this means, for input xi\nto a Conformer block i, the output yi of the block is:\n\u02dcxi = xi + 1\n2FFN(xi)\nx\u2032\ni = \u02dcxi + MHSA( \u02dcxi)\nx\u2032\u2032\ni = x\u2032\ni + Conv(x\u2032\ni)\nyi = Layernorm(x\u2032\u2032\ni + 1\n2FFN(x\u2032\u2032\ni ))\n(1)\nwhere FFN refers to the Feed forward module, MHSA refers to\nthe Multi-Head Self-Attention module, and Conv refers to the\nConvolution module as described in the preceding sections.\nOur ablation study discussed in Sec 3.4.3 compares the\nMacaron-style half-step FFNs with the vanilla FFN as used in\nprevious works. We \ufb01nd that having two Macaron-net style\nfeed-forward layers with half-step residual connections sand-\nwiching the attention and convolution modules in between pro-\nvides a signi\ufb01cant improvement over having a single feed-\nforward module in our Conformer architecture.\nThe combination of convolution and self-attention has been\nstudied before and one can imagine many ways to achieve\n"
    },
    {
        "pdf_file": "paper1.pdf",
        "text": "Conformer: Convolution-augmented Transformer for Speech Recognition\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang\nGoogle Inc.\n{anmolgulati, jamesqin, chungchengc, nikip, ngyuzh, jiahuiyu, weihan, shibow, zhangzd,\nyonghui, rpang}@google.com\nAbstract\nRecently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs).\nTransformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies.\nOn the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother.\nWe also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1. Introduction\nEnd-to-end automatic speech recognition (ASR) systems based\non neural networks have seen large improvements in recent\nyears. Recurrent neural networks (RNNs) have been the de-\nfacto choice for ASR [1, 2, 3, 4] as they can model the temporal\ndependencies in the audio sequences effectively [5]. Recently,\nthe Transformer architecture based on self-attention [6, 7] has\nenjoyed widespread adoption for modeling sequences due to its\nability to capture long distance interactions and the high train-\ning ef\ufb01ciency. Alternatively, convolutions have also been suc-\ncessful for ASR [8, 9, 10, 11, 12], which capture local context\nprogressively via a local receptive \ufb01eld layer by layer.\nHowever, models with self-attention or convolutions each\nhas its limitations. While Transformers are good at modeling\nlong-range global context, they are less capable to extract \ufb01ne-\ngrained local feature patterns.\nConvolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation. To combat this issue, contemporary work Con-\ntextNet [10] adopts the squeeze-and-excitation module [13] in\neach residual block to capture longer context. However, it is still\nlimited in capturing dynamic global context as it only applies a\nglobal averaging over the entire sequence.\nRecent works have shown that combining convolution and\nFeed Forward Module\nMulti-Head Self Attention\nModule\nConvolution Module\n1/2 x\n1/2 x\nFeed Forward Module\nDropout\nConformer Blocks\nLinear\nSpecAug\n10 ms rate\n10 ms rate\nConvolution\nSubsampling\n40 ms rate\nx N\n40 ms rate\n40 ms rate\n+\n+\n+\n+\nLayernorm\nFigure 1: Conformer encoder model architecture. Conformer\ncomprises of two macaron-like feed-forward layers with half-\nstep residual connections sandwiching the multi-headed self-\nattention and convolution modules. This is followed by a post\nlayernorm.\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.\n[17] proposed a multi-branch architecture with splitting the in-\nput into two branches: self-attention and convolution; and con-\ncatenating their outputs. Their work targeted mobile applica-\ntions and showed improvements in machine translation tasks.\nIn this work, we study how to organically combine con-\nvolutions with self-attention in ASR models. We hypothesize\nthat both global and local interactions are important for being\nparameter ef\ufb01cient. To achieve this, we propose a novel combi-\nnation of self-attention and convolution will achieve the best of\nboth worlds \u2013 self-attention learns the global interaction whilst\nthe convolutions ef\ufb01ciently capture the relative-offset-based lo-\ncal correlations. Inspired by Wu et al. [17, 18], we introduce\na novel combination of self-attention and convolution, sand-\nwiched between a pair feed forward modules, as illustrated in\nFig 1.\nOur proposed model, named Conformer, achieves state-of-\nthe-art results on LibriSpeech, outperforming the previous best\npublished Transformer Transducer [7] by 15% relative improve-\narXiv:2005.08100v1  [eess.AS]  16 May 2020\nLayernorm\nGlu\nActivation\nPointwise\nConv\nBatchNorm\nSwish\nActivation\n1D\nDepthwise\nConv\nPointwise\nConv\nDropout\n+\nFigure 2: Convolution module. The convolution module contains a pointwise convolution with an expansion factor of 2 projecting the\nnumber of channels with a GLU activation layer, followed by a 1-D Depthwise convolution. The 1-D depthwise conv is followed by a\nBatchnorm and then a swish activation layer.\nment on the testother dataset with an external language model.\nWe present three models based on model parameter limit con-\nstraints of 10M , 30M and 118M. Our 10M model shows an im-\nprovement when compared to similar sized contemporary work\n[10] with 2.7%/6.3% on test/testother datasets. Our medium\n30M parameters-sized model already outperforms transformer\ntransducer published in [7] which uses 139M model parameters.\nWith the big 118M parameter model, we are able to achieve\n2.1%/4.3% without using language models and 1.9%/3.9% with\nan external language model.\nWe further carefully study the effects of the number of at-\ntention heads, convolution kernel sizes, activation functions,\nplacement of feed-forward layers, and different strategies of\nadding convolution modules to a Transformer-based network,\nand shed light on how each contributes to the accuracy improve-\nments.\n2. Conformer Encoder\nOur audio encoder \ufb01rst processes the input with a convolution\nsubsampling layer and then with a number of conformer blocks,\nas illustrated in Figure 1. The distinctive feature of our model is\nthe use of Conformer blocks in the place of Transformer blocks\nas in [7, 19].\nA conformer block is composed of four modules stacked\ntogether, i.e, a feed-forward module, a self-attention module,\na convolution module, and a second feed-forward module in\nthe end. Sections 2.1, 1, and 2.3 introduce the self-attention,\nconvolution, and feed-forward modules, respectively. Finally,\n2.4 describes how these sub blocks are combined.\n2.1. Multi-Headed Self-Attention Module\nWe employ multi-headed self-attention (MHSA) while integrat-\ning an important technique from Transformer-XL [20], the rel-\native sinusoidal positional encoding scheme. The relative po-\nsitional encoding allows the self-attention module to general-\nize better on different input length and the resulting encoder is\nmore robust to the variance of the utterance length. We use pre-\nnorm residual units [21, 22] with dropout which helps training\nand regularizing deeper models. Figure 3 below illustrates the\nmulti-headed self-attention block.\nLayernorm\nMulti-Head Attention with\nRelative Positional\nEmbedding\nDropout\n+\nFigure 3: Multi-Headed self-attention module. We use multi-\nheaded self-attention with relative positional embedding in a\npre-norm residual unit.\n2.2. Convolution Module\nInspired by [17], the convolution module starts with a gating\nmechanism [23]\u2014a pointwise convolution and a gated linear\nunit (GLU). This is followed by a single 1-D depthwise convo-\nlution layer. Batchnorm is deployed just after the convolution\nto aid training deep models. Figure 2 illustrates the convolution\nblock.\n2.3. Feed Forward Module\nThe Transformer architecture as proposed in [6] deploys a feed\nforward module after the MHSA layer and is composed of two\nlinear transformations and a nonlinear activation in between. A\nresidual connection is added over the feed-forward layers, fol-\nlowed by layer normalization. This structure is also adopted by\nTransformer ASR models [7, 24].\nWe follow pre-norm residual units [21, 22] and apply layer\nnormalization within the residual unit and on the input before\nthe \ufb01rst linear layer. We also apply Swish activation [25] and\ndropout, which helps regularizing the network. Figure 4 illus-\ntrates the Feed Forward (FFN) module.\n2.4. Conformer Block\nOur proposed Conformer block contains two Feed Forward\nmodules sandwiching the Multi-Headed Self-Attention module\nand the Convolution module, as shown in Figure 1.\nThis sandwich structure is inspired by Macaron-Net [18],\nwhich proposes replacing the original feed-forward layer in the\nTransformer block into two half-step feed-forward layers, one\nbefore the attention layer and one after. As in Macron-Net, we\nemploy half-step residual weights in our feed-forward (FFN)\nmodules. The second feed-forward module is followed by a\n\ufb01nal layernorm layer. Mathematically, this means, for input xi\nto a Conformer block i, the output yi of the block is:\n\u02dcxi = xi + 1\n2FFN(xi)\nx\u2032\ni = \u02dcxi + MHSA( \u02dcxi)\nx\u2032\u2032\ni = x\u2032\ni + Conv(x\u2032\ni)\nyi = Layernorm(x\u2032\u2032\ni + 1\n2FFN(x\u2032\u2032\ni ))\n(1)\nwhere FFN refers to the Feed forward module, MHSA refers to\nthe Multi-Head Self-Attention module, and Conv refers to the\nConvolution module as described in the preceding sections.\nOur ablation study discussed in Sec 3.4.3 compares the\nMacaron-style half-step FFNs with the vanilla FFN as used in\nprevious works. We \ufb01nd that having two Macaron-net style\nfeed-forward layers with half-step residual connections sand-\nwiching the attention and convolution modules in between pro-\nvides a signi\ufb01cant improvement over having a single feed-\nforward module in our Conformer architecture.\nThe combination of convolution and self-attention has been\nstudied before and one can imagine many ways to achieve\nLayernorm\nLinear\nLayer\nDropout\nLinear\nLayer\nSwish\nActivation\nDropout\n+\nFigure 4: Feed forward module. The \ufb01rst linear layer uses an expansion factor of 4 and the second linear layer projects it back to the\nmodel dimension. We use swish activation and a pre-norm residual units in feed forward module.\nthat. Different options of augmenting convolutions with self-\nattention are studied in Sec 3.4.2. We found that convolution\nmodule stacked after the self-attention module works best for\nspeech recognition.\n3. Experiments\n3.1. Data\nWe evaluate the proposed model on the LibriSpeech [26]\ndataset, which consists of 970 hours of labeled speech and\nan additional 800M word token text-only corpus for building\nlanguage model. We extracted 80-channel \ufb01lterbanks features\ncomputed from a 25ms window with a stride of 10ms. We use\nSpecAugment [27, 28] with mask parameter (F = 27), and ten\ntime masks with maximum time-mask ratio (pS = 0.05), where\nthe maximum-size of the time mask is set to pS times the length\nof the utterance.\n3.2. Conformer Transducer\nWe identify three models, small, medium and large, with 10M,\n30M, and 118M params, respectively, by sweeping different\ncombinations of network depth, model dimensions, number of\nattention heads and choosing the best performing one within\nmodel parameter size constraints. We use a single-LSTM-layer\ndecoder in all our models. Table 1 describes their architecture\nhyper-parameters.\nFor regularization, we apply dropout [29] in each residual\nunit of the conformer, i.e, to the output of each module, before\nit is added to the module input. We use a rate of Pdrop = 0.1.\nVariational noise [5, 30] is introduced to the model as a regu-\nlarization. A \u21132 regularization with 1e \u22126 weight is also added\nto all the trainable weights in the network. We train the models\nwith the Adam optimizer [31] with \u03b21 = 0.9, \u03b22 = 0.98 and\n\u03f5 = 10\u22129 and a transformer learning rate schedule [6], with\n10k warm-up steps and peak learning rate 0.05/\n\u221a\nd where d is\nthe model dimension in conformer encoder.\nWe use a 3-layer LSTM language model (LM) with width\n4096 trained on the LibriSpeech langauge model corpus with\nthe LibriSpeech960h transcripts added, tokenized with the 1k\nWPM built from LibriSpeech 960h. The LM has word-level\nperplexity 63.9 on the dev-set transcripts. The LM weight \u03bb\nfor shallow fusion is tuned on the dev-set via grid search. All\nmodels are implemented with Lingvo toolkit [32].\n3.3. Results on LibriSpeech\nTable 2 compares the (WER) result of our model on Lib-\nriSpeech test-clean/test-other with a few state-of-the-art mod-\nels include: ContextNet [10], Transformer transducer [7], and\nQuartzNet [9]. All our evaluation results round up to 1 digit\nafter decimal point.\nWithout a language model,\nthe performance of our\nmedium model already achieve competitive results of 2.3/5.0\non test/testother outperforming the best known Transformer,\nLSTM based model, or a similar sized convolution model. With\nthe language model added, our model achieves the lowest word\nTable 1: Model hyper-parameters for Conformer S, M, and L\nmodels, found via sweeping different combinations and choos-\ning the best performing models within the parameter limits.\nModel\nConformer\n(S)\nConformer\n(M)\nConformer\n(L)\nNum Params (M)\n10.3\n30.7\n118.8\nEncoder Layers\n16\n16\n17\nEncoder Dim\n144\n256\n512\nAttention Heads\n4\n4\n8\nConv Kernel Size\n32\n32\n32\nDecoder Layers\n1\n1\n1\nDecoder Dim\n320\n640\n640\nTable 2: Comparison of Conformer with recent published mod-\nels. Our model shows improvements consistently over various\nmodel parameter size constraints. At 10.3M parameters, our\nmodel is 0.7% better on testother when compared to contempo-\nrary work, ContextNet(S) [10]. At 30.7M model parameters our\nmodel already signi\ufb01cantly outperforms the previous published\nstate of the art results of Transformer Transducer [7] with 139M\nparameters.\nMethod\n#Params (M)\nWER Without LM\nWER With LM\ntestclean\ntestother\ntestclean\ntestother\nHybrid\nTransformer [33]\n-\n-\n-\n2.26\n4.85\nCTC\nQuartzNet [9]\n19\n3.90\n11.28\n2.69\n7.25\nLAS\nTransformer [34]\n270\n2.89\n6.98\n2.33\n5.17\nTransformer [19]\n-\n2.2\n5.6\n2.6\n5.7\nLSTM\n360\n2.6\n6.0\n2.2\n5.2\nTransducer\nTransformer [7]\n139\n2.4\n5.6\n2.0\n4.6\nContextNet(S) [10]\n10.8\n2.9\n7.0\n2.3\n5.5\nContextNet(M) [10]\n31.4\n2.4\n5.4\n2.0\n4.5\nContextNet(L) [10]\n112.7\n2.1\n4.6\n1.9\n4.1\nConformer (Ours)\nConformer(S)\n10.3\n2.7\n6.3\n2.1\n5.0\nConformer(M)\n30.7\n2.3\n5.0\n2.0\n4.3\nConformer(L)\n118.8\n2.1\n4.3\n1.9\n3.9\nerror rate among all the existing models. This clearly demon-\nstrates the effectiveness of combining Transformer and convo-\nlution in a single neural network.\n3.4. Ablation Studies\n3.4.1. Conformer Block vs. Transformer Block\nA Conformer block differs from a Transformer block in a\nnumber of ways, in particular, the inclusion of a convolution\nblock and having a pair of FFNs surrounding the block in the\nMacaron-style. Below we study these effects of these differ-\nences by mutating a Conformer block towards a Transformer\nblock, while keeping the total number of parameters unchanged.\nTable 3 shows the impact of each change to the Conformer\nblock. Among all differences, convolution sub-block is the most\n"
    },
    {
        "pdf_file": "paper1.pdf",
        "text": "Conformer: Convolution-augmented Transformer for Speech Recognition\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang\nGoogle Inc.\n{anmolgulati, jamesqin, chungchengc, nikip, ngyuzh, jiahuiyu, weihan, shibow, zhangzd,\nyonghui, rpang}@google.com\nAbstract\nRecently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs).\nTransformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies.\nOn the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother.\nWe also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1. Introduction\nEnd-to-end automatic speech recognition (ASR) systems based\non neural networks have seen large improvements in recent\nyears. Recurrent neural networks (RNNs) have been the de-\nfacto choice for ASR [1, 2, 3, 4] as they can model the temporal\ndependencies in the audio sequences effectively [5]. Recently,\nthe Transformer architecture based on self-attention [6, 7] has\nenjoyed widespread adoption for modeling sequences due to its\nability to capture long distance interactions and the high train-\ning ef\ufb01ciency. Alternatively, convolutions have also been suc-\ncessful for ASR [8, 9, 10, 11, 12], which capture local context\nprogressively via a local receptive \ufb01eld layer by layer.\nHowever, models with self-attention or convolutions each\nhas its limitations. While Transformers are good at modeling\nlong-range global context, they are less capable to extract \ufb01ne-\ngrained local feature patterns.\nConvolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation. To combat this issue, contemporary work Con-\ntextNet [10] adopts the squeeze-and-excitation module [13] in\neach residual block to capture longer context. However, it is still\nlimited in capturing dynamic global context as it only applies a\nglobal averaging over the entire sequence.\nRecent works have shown that combining convolution and\nFeed Forward Module\nMulti-Head Self Attention\nModule\nConvolution Module\n1/2 x\n1/2 x\nFeed Forward Module\nDropout\nConformer Blocks\nLinear\nSpecAug\n10 ms rate\n10 ms rate\nConvolution\nSubsampling\n40 ms rate\nx N\n40 ms rate\n40 ms rate\n+\n+\n+\n+\nLayernorm\nFigure 1: Conformer encoder model architecture. Conformer\ncomprises of two macaron-like feed-forward layers with half-\nstep residual connections sandwiching the multi-headed self-\nattention and convolution modules. This is followed by a post\nlayernorm.\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.\n[17] proposed a multi-branch architecture with splitting the in-\nput into two branches: self-attention and convolution; and con-\ncatenating their outputs. Their work targeted mobile applica-\ntions and showed improvements in machine translation tasks.\nIn this work, we study how to organically combine con-\nvolutions with self-attention in ASR models. We hypothesize\nthat both global and local interactions are important for being\nparameter ef\ufb01cient. To achieve this, we propose a novel combi-\nnation of self-attention and convolution will achieve the best of\nboth worlds \u2013 self-attention learns the global interaction whilst\nthe convolutions ef\ufb01ciently capture the relative-offset-based lo-\ncal correlations. Inspired by Wu et al. [17, 18], we introduce\na novel combination of self-attention and convolution, sand-\nwiched between a pair feed forward modules, as illustrated in\nFig 1.\nOur proposed model, named Conformer, achieves state-of-\nthe-art results on LibriSpeech, outperforming the previous best\npublished Transformer Transducer [7] by 15% relative improve-\narXiv:2005.08100v1  [eess.AS]  16 May 2020\nLayernorm\nGlu\nActivation\nPointwise\nConv\nBatchNorm\nSwish\nActivation\n1D\nDepthwise\nConv\nPointwise\nConv\nDropout\n+\nFigure 2: Convolution module. The convolution module contains a pointwise convolution with an expansion factor of 2 projecting the\nnumber of channels with a GLU activation layer, followed by a 1-D Depthwise convolution. The 1-D depthwise conv is followed by a\nBatchnorm and then a swish activation layer.\nment on the testother dataset with an external language model.\nWe present three models based on model parameter limit con-\nstraints of 10M , 30M and 118M. Our 10M model shows an im-\nprovement when compared to similar sized contemporary work\n[10] with 2.7%/6.3% on test/testother datasets. Our medium\n30M parameters-sized model already outperforms transformer\ntransducer published in [7] which uses 139M model parameters.\nWith the big 118M parameter model, we are able to achieve\n2.1%/4.3% without using language models and 1.9%/3.9% with\nan external language model.\nWe further carefully study the effects of the number of at-\ntention heads, convolution kernel sizes, activation functions,\nplacement of feed-forward layers, and different strategies of\nadding convolution modules to a Transformer-based network,\nand shed light on how each contributes to the accuracy improve-\nments.\n2. Conformer Encoder\nOur audio encoder \ufb01rst processes the input with a convolution\nsubsampling layer and then with a number of conformer blocks,\nas illustrated in Figure 1. The distinctive feature of our model is\nthe use of Conformer blocks in the place of Transformer blocks\nas in [7, 19].\nA conformer block is composed of four modules stacked\ntogether, i.e, a feed-forward module, a self-attention module,\na convolution module, and a second feed-forward module in\nthe end. Sections 2.1, 1, and 2.3 introduce the self-attention,\nconvolution, and feed-forward modules, respectively. Finally,\n2.4 describes how these sub blocks are combined.\n2.1. Multi-Headed Self-Attention Module\nWe employ multi-headed self-attention (MHSA) while integrat-\ning an important technique from Transformer-XL [20], the rel-\native sinusoidal positional encoding scheme. The relative po-\nsitional encoding allows the self-attention module to general-\nize better on different input length and the resulting encoder is\nmore robust to the variance of the utterance length. We use pre-\nnorm residual units [21, 22] with dropout which helps training\nand regularizing deeper models. Figure 3 below illustrates the\nmulti-headed self-attention block.\nLayernorm\nMulti-Head Attention with\nRelative Positional\nEmbedding\nDropout\n+\nFigure 3: Multi-Headed self-attention module. We use multi-\nheaded self-attention with relative positional embedding in a\npre-norm residual unit.\n2.2. Convolution Module\nInspired by [17], the convolution module starts with a gating\nmechanism [23]\u2014a pointwise convolution and a gated linear\nunit (GLU). This is followed by a single 1-D depthwise convo-\nlution layer. Batchnorm is deployed just after the convolution\nto aid training deep models. Figure 2 illustrates the convolution\nblock.\n2.3. Feed Forward Module\nThe Transformer architecture as proposed in [6] deploys a feed\nforward module after the MHSA layer and is composed of two\nlinear transformations and a nonlinear activation in between. A\nresidual connection is added over the feed-forward layers, fol-\nlowed by layer normalization. This structure is also adopted by\nTransformer ASR models [7, 24].\nWe follow pre-norm residual units [21, 22] and apply layer\nnormalization within the residual unit and on the input before\nthe \ufb01rst linear layer. We also apply Swish activation [25] and\ndropout, which helps regularizing the network. Figure 4 illus-\ntrates the Feed Forward (FFN) module.\n2.4. Conformer Block\nOur proposed Conformer block contains two Feed Forward\nmodules sandwiching the Multi-Headed Self-Attention module\nand the Convolution module, as shown in Figure 1.\nThis sandwich structure is inspired by Macaron-Net [18],\nwhich proposes replacing the original feed-forward layer in the\nTransformer block into two half-step feed-forward layers, one\nbefore the attention layer and one after. As in Macron-Net, we\nemploy half-step residual weights in our feed-forward (FFN)\nmodules. The second feed-forward module is followed by a\n\ufb01nal layernorm layer. Mathematically, this means, for input xi\nto a Conformer block i, the output yi of the block is:\n\u02dcxi = xi + 1\n2FFN(xi)\nx\u2032\ni = \u02dcxi + MHSA( \u02dcxi)\nx\u2032\u2032\ni = x\u2032\ni + Conv(x\u2032\ni)\nyi = Layernorm(x\u2032\u2032\ni + 1\n2FFN(x\u2032\u2032\ni ))\n(1)\nwhere FFN refers to the Feed forward module, MHSA refers to\nthe Multi-Head Self-Attention module, and Conv refers to the\nConvolution module as described in the preceding sections.\nOur ablation study discussed in Sec 3.4.3 compares the\nMacaron-style half-step FFNs with the vanilla FFN as used in\nprevious works. We \ufb01nd that having two Macaron-net style\nfeed-forward layers with half-step residual connections sand-\nwiching the attention and convolution modules in between pro-\nvides a signi\ufb01cant improvement over having a single feed-\nforward module in our Conformer architecture.\nThe combination of convolution and self-attention has been\nstudied before and one can imagine many ways to achieve\nLayernorm\nLinear\nLayer\nDropout\nLinear\nLayer\nSwish\nActivation\nDropout\n+\nFigure 4: Feed forward module. The \ufb01rst linear layer uses an expansion factor of 4 and the second linear layer projects it back to the\nmodel dimension. We use swish activation and a pre-norm residual units in feed forward module.\nthat. Different options of augmenting convolutions with self-\nattention are studied in Sec 3.4.2. We found that convolution\nmodule stacked after the self-attention module works best for\nspeech recognition.\n3. Experiments\n3.1. Data\nWe evaluate the proposed model on the LibriSpeech [26]\ndataset, which consists of 970 hours of labeled speech and\nan additional 800M word token text-only corpus for building\nlanguage model. We extracted 80-channel \ufb01lterbanks features\ncomputed from a 25ms window with a stride of 10ms. We use\nSpecAugment [27, 28] with mask parameter (F = 27), and ten\ntime masks with maximum time-mask ratio (pS = 0.05), where\nthe maximum-size of the time mask is set to pS times the length\nof the utterance.\n3.2. Conformer Transducer\nWe identify three models, small, medium and large, with 10M,\n30M, and 118M params, respectively, by sweeping different\ncombinations of network depth, model dimensions, number of\nattention heads and choosing the best performing one within\nmodel parameter size constraints. We use a single-LSTM-layer\ndecoder in all our models. Table 1 describes their architecture\nhyper-parameters.\nFor regularization, we apply dropout [29] in each residual\nunit of the conformer, i.e, to the output of each module, before\nit is added to the module input. We use a rate of Pdrop = 0.1.\nVariational noise [5, 30] is introduced to the model as a regu-\nlarization. A \u21132 regularization with 1e \u22126 weight is also added\nto all the trainable weights in the network. We train the models\nwith the Adam optimizer [31] with \u03b21 = 0.9, \u03b22 = 0.98 and\n\u03f5 = 10\u22129 and a transformer learning rate schedule [6], with\n10k warm-up steps and peak learning rate 0.05/\n\u221a\nd where d is\nthe model dimension in conformer encoder.\nWe use a 3-layer LSTM language model (LM) with width\n4096 trained on the LibriSpeech langauge model corpus with\nthe LibriSpeech960h transcripts added, tokenized with the 1k\nWPM built from LibriSpeech 960h. The LM has word-level\nperplexity 63.9 on the dev-set transcripts. The LM weight \u03bb\nfor shallow fusion is tuned on the dev-set via grid search. All\nmodels are implemented with Lingvo toolkit [32].\n3.3. Results on LibriSpeech\nTable 2 compares the (WER) result of our model on Lib-\nriSpeech test-clean/test-other with a few state-of-the-art mod-\nels include: ContextNet [10], Transformer transducer [7], and\nQuartzNet [9]. All our evaluation results round up to 1 digit\nafter decimal point.\nWithout a language model,\nthe performance of our\nmedium model already achieve competitive results of 2.3/5.0\non test/testother outperforming the best known Transformer,\nLSTM based model, or a similar sized convolution model. With\nthe language model added, our model achieves the lowest word\nTable 1: Model hyper-parameters for Conformer S, M, and L\nmodels, found via sweeping different combinations and choos-\ning the best performing models within the parameter limits.\nModel\nConformer\n(S)\nConformer\n(M)\nConformer\n(L)\nNum Params (M)\n10.3\n30.7\n118.8\nEncoder Layers\n16\n16\n17\nEncoder Dim\n144\n256\n512\nAttention Heads\n4\n4\n8\nConv Kernel Size\n32\n32\n32\nDecoder Layers\n1\n1\n1\nDecoder Dim\n320\n640\n640\nTable 2: Comparison of Conformer with recent published mod-\nels. Our model shows improvements consistently over various\nmodel parameter size constraints. At 10.3M parameters, our\nmodel is 0.7% better on testother when compared to contempo-\nrary work, ContextNet(S) [10]. At 30.7M model parameters our\nmodel already signi\ufb01cantly outperforms the previous published\nstate of the art results of Transformer Transducer [7] with 139M\nparameters.\nMethod\n#Params (M)\nWER Without LM\nWER With LM\ntestclean\ntestother\ntestclean\ntestother\nHybrid\nTransformer [33]\n-\n-\n-\n2.26\n4.85\nCTC\nQuartzNet [9]\n19\n3.90\n11.28\n2.69\n7.25\nLAS\nTransformer [34]\n270\n2.89\n6.98\n2.33\n5.17\nTransformer [19]\n-\n2.2\n5.6\n2.6\n5.7\nLSTM\n360\n2.6\n6.0\n2.2\n5.2\nTransducer\nTransformer [7]\n139\n2.4\n5.6\n2.0\n4.6\nContextNet(S) [10]\n10.8\n2.9\n7.0\n2.3\n5.5\nContextNet(M) [10]\n31.4\n2.4\n5.4\n2.0\n4.5\nContextNet(L) [10]\n112.7\n2.1\n4.6\n1.9\n4.1\nConformer (Ours)\nConformer(S)\n10.3\n2.7\n6.3\n2.1\n5.0\nConformer(M)\n30.7\n2.3\n5.0\n2.0\n4.3\nConformer(L)\n118.8\n2.1\n4.3\n1.9\n3.9\nerror rate among all the existing models. This clearly demon-\nstrates the effectiveness of combining Transformer and convo-\nlution in a single neural network.\n3.4. Ablation Studies\n3.4.1. Conformer Block vs. Transformer Block\nA Conformer block differs from a Transformer block in a\nnumber of ways, in particular, the inclusion of a convolution\nblock and having a pair of FFNs surrounding the block in the\nMacaron-style. Below we study these effects of these differ-\nences by mutating a Conformer block towards a Transformer\nblock, while keeping the total number of parameters unchanged.\nTable 3 shows the impact of each change to the Conformer\nblock. Among all differences, convolution sub-block is the most\nimportant feature, while having a Macaron-style FFN pair is\nalso more effective than a single FFN of the same number of\nparameters. Using swish activations led to faster convergence\nin the Conformer models.\nTable 3: Disentangling Conformer. Starting from a Conformer\nblock, we remove its features and move towards a vanilla Trans-\nformer block: (1) replacing SWISH with ReLU; (2) remov-\ning the convolution sub-block; (3) replacing the Macaron-style\nFFN pairs with a single FFN; (4) replacing self-attention with\nrelative positional embedding [20] with a vanilla self-attention\nlayer [6]. All ablation study results are evaluated without the\nexternal LM.\nModel\nArchitecture\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\nConformer Model\n1.9\n4.4\n2.1\n4.3\n\u2013 SWISH + ReLU\n1.9\n4.4\n2.0\n4.5\n\u2013 Convolution Block\n2.1\n4.8\n2.1\n4.9\n\u2013 Macaron FFN\n2.1\n5.1\n2.1\n5.0\n\u2013 Relative Pos. Emb.\n2.3\n5.8\n2.4\n5.6\n3.4.2. Combinations of Convolution and Transformer Modules\nWe study the effects of various different ways of combining the\nmulti-headed self-attention (MHSA) module with the convolu-\ntion module. First, we try replacing the depthwise convolution\nin the convolution module with a lightweight convolution [35],\nsee a signi\ufb01cant drop in the performance especially on the dev-\nother dataset. Second, we study placing the convolution mod-\nule before the MHSA module in our Conformer model and \ufb01nd\nthat it degrades the results by 0.1 on dev-other. Another pos-\nsible way of the architecture is to split the input into parallel\nbranches of multi-headed self attention module and a convolu-\ntion module with their output concatenated as suggested in [17].\nWe found that this worsens the performance when compared to\nour proposed architecture.\nThese results in Table 4 suggest the advantage of placing\nthe convolution module after the self-attention module in the\nConformer block.\nTable 4: Ablation study of Conformer Attention Convolution\nBlocks. Varying the combination of the convolution block with\nthe multi-headed self attention: (1) Conformer architecture; (2)\nUsing Lightweight convolutions instead of depthwise convolu-\ntion in the convolution block in Conformer; (3) Convolution be-\nfore multi-headed self attention; (4) Convolution and MHSA in\nparallel with their output concatenated [17].\nModel Architecture\ndev\nclean\ndev\nother\nConformer\n1.9\n4.4\n\u2013 Depthwise conv + Lightweight convolution\n2.0\n4.8\nConvolution block before MHSA\n1.9\n4.5\nParallel MHSA and Convolution\n2.0\n4.9\n3.4.3. Macaron Feed Forward Modules\nInstead of a single feed-forward module (FFN) post the atten-\ntion blocks as in the Transformer models, the Conformer block\nhas a pair of macaron-like Feed forward modules sandwiching\nthe self-attention and convolution modules. Further, the Con-\nformer feed forward modules are used with half-step residuals.\nTable 5 shows the impact of changing the Conformer block to\nuse a single FFN or full-step residuals.\nTable 5: Ablation study of Macaron-net Feed Forward mod-\nules. Ablating the differences between the Conformer feed for-\nward module with that of a single FFN used in Transformer\nmodels: (1) Conformer; (2) Conformer with full-step residuals\nin Feed forward modules; (3) replacing the Macaron-style FFN\npair with a single FFN.\nModel\nArchitecture\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\nConformer\n1.9\n4.4\n2.1\n4.3\nSingle FFN\n1.9\n4.5\n2.1\n4.5\nFull step residuals\n1.9\n4.5\n2.1\n4.5\n3.4.4. Number of Attention Heads\nIn self-attention, each attention head learns to focus on different\nparts of the input, making it possible to improve predictions\nbeyond the simple weighted average. We perform experiments\nto study the effect of varying the number of attention heads from\n4 to 32 in our large model, using the same number of heads\nin all layers. We \ufb01nd that increasing attention heads up to 16\nimproves the accuracy, especially over the devother datasets, as\nshown in Table 6.\nTable 6: Ablation study on the attention heads in multi-headed\nself attention.\nAttention\nHeads\nDim per\nHead\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\n4\n128\n1.9\n4.6\n2.0\n4.5\n8\n64\n1.9\n4.4\n2.1\n4.3\n16\n32\n2.0\n4.3\n2.2\n4.4\n32\n16\n1.9\n4.4\n2.1\n4.5\n3.4.5. Convolution Kernel Sizes\nTo study the effect of kernel sizes in the depthwise convolu-\ntion, we sweep the kernel size in {3, 7, 17, 32, 65} of the large\nmodel, using the same kernel size for all layers. We \ufb01nd that the\nperformance improves with larger kernel sizes till kernel sizes\n17 and 32 but worsens in the case of kernel size 65, as show\nin Table 7. On comparing the second decimal in dev WER, we\n\ufb01nd kernel size 32 to perform better than rest.\nTable 7: Ablation study on depthwise convolution kernel sizes.\nKernel\nsize\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\n3\n1.88\n4.41\n1.99\n4.39\n7\n1.88\n4.30\n2.02\n4.44\n17\n1.87\n4.31\n2.04\n4.38\n32\n1.83\n4.30\n2.03\n4.29\n65\n1.89\n4.47\n1.98\n4.46\n4. Conclusion\nIn this work, we introduced Conformer, an architecture that\nintegrates components from CNNs and Transformers for end-\nto-end speech recognition. We studied the importance of each\ncomponent, and demonstrated that the inclusion of convolution\nmodules is critical to the performance of the Conformer model.\nThe model exhibits better accuracy with fewer parameters than\nprevious work on the LibriSpeech dataset, and achieves a new\nstate-of-the-art performance at 1.9%/3.9% for test/testother.\n"
    },
    {
        "pdf_file": "paper1.pdf",
        "text": "Conformer: Convolution-augmented Transformer for Speech Recognition\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang\nGoogle Inc.\n{anmolgulati, jamesqin, chungchengc, nikip, ngyuzh, jiahuiyu, weihan, shibow, zhangzd,\nyonghui, rpang}@google.com\nAbstract\nRecently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs).\nTransformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\ncal features effectively. In this work, we achieve the best of\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies.\nOn the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%/4.3% without using a language model and 1.9%/3.9%\nwith an external language model on test/testother.\nWe also\nobserve competitive performance of 2.7%/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1. Introduction\nEnd-to-end automatic speech recognition (ASR) systems based\non neural networks have seen large improvements in recent\nyears. Recurrent neural networks (RNNs) have been the de-\nfacto choice for ASR [1, 2, 3, 4] as they can model the temporal\ndependencies in the audio sequences effectively [5]. Recently,\nthe Transformer architecture based on self-attention [6, 7] has\nenjoyed widespread adoption for modeling sequences due to its\nability to capture long distance interactions and the high train-\ning ef\ufb01ciency. Alternatively, convolutions have also been suc-\ncessful for ASR [8, 9, 10, 11, 12], which capture local context\nprogressively via a local receptive \ufb01eld layer by layer.\nHowever, models with self-attention or convolutions each\nhas its limitations. While Transformers are good at modeling\nlong-range global context, they are less capable to extract \ufb01ne-\ngrained local feature patterns.\nConvolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation. To combat this issue, contemporary work Con-\ntextNet [10] adopts the squeeze-and-excitation module [13] in\neach residual block to capture longer context. However, it is still\nlimited in capturing dynamic global context as it only applies a\nglobal averaging over the entire sequence.\nRecent works have shown that combining convolution and\nFeed Forward Module\nMulti-Head Self Attention\nModule\nConvolution Module\n1/2 x\n1/2 x\nFeed Forward Module\nDropout\nConformer Blocks\nLinear\nSpecAug\n10 ms rate\n10 ms rate\nConvolution\nSubsampling\n40 ms rate\nx N\n40 ms rate\n40 ms rate\n+\n+\n+\n+\nLayernorm\nFigure 1: Conformer encoder model architecture. Conformer\ncomprises of two macaron-like feed-forward layers with half-\nstep residual connections sandwiching the multi-headed self-\nattention and convolution modules. This is followed by a post\nlayernorm.\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.\n[17] proposed a multi-branch architecture with splitting the in-\nput into two branches: self-attention and convolution; and con-\ncatenating their outputs. Their work targeted mobile applica-\ntions and showed improvements in machine translation tasks.\nIn this work, we study how to organically combine con-\nvolutions with self-attention in ASR models. We hypothesize\nthat both global and local interactions are important for being\nparameter ef\ufb01cient. To achieve this, we propose a novel combi-\nnation of self-attention and convolution will achieve the best of\nboth worlds \u2013 self-attention learns the global interaction whilst\nthe convolutions ef\ufb01ciently capture the relative-offset-based lo-\ncal correlations. Inspired by Wu et al. [17, 18], we introduce\na novel combination of self-attention and convolution, sand-\nwiched between a pair feed forward modules, as illustrated in\nFig 1.\nOur proposed model, named Conformer, achieves state-of-\nthe-art results on LibriSpeech, outperforming the previous best\npublished Transformer Transducer [7] by 15% relative improve-\narXiv:2005.08100v1  [eess.AS]  16 May 2020\nLayernorm\nGlu\nActivation\nPointwise\nConv\nBatchNorm\nSwish\nActivation\n1D\nDepthwise\nConv\nPointwise\nConv\nDropout\n+\nFigure 2: Convolution module. The convolution module contains a pointwise convolution with an expansion factor of 2 projecting the\nnumber of channels with a GLU activation layer, followed by a 1-D Depthwise convolution. The 1-D depthwise conv is followed by a\nBatchnorm and then a swish activation layer.\nment on the testother dataset with an external language model.\nWe present three models based on model parameter limit con-\nstraints of 10M , 30M and 118M. Our 10M model shows an im-\nprovement when compared to similar sized contemporary work\n[10] with 2.7%/6.3% on test/testother datasets. Our medium\n30M parameters-sized model already outperforms transformer\ntransducer published in [7] which uses 139M model parameters.\nWith the big 118M parameter model, we are able to achieve\n2.1%/4.3% without using language models and 1.9%/3.9% with\nan external language model.\nWe further carefully study the effects of the number of at-\ntention heads, convolution kernel sizes, activation functions,\nplacement of feed-forward layers, and different strategies of\nadding convolution modules to a Transformer-based network,\nand shed light on how each contributes to the accuracy improve-\nments.\n2. Conformer Encoder\nOur audio encoder \ufb01rst processes the input with a convolution\nsubsampling layer and then with a number of conformer blocks,\nas illustrated in Figure 1. The distinctive feature of our model is\nthe use of Conformer blocks in the place of Transformer blocks\nas in [7, 19].\nA conformer block is composed of four modules stacked\ntogether, i.e, a feed-forward module, a self-attention module,\na convolution module, and a second feed-forward module in\nthe end. Sections 2.1, 1, and 2.3 introduce the self-attention,\nconvolution, and feed-forward modules, respectively. Finally,\n2.4 describes how these sub blocks are combined.\n2.1. Multi-Headed Self-Attention Module\nWe employ multi-headed self-attention (MHSA) while integrat-\ning an important technique from Transformer-XL [20], the rel-\native sinusoidal positional encoding scheme. The relative po-\nsitional encoding allows the self-attention module to general-\nize better on different input length and the resulting encoder is\nmore robust to the variance of the utterance length. We use pre-\nnorm residual units [21, 22] with dropout which helps training\nand regularizing deeper models. Figure 3 below illustrates the\nmulti-headed self-attention block.\nLayernorm\nMulti-Head Attention with\nRelative Positional\nEmbedding\nDropout\n+\nFigure 3: Multi-Headed self-attention module. We use multi-\nheaded self-attention with relative positional embedding in a\npre-norm residual unit.\n2.2. Convolution Module\nInspired by [17], the convolution module starts with a gating\nmechanism [23]\u2014a pointwise convolution and a gated linear\nunit (GLU). This is followed by a single 1-D depthwise convo-\nlution layer. Batchnorm is deployed just after the convolution\nto aid training deep models. Figure 2 illustrates the convolution\nblock.\n2.3. Feed Forward Module\nThe Transformer architecture as proposed in [6] deploys a feed\nforward module after the MHSA layer and is composed of two\nlinear transformations and a nonlinear activation in between. A\nresidual connection is added over the feed-forward layers, fol-\nlowed by layer normalization. This structure is also adopted by\nTransformer ASR models [7, 24].\nWe follow pre-norm residual units [21, 22] and apply layer\nnormalization within the residual unit and on the input before\nthe \ufb01rst linear layer. We also apply Swish activation [25] and\ndropout, which helps regularizing the network. Figure 4 illus-\ntrates the Feed Forward (FFN) module.\n2.4. Conformer Block\nOur proposed Conformer block contains two Feed Forward\nmodules sandwiching the Multi-Headed Self-Attention module\nand the Convolution module, as shown in Figure 1.\nThis sandwich structure is inspired by Macaron-Net [18],\nwhich proposes replacing the original feed-forward layer in the\nTransformer block into two half-step feed-forward layers, one\nbefore the attention layer and one after. As in Macron-Net, we\nemploy half-step residual weights in our feed-forward (FFN)\nmodules. The second feed-forward module is followed by a\n\ufb01nal layernorm layer. Mathematically, this means, for input xi\nto a Conformer block i, the output yi of the block is:\n\u02dcxi = xi + 1\n2FFN(xi)\nx\u2032\ni = \u02dcxi + MHSA( \u02dcxi)\nx\u2032\u2032\ni = x\u2032\ni + Conv(x\u2032\ni)\nyi = Layernorm(x\u2032\u2032\ni + 1\n2FFN(x\u2032\u2032\ni ))\n(1)\nwhere FFN refers to the Feed forward module, MHSA refers to\nthe Multi-Head Self-Attention module, and Conv refers to the\nConvolution module as described in the preceding sections.\nOur ablation study discussed in Sec 3.4.3 compares the\nMacaron-style half-step FFNs with the vanilla FFN as used in\nprevious works. We \ufb01nd that having two Macaron-net style\nfeed-forward layers with half-step residual connections sand-\nwiching the attention and convolution modules in between pro-\nvides a signi\ufb01cant improvement over having a single feed-\nforward module in our Conformer architecture.\nThe combination of convolution and self-attention has been\nstudied before and one can imagine many ways to achieve\nLayernorm\nLinear\nLayer\nDropout\nLinear\nLayer\nSwish\nActivation\nDropout\n+\nFigure 4: Feed forward module. The \ufb01rst linear layer uses an expansion factor of 4 and the second linear layer projects it back to the\nmodel dimension. We use swish activation and a pre-norm residual units in feed forward module.\nthat. Different options of augmenting convolutions with self-\nattention are studied in Sec 3.4.2. We found that convolution\nmodule stacked after the self-attention module works best for\nspeech recognition.\n3. Experiments\n3.1. Data\nWe evaluate the proposed model on the LibriSpeech [26]\ndataset, which consists of 970 hours of labeled speech and\nan additional 800M word token text-only corpus for building\nlanguage model. We extracted 80-channel \ufb01lterbanks features\ncomputed from a 25ms window with a stride of 10ms. We use\nSpecAugment [27, 28] with mask parameter (F = 27), and ten\ntime masks with maximum time-mask ratio (pS = 0.05), where\nthe maximum-size of the time mask is set to pS times the length\nof the utterance.\n3.2. Conformer Transducer\nWe identify three models, small, medium and large, with 10M,\n30M, and 118M params, respectively, by sweeping different\ncombinations of network depth, model dimensions, number of\nattention heads and choosing the best performing one within\nmodel parameter size constraints. We use a single-LSTM-layer\ndecoder in all our models. Table 1 describes their architecture\nhyper-parameters.\nFor regularization, we apply dropout [29] in each residual\nunit of the conformer, i.e, to the output of each module, before\nit is added to the module input. We use a rate of Pdrop = 0.1.\nVariational noise [5, 30] is introduced to the model as a regu-\nlarization. A \u21132 regularization with 1e \u22126 weight is also added\nto all the trainable weights in the network. We train the models\nwith the Adam optimizer [31] with \u03b21 = 0.9, \u03b22 = 0.98 and\n\u03f5 = 10\u22129 and a transformer learning rate schedule [6], with\n10k warm-up steps and peak learning rate 0.05/\n\u221a\nd where d is\nthe model dimension in conformer encoder.\nWe use a 3-layer LSTM language model (LM) with width\n4096 trained on the LibriSpeech langauge model corpus with\nthe LibriSpeech960h transcripts added, tokenized with the 1k\nWPM built from LibriSpeech 960h. The LM has word-level\nperplexity 63.9 on the dev-set transcripts. The LM weight \u03bb\nfor shallow fusion is tuned on the dev-set via grid search. All\nmodels are implemented with Lingvo toolkit [32].\n3.3. Results on LibriSpeech\nTable 2 compares the (WER) result of our model on Lib-\nriSpeech test-clean/test-other with a few state-of-the-art mod-\nels include: ContextNet [10], Transformer transducer [7], and\nQuartzNet [9]. All our evaluation results round up to 1 digit\nafter decimal point.\nWithout a language model,\nthe performance of our\nmedium model already achieve competitive results of 2.3/5.0\non test/testother outperforming the best known Transformer,\nLSTM based model, or a similar sized convolution model. With\nthe language model added, our model achieves the lowest word\nTable 1: Model hyper-parameters for Conformer S, M, and L\nmodels, found via sweeping different combinations and choos-\ning the best performing models within the parameter limits.\nModel\nConformer\n(S)\nConformer\n(M)\nConformer\n(L)\nNum Params (M)\n10.3\n30.7\n118.8\nEncoder Layers\n16\n16\n17\nEncoder Dim\n144\n256\n512\nAttention Heads\n4\n4\n8\nConv Kernel Size\n32\n32\n32\nDecoder Layers\n1\n1\n1\nDecoder Dim\n320\n640\n640\nTable 2: Comparison of Conformer with recent published mod-\nels. Our model shows improvements consistently over various\nmodel parameter size constraints. At 10.3M parameters, our\nmodel is 0.7% better on testother when compared to contempo-\nrary work, ContextNet(S) [10]. At 30.7M model parameters our\nmodel already signi\ufb01cantly outperforms the previous published\nstate of the art results of Transformer Transducer [7] with 139M\nparameters.\nMethod\n#Params (M)\nWER Without LM\nWER With LM\ntestclean\ntestother\ntestclean\ntestother\nHybrid\nTransformer [33]\n-\n-\n-\n2.26\n4.85\nCTC\nQuartzNet [9]\n19\n3.90\n11.28\n2.69\n7.25\nLAS\nTransformer [34]\n270\n2.89\n6.98\n2.33\n5.17\nTransformer [19]\n-\n2.2\n5.6\n2.6\n5.7\nLSTM\n360\n2.6\n6.0\n2.2\n5.2\nTransducer\nTransformer [7]\n139\n2.4\n5.6\n2.0\n4.6\nContextNet(S) [10]\n10.8\n2.9\n7.0\n2.3\n5.5\nContextNet(M) [10]\n31.4\n2.4\n5.4\n2.0\n4.5\nContextNet(L) [10]\n112.7\n2.1\n4.6\n1.9\n4.1\nConformer (Ours)\nConformer(S)\n10.3\n2.7\n6.3\n2.1\n5.0\nConformer(M)\n30.7\n2.3\n5.0\n2.0\n4.3\nConformer(L)\n118.8\n2.1\n4.3\n1.9\n3.9\nerror rate among all the existing models. This clearly demon-\nstrates the effectiveness of combining Transformer and convo-\nlution in a single neural network.\n3.4. Ablation Studies\n3.4.1. Conformer Block vs. Transformer Block\nA Conformer block differs from a Transformer block in a\nnumber of ways, in particular, the inclusion of a convolution\nblock and having a pair of FFNs surrounding the block in the\nMacaron-style. Below we study these effects of these differ-\nences by mutating a Conformer block towards a Transformer\nblock, while keeping the total number of parameters unchanged.\nTable 3 shows the impact of each change to the Conformer\nblock. Among all differences, convolution sub-block is the most\nimportant feature, while having a Macaron-style FFN pair is\nalso more effective than a single FFN of the same number of\nparameters. Using swish activations led to faster convergence\nin the Conformer models.\nTable 3: Disentangling Conformer. Starting from a Conformer\nblock, we remove its features and move towards a vanilla Trans-\nformer block: (1) replacing SWISH with ReLU; (2) remov-\ning the convolution sub-block; (3) replacing the Macaron-style\nFFN pairs with a single FFN; (4) replacing self-attention with\nrelative positional embedding [20] with a vanilla self-attention\nlayer [6]. All ablation study results are evaluated without the\nexternal LM.\nModel\nArchitecture\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\nConformer Model\n1.9\n4.4\n2.1\n4.3\n\u2013 SWISH + ReLU\n1.9\n4.4\n2.0\n4.5\n\u2013 Convolution Block\n2.1\n4.8\n2.1\n4.9\n\u2013 Macaron FFN\n2.1\n5.1\n2.1\n5.0\n\u2013 Relative Pos. Emb.\n2.3\n5.8\n2.4\n5.6\n3.4.2. Combinations of Convolution and Transformer Modules\nWe study the effects of various different ways of combining the\nmulti-headed self-attention (MHSA) module with the convolu-\ntion module. First, we try replacing the depthwise convolution\nin the convolution module with a lightweight convolution [35],\nsee a signi\ufb01cant drop in the performance especially on the dev-\nother dataset. Second, we study placing the convolution mod-\nule before the MHSA module in our Conformer model and \ufb01nd\nthat it degrades the results by 0.1 on dev-other. Another pos-\nsible way of the architecture is to split the input into parallel\nbranches of multi-headed self attention module and a convolu-\ntion module with their output concatenated as suggested in [17].\nWe found that this worsens the performance when compared to\nour proposed architecture.\nThese results in Table 4 suggest the advantage of placing\nthe convolution module after the self-attention module in the\nConformer block.\nTable 4: Ablation study of Conformer Attention Convolution\nBlocks. Varying the combination of the convolution block with\nthe multi-headed self attention: (1) Conformer architecture; (2)\nUsing Lightweight convolutions instead of depthwise convolu-\ntion in the convolution block in Conformer; (3) Convolution be-\nfore multi-headed self attention; (4) Convolution and MHSA in\nparallel with their output concatenated [17].\nModel Architecture\ndev\nclean\ndev\nother\nConformer\n1.9\n4.4\n\u2013 Depthwise conv + Lightweight convolution\n2.0\n4.8\nConvolution block before MHSA\n1.9\n4.5\nParallel MHSA and Convolution\n2.0\n4.9\n3.4.3. Macaron Feed Forward Modules\nInstead of a single feed-forward module (FFN) post the atten-\ntion blocks as in the Transformer models, the Conformer block\nhas a pair of macaron-like Feed forward modules sandwiching\nthe self-attention and convolution modules. Further, the Con-\nformer feed forward modules are used with half-step residuals.\nTable 5 shows the impact of changing the Conformer block to\nuse a single FFN or full-step residuals.\nTable 5: Ablation study of Macaron-net Feed Forward mod-\nules. Ablating the differences between the Conformer feed for-\nward module with that of a single FFN used in Transformer\nmodels: (1) Conformer; (2) Conformer with full-step residuals\nin Feed forward modules; (3) replacing the Macaron-style FFN\npair with a single FFN.\nModel\nArchitecture\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\nConformer\n1.9\n4.4\n2.1\n4.3\nSingle FFN\n1.9\n4.5\n2.1\n4.5\nFull step residuals\n1.9\n4.5\n2.1\n4.5\n3.4.4. Number of Attention Heads\nIn self-attention, each attention head learns to focus on different\nparts of the input, making it possible to improve predictions\nbeyond the simple weighted average. We perform experiments\nto study the effect of varying the number of attention heads from\n4 to 32 in our large model, using the same number of heads\nin all layers. We \ufb01nd that increasing attention heads up to 16\nimproves the accuracy, especially over the devother datasets, as\nshown in Table 6.\nTable 6: Ablation study on the attention heads in multi-headed\nself attention.\nAttention\nHeads\nDim per\nHead\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\n4\n128\n1.9\n4.6\n2.0\n4.5\n8\n64\n1.9\n4.4\n2.1\n4.3\n16\n32\n2.0\n4.3\n2.2\n4.4\n32\n16\n1.9\n4.4\n2.1\n4.5\n3.4.5. Convolution Kernel Sizes\nTo study the effect of kernel sizes in the depthwise convolu-\ntion, we sweep the kernel size in {3, 7, 17, 32, 65} of the large\nmodel, using the same kernel size for all layers. We \ufb01nd that the\nperformance improves with larger kernel sizes till kernel sizes\n17 and 32 but worsens in the case of kernel size 65, as show\nin Table 7. On comparing the second decimal in dev WER, we\n\ufb01nd kernel size 32 to perform better than rest.\nTable 7: Ablation study on depthwise convolution kernel sizes.\nKernel\nsize\ndev\nclean\ndev\nother\ntest\nclean\ntest\nother\n3\n1.88\n4.41\n1.99\n4.39\n7\n1.88\n4.30\n2.02\n4.44\n17\n1.87\n4.31\n2.04\n4.38\n32\n1.83\n4.30\n2.03\n4.29\n65\n1.89\n4.47\n1.98\n4.46\n4. Conclusion\nIn this work, we introduced Conformer, an architecture that\nintegrates components from CNNs and Transformers for end-\nto-end speech recognition. We studied the importance of each\ncomponent, and demonstrated that the inclusion of convolution\nmodules is critical to the performance of the Conformer model.\nThe model exhibits better accuracy with fewer parameters than\nprevious work on the LibriSpeech dataset, and achieves a new\nstate-of-the-art performance at 1.9%/3.9% for test/testother.\n5. References\n[1] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al., \u201cState-\nof-the-art speech recognition with sequence-to-sequence models,\u201d\nin 2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2018, pp. 4774\u20134778.\n[2] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures,\ndata and units for streaming end-to-end speech recognition with\nrnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU).\nIEEE, 2017, pp. 193\u2013199.\n[3] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez,\nD. Zhao, D. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang,\nD. Bhatia, Y. Shangguan, B. Li, G. Pundak, K. C. Sim, T. Bagby,\nS.-Y. Chang, K. Rao, and A. Gruenstein, \u201cStreaming End-to-end\nSpeech Recognition For Mobile Devices,\u201d in Proc. ICASSP, 2019.\n[4] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen, and et al., \u201cA streaming\non-device end-to-end model surpassing server-side conventional\nmodel quality and latency,\u201d in ICASSP, 2020.\n[5] A. Graves, \u201cSequence transduction with recurrent neural net-\nworks,\u201d arXiv preprint arXiv:1211.3711, 2012.\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\n2017.\n[7] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, \u201cTransformer transducer: A streamable speech recog-\nnition model with transformer encoders and rnn-t loss,\u201d in ICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nIEEE, 2020, pp. 7829\u20137833.\n[8] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Co-\nhen, H. Nguyen, and R. T. Gadde, \u201cJasper: An end-to-end convo-\nlutional neural acoustic model,\u201d arXiv preprint arXiv:1904.03288,\n2019.\n[9] S. Kriman, S. Beliaev, B. Ginsburg, J. Huang, O. Kuchaiev,\nV. Lavrukhin, R. Leary, J. Li, and Y. Zhang, \u201cQuartznet: Deep\nautomatic speech recognition with 1d time-channel separable con-\nvolutions,\u201d arXiv preprint arXiv:1910.10261, 2019.\n[10] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati,\nR. Pang, and Y. Wu, \u201cContextnet: Improving convolutional neural\nnetworks for automatic speech recognition with global context,\u201d\narXiv preprint arXiv:2005.03191, 2020.\n[11] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhad-\nran, \u201cDeep convolutional neural networks for lvcsr,\u201d in 2013 IEEE\ninternational conference on acoustics, speech and signal process-\ning.\nIEEE, 2013, pp. 8614\u20138618.\n[12] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn,\nand D. Yu, \u201cConvolutional neural networks for speech recogni-\ntion,\u201d IEEE/ACM Transactions on audio, speech, and language\nprocessing, vol. 22, no. 10, pp. 1533\u20131545, 2014.\n[13] J. Hu, L. Shen, and G. Sun, \u201cSqueeze-and-excitation networks,\u201d\nin Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7132\u20137141.\n[14] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, \u201cAttention\naugmented convolutional networks,\u201d in Proceedings of the IEEE\nInternational Conference on Computer Vision, 2019, pp. 3286\u2013\n3295.\n[15] B. Yang, L. Wang, D. Wong, L. S. Chao, and Z. Tu, \u201cConvolu-\ntional self-attention networks,\u201d arXiv preprint arXiv:1904.03107,\n2019.\n[16] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi,\nand Q. V. Le, \u201cQanet:\nCombining local convolution with\nglobal self-attention for reading comprehension,\u201d arXiv preprint\narXiv:1804.09541, 2018.\n[17] Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, \u201cLite transformer with\nlong-short range attention,\u201d arXiv preprint arXiv:2004.11886,\n2020.\n[18] Y. Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and\nT.-Y. Liu, \u201cUnderstanding and improving transformer from a\nmulti-particle dynamic system point of view,\u201d arXiv preprint\narXiv:1906.02762, 2019.\n[19] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., \u201cA\ncomparative study on transformer vs rnn in speech applications,\u201d\narXiv preprint arXiv:1909.06317, 2019.\n[20] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhut-\ndinov, \u201cTransformer-xl:\nAttentive language models beyond a\n\ufb01xed-length context,\u201d 2019.\n[21] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao,\n\u201cLearning deep transformer models for machine translation,\u201d in\nProceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics.\nAssociation for Computational Lin-\nguistics, Jul. 2019, pp. 1810\u20131822.\n[22] T. Q. Nguyen and J. Salazar, \u201cTransformers without tears:\nImproving the normalization of self-attention,\u201d arXiv preprint\narXiv:1910.05895, 2019.\n[23] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, \u201cLanguage\nmodeling with gated convolutional networks,\u201d in Proceedings of\nthe 34th International Conference on Machine Learning-Volume\n70.\nJMLR. org, 2017, pp. 933\u2013941.\n[24] L. Dong, S. Xu, and B. Xu, \u201cSpeech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,\u201d in 2018\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2018, pp. 5884\u20135888.\n[25] P. Ramachandran, B. Zoph, and Q. V. Le, \u201cSearching for activa-\ntion functions,\u201d arXiv preprint arXiv:1710.05941, 2017.\n[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech: an asr corpus based on public domain audio books,\u201d\nin 2015 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2015, pp. 5206\u20135210.\n[27] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V. Le, \u201cSpecaugment: A simple data augmen-\ntation method for automatic speech recognition,\u201d arXiv preprint\narXiv:1904.08779, 2019.\n[28] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V.\nLe, and Y. Wu, \u201cSpecaugment on large scale datasets,\u201d arXiv\npreprint arXiv:1912.05533, 2019.\n[29] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, \u201cDropout: A simple way to prevent neural net-\nworks from over\ufb01tting,\u201d Journal of Machine Learning Research,\nvol. 15, no. 56, pp. 1929\u20131958, 2014.\n[30] K.-C. Jim, C. L. Giles, and B. G. Horne, \u201cAn analysis of noise\nin recurrent neural networks: convergence and generalization,\u201d\nIEEE Transactions on neural networks, vol. 7, no. 6, pp. 1424\u2013\n1438, 1996.\n[31] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic opti-\nmization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[32] J. Shen, P. Nguyen, Y. Wu, Z. Chen, and et al., \u201cLingvo: a modu-\nlar and scalable framework for sequence-to-sequence modeling,\u201d\n2019.\n[33] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang et al., \u201cTransformer-\nbased acoustic modeling for hybrid speech recognition,\u201d arXiv\npreprint arXiv:1910.09799, 2019.\n[34] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave,\nV. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert, \u201cEnd-to-\nend asr: from supervised to semi-supervised learning with modern\narchitectures,\u201d 2019.\n[35] F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli, \u201cPay\nless attention with lightweight and dynamic convolutions,\u201d arXiv\npreprint arXiv:1901.10430, 2019.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "SpecAugment: A Simple Data Augmentation Method\nfor Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art results even with-\nout the use of Language Models (LMs).\nOn LibriSpeech\n[20], we achieve 2.8% Word Error Rate (WER) on the test-\nclean set and 6.8% WER on the test-other set, without the\nuse of an LM. Upon shallow fusion [21] with an LM trained\non the LibriSpeech LM corpus, we are able to better our per-\nformance (2.5% WER on test-clean and 5.8% WER on test-\nother), improving the current state of the art on test-other by\n22% relatively. On Switchboard 300h (LDC97S62) [22], we\nobtain 7.2% WER on the Switchboard portion of the Hub5\u201900\n(LDC2002S09, LDC2003T02) test set, and 14.6% on the Call-\nHome portion, without using an LM. Upon shallow fusion\nwith an LM trained on the combined transcript of the Switch-\nboard and Fisher (LDC200{4,5}T19) [23] corpora, we obtain\n6.8%/14.1% WER on the Switchboard/Callhome portion.\n2. Augmentation Policy\nWe aim to construct an augmentation policy that acts on the log\nmel spectrogram directly, which helps the network learn use-\nful features. Motivated by the goal that these features should\nbe robust to deformations in the time direction, partial loss of\nfrequency information and partial loss of small segments of\nspeech, we have chosen the following deformations to make up\na policy:\n1. Time\nwarping\nis\napplied\nvia\nthe\nfunction\nsparse image warp of tensorflow.\nGiven\na log mel spectrogram with \u03c4 time steps, we view it\nas an image where the time axis is horizontal and the\nfrequency axis is vertical.\nA random point along the\nhorizontal line passing through the center of the image\nwithin the time steps (W, \u03c4 \u2212W) is to be warped either\nto the left or right by a distance w chosen from a uniform\ndistribution from 0 to the time warp parameter W along\nthat line. We \ufb01x six anchor points on the boundary\u2014the\nfour corners and the mid-points of the vertical edges.\n2. Frequency masking is applied so that f consecutive mel\nfrequency channels [f0, f0 + f) are masked, where f is\n\ufb01rst chosen from a uniform distribution from 0 to the\narXiv:1904.08779v3  [eess.AS]  3 Dec 2019\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "SpecAugment: A Simple Data Augmentation Method\nfor Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art results even with-\nout the use of Language Models (LMs).\nOn LibriSpeech\n[20], we achieve 2.8% Word Error Rate (WER) on the test-\nclean set and 6.8% WER on the test-other set, without the\nuse of an LM. Upon shallow fusion [21] with an LM trained\non the LibriSpeech LM corpus, we are able to better our per-\nformance (2.5% WER on test-clean and 5.8% WER on test-\nother), improving the current state of the art on test-other by\n22% relatively. On Switchboard 300h (LDC97S62) [22], we\nobtain 7.2% WER on the Switchboard portion of the Hub5\u201900\n(LDC2002S09, LDC2003T02) test set, and 14.6% on the Call-\nHome portion, without using an LM. Upon shallow fusion\nwith an LM trained on the combined transcript of the Switch-\nboard and Fisher (LDC200{4,5}T19) [23] corpora, we obtain\n6.8%/14.1% WER on the Switchboard/Callhome portion.\n2. Augmentation Policy\nWe aim to construct an augmentation policy that acts on the log\nmel spectrogram directly, which helps the network learn use-\nful features. Motivated by the goal that these features should\nbe robust to deformations in the time direction, partial loss of\nfrequency information and partial loss of small segments of\nspeech, we have chosen the following deformations to make up\na policy:\n1. Time\nwarping\nis\napplied\nvia\nthe\nfunction\nsparse image warp of tensorflow.\nGiven\na log mel spectrogram with \u03c4 time steps, we view it\nas an image where the time axis is horizontal and the\nfrequency axis is vertical.\nA random point along the\nhorizontal line passing through the center of the image\nwithin the time steps (W, \u03c4 \u2212W) is to be warped either\nto the left or right by a distance w chosen from a uniform\ndistribution from 0 to the time warp parameter W along\nthat line. We \ufb01x six anchor points on the boundary\u2014the\nfour corners and the mid-points of the vertical edges.\n2. Frequency masking is applied so that f consecutive mel\nfrequency channels [f0, f0 + f) are masked, where f is\n\ufb01rst chosen from a uniform distribution from 0 to the\narXiv:1904.08779v3  [eess.AS]  3 Dec 2019\nFigure 1: Augmentations applied to the base input, given at the\ntop. From top to bottom, the \ufb01gures depict the log mel spectro-\ngram of the base input with no augmentation, time warp, fre-\nquency masking and time masking applied.\nFigure 2: Augmentation policies applied to the base input. From\ntop to bottom, the \ufb01gures depict the log mel spectrogram of the\nbase input with policies None, LB and LD applied.\nfrequency mask parameter F, and f0 is chosen from\n[0, \u03bd \u2212f). \u03bd is the number of mel frequency channels.\n3. Time masking is applied so that t consecutive time steps\n[t0, t0 + t) are masked, where t is \ufb01rst chosen from a\nuniform distribution from 0 to the time mask parameter\nT, and t0 is chosen from [0, \u03c4 \u2212t).\n\u2022 We introduce an upper bound on the time mask so\nthat a time mask cannot be wider than p times the\nnumber of time steps.\nFigure 1 shows examples of the individual augmentations ap-\nplied to a single input. The log mel spectrograms are normal-\nized to have zero mean value, and thus setting the masked value\nto zero is equivalent to setting it to the mean value.\nWe can consider policies where multiple frequency and\ntime masks are applied. The multiple masks may overlap. In\nthis work, we mainly consider a series of hand-crafted policies,\nLibriSpeech basic (LB), LibriSpeech double (LD), Switchboard\nmild (SM) and Switchboard strong (SS) whose parameters are\nsummarized in Table 1. In Figure 2, we show an example of a\nlog mel spectrogram augmented with policies LB and LD.\nTable 1: Augmentation parameters for policies. mF and mT\ndenote the number of frequency and time masks applied.\nPolicy\nW\nF\nmF\nT\np\nmT\nNone\n0\n0\n-\n0\n-\n-\nLB\n80\n27\n1\n100\n1.0\n1\nLD\n80\n27\n2\n100\n1.0\n2\nSM\n40\n15\n2\n70\n0.2\n2\nSS\n40\n27\n2\n70\n0.2\n2\n3. Model\nWe use Listen, Attend and Spell (LAS) networks [6] for our\nASR tasks. These models, being end-to-end, are simple to train\nand have the added bene\ufb01t of having well-documented bench-\nmarks [24, 25] that we are able to build upon to get our results.\nIn this section, we review LAS networks and introduce some\nnotation to parameterize them. We also introduce the learning\nrate schedules we use to train the networks, as they turn out\nto be an important factor in determining performance. We end\nwith reviewing shallow fusion [21], which we have used to in-\ncorporate language models for further gains in performance.\n3.1. LAS Network Architectures\nWe use Listen, Attend and Spell (LAS) networks [6] for end-to-\nend ASR studied in [25], for which we use the notation LAS-\nd-w. The input log mel spectrogram is passed in to a 2-layer\nConvolutional Neural Network (CNN) with max-pooling and\nstride of 2. The output of the CNN is passes through an en-\ncoder consisting of d stacked bi-directional LSTMs with cell\nsize w to yield a series of attention vectors. The attention vec-\ntors are fed into a 2-layer RNN decoder of cell dimension w,\nwhich yields the tokens for the transcript. The text is tokenized\nusing a Word Piece Model (WPM) [26] of vocabulary size 16k\nfor LibriSpeech and 1k for Switchboard. The WPM for Lib-\nriSpeech 960h is constructed using the training set transcripts.\nFor the Switchboard 300h task, transcripts from the training set\nare combined with those of the Fisher corpus to construct the\nWPM. The \ufb01nal transcripts are obtained by a beam search with\nbeam size 8. For comparison with [25], we note that their \u201clarge\nmodel\u201d in our notation is LAS-4-1024.\n3.2. Learning Rate Schedules\nThe learning rate schedule turns out to be an important factor\nin determining the performance of ASR networks, especially\nso when augmentation is present. Here, we introduce training\nschedules that serve two purposes. First, we use these schedules\nto verify that a longer schedule improves the \ufb01nal performance\nof the network, even more so with augmentation (Table 2). Sec-\nond, based on this, we introduce very long schedules that are\nused to maximize the performance of the networks.\nWe use a learning rate schedule in which we ramp-up, hold,\nthen exponentially decay the learning rate until it reaches 1/100\nof its maximum value. The learning rate is kept constant beyond\nthis point. This schedule is parameterized by three time stamps\n(sr, si, sf) \u2013 the step sr where the ramp-up (from zero learning\nrate) is complete, the step si where exponential decay starts, and\nthe step sf where the exponential decay stops.\nThere are two more factors that introduce time scales in our\nexperiment. First, we turn on a variational weight noise [27]\nof standard deviation 0.075 at step snoise and keep it constant\nthroughout training. Weight noise is introduced in the step in-\nterval (sr, si), i.e., during the high plateau of the learning rate.\nSecond, we introduce uniform label smoothing [28] with\nuncertainty 0.1, i.e., the correct class label is assigned the con\ufb01-\ndence 0.9, while the con\ufb01dence of the other labels are increased\naccordingly. As is commented on again later on, label smooth-\ning can destabilize training for smaller learning rates, and we\nsometimes choose to turn it on only at the beginning of training\nand off when the learning rate starts to decay.\nThe two basic schedules we use, are given as the following:\n1. B(asic): (sr, snoise, si, sf) = (0.5k, 10k, 20k, 80k)\n2. D(ouble): (sr, snoise, si, sf) = (1k, 20k, 40k, 160k)\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "SpecAugment: A Simple Data Augmentation Method\nfor Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art results even with-\nout the use of Language Models (LMs).\nOn LibriSpeech\n[20], we achieve 2.8% Word Error Rate (WER) on the test-\nclean set and 6.8% WER on the test-other set, without the\nuse of an LM. Upon shallow fusion [21] with an LM trained\non the LibriSpeech LM corpus, we are able to better our per-\nformance (2.5% WER on test-clean and 5.8% WER on test-\nother), improving the current state of the art on test-other by\n22% relatively. On Switchboard 300h (LDC97S62) [22], we\nobtain 7.2% WER on the Switchboard portion of the Hub5\u201900\n(LDC2002S09, LDC2003T02) test set, and 14.6% on the Call-\nHome portion, without using an LM. Upon shallow fusion\nwith an LM trained on the combined transcript of the Switch-\nboard and Fisher (LDC200{4,5}T19) [23] corpora, we obtain\n6.8%/14.1% WER on the Switchboard/Callhome portion.\n2. Augmentation Policy\nWe aim to construct an augmentation policy that acts on the log\nmel spectrogram directly, which helps the network learn use-\nful features. Motivated by the goal that these features should\nbe robust to deformations in the time direction, partial loss of\nfrequency information and partial loss of small segments of\nspeech, we have chosen the following deformations to make up\na policy:\n1. Time\nwarping\nis\napplied\nvia\nthe\nfunction\nsparse image warp of tensorflow.\nGiven\na log mel spectrogram with \u03c4 time steps, we view it\nas an image where the time axis is horizontal and the\nfrequency axis is vertical.\nA random point along the\nhorizontal line passing through the center of the image\nwithin the time steps (W, \u03c4 \u2212W) is to be warped either\nto the left or right by a distance w chosen from a uniform\ndistribution from 0 to the time warp parameter W along\nthat line. We \ufb01x six anchor points on the boundary\u2014the\nfour corners and the mid-points of the vertical edges.\n2. Frequency masking is applied so that f consecutive mel\nfrequency channels [f0, f0 + f) are masked, where f is\n\ufb01rst chosen from a uniform distribution from 0 to the\narXiv:1904.08779v3  [eess.AS]  3 Dec 2019\nFigure 1: Augmentations applied to the base input, given at the\ntop. From top to bottom, the \ufb01gures depict the log mel spectro-\ngram of the base input with no augmentation, time warp, fre-\nquency masking and time masking applied.\nFigure 2: Augmentation policies applied to the base input. From\ntop to bottom, the \ufb01gures depict the log mel spectrogram of the\nbase input with policies None, LB and LD applied.\nfrequency mask parameter F, and f0 is chosen from\n[0, \u03bd \u2212f). \u03bd is the number of mel frequency channels.\n3. Time masking is applied so that t consecutive time steps\n[t0, t0 + t) are masked, where t is \ufb01rst chosen from a\nuniform distribution from 0 to the time mask parameter\nT, and t0 is chosen from [0, \u03c4 \u2212t).\n\u2022 We introduce an upper bound on the time mask so\nthat a time mask cannot be wider than p times the\nnumber of time steps.\nFigure 1 shows examples of the individual augmentations ap-\nplied to a single input. The log mel spectrograms are normal-\nized to have zero mean value, and thus setting the masked value\nto zero is equivalent to setting it to the mean value.\nWe can consider policies where multiple frequency and\ntime masks are applied. The multiple masks may overlap. In\nthis work, we mainly consider a series of hand-crafted policies,\nLibriSpeech basic (LB), LibriSpeech double (LD), Switchboard\nmild (SM) and Switchboard strong (SS) whose parameters are\nsummarized in Table 1. In Figure 2, we show an example of a\nlog mel spectrogram augmented with policies LB and LD.\nTable 1: Augmentation parameters for policies. mF and mT\ndenote the number of frequency and time masks applied.\nPolicy\nW\nF\nmF\nT\np\nmT\nNone\n0\n0\n-\n0\n-\n-\nLB\n80\n27\n1\n100\n1.0\n1\nLD\n80\n27\n2\n100\n1.0\n2\nSM\n40\n15\n2\n70\n0.2\n2\nSS\n40\n27\n2\n70\n0.2\n2\n3. Model\nWe use Listen, Attend and Spell (LAS) networks [6] for our\nASR tasks. These models, being end-to-end, are simple to train\nand have the added bene\ufb01t of having well-documented bench-\nmarks [24, 25] that we are able to build upon to get our results.\nIn this section, we review LAS networks and introduce some\nnotation to parameterize them. We also introduce the learning\nrate schedules we use to train the networks, as they turn out\nto be an important factor in determining performance. We end\nwith reviewing shallow fusion [21], which we have used to in-\ncorporate language models for further gains in performance.\n3.1. LAS Network Architectures\nWe use Listen, Attend and Spell (LAS) networks [6] for end-to-\nend ASR studied in [25], for which we use the notation LAS-\nd-w. The input log mel spectrogram is passed in to a 2-layer\nConvolutional Neural Network (CNN) with max-pooling and\nstride of 2. The output of the CNN is passes through an en-\ncoder consisting of d stacked bi-directional LSTMs with cell\nsize w to yield a series of attention vectors. The attention vec-\ntors are fed into a 2-layer RNN decoder of cell dimension w,\nwhich yields the tokens for the transcript. The text is tokenized\nusing a Word Piece Model (WPM) [26] of vocabulary size 16k\nfor LibriSpeech and 1k for Switchboard. The WPM for Lib-\nriSpeech 960h is constructed using the training set transcripts.\nFor the Switchboard 300h task, transcripts from the training set\nare combined with those of the Fisher corpus to construct the\nWPM. The \ufb01nal transcripts are obtained by a beam search with\nbeam size 8. For comparison with [25], we note that their \u201clarge\nmodel\u201d in our notation is LAS-4-1024.\n3.2. Learning Rate Schedules\nThe learning rate schedule turns out to be an important factor\nin determining the performance of ASR networks, especially\nso when augmentation is present. Here, we introduce training\nschedules that serve two purposes. First, we use these schedules\nto verify that a longer schedule improves the \ufb01nal performance\nof the network, even more so with augmentation (Table 2). Sec-\nond, based on this, we introduce very long schedules that are\nused to maximize the performance of the networks.\nWe use a learning rate schedule in which we ramp-up, hold,\nthen exponentially decay the learning rate until it reaches 1/100\nof its maximum value. The learning rate is kept constant beyond\nthis point. This schedule is parameterized by three time stamps\n(sr, si, sf) \u2013 the step sr where the ramp-up (from zero learning\nrate) is complete, the step si where exponential decay starts, and\nthe step sf where the exponential decay stops.\nThere are two more factors that introduce time scales in our\nexperiment. First, we turn on a variational weight noise [27]\nof standard deviation 0.075 at step snoise and keep it constant\nthroughout training. Weight noise is introduced in the step in-\nterval (sr, si), i.e., during the high plateau of the learning rate.\nSecond, we introduce uniform label smoothing [28] with\nuncertainty 0.1, i.e., the correct class label is assigned the con\ufb01-\ndence 0.9, while the con\ufb01dence of the other labels are increased\naccordingly. As is commented on again later on, label smooth-\ning can destabilize training for smaller learning rates, and we\nsometimes choose to turn it on only at the beginning of training\nand off when the learning rate starts to decay.\nThe two basic schedules we use, are given as the following:\n1. B(asic): (sr, snoise, si, sf) = (0.5k, 10k, 20k, 80k)\n2. D(ouble): (sr, snoise, si, sf) = (1k, 20k, 40k, 160k)\nAs discussed further in section 5, we can improve the perfor-\nmance of the trained network by using a longer schedule. We\nthus introduce the following schedule:\n3. L(ong): (sr, snoise, si, sf) = (1k, 20k, 140k, 320k)\nwhich we use to train the largest model to improve performance.\nWhen using schedule L, label smoothing with uncertainty 0.1 is\nintroduced for time steps < si = 140k for LibriSpeech 960h,\nand is subsequently turned off. For Switchboard 300h, label\nsmoothing is turned on throughout training.\n3.3. Shallow Fusion with Language Models\nWhile we are able to get state-of-the-art results with augmen-\ntation, we can get further improvements by using a language\nmodel. We thus incorporate an RNN language model by shal-\nlow fusion for both tasks. In shallow fusion, the \u201cnext token\u201d\ny\u2217in the decoding process is determined by\ny\u2217= argmax\ny\n(log P(y|x) + \u03bb log PLM(y)) ,\n(1)\ni.e., by jointly scoring the token using the base ASR model and\nthe language model. We also use a coverage penalty c [29].\nFor LibriSpeech, we use a two-layer RNN with embedding\ndimension 1024 used in [25] for the LM, which is trained on\nthe LibriSpeech LM corpus. We use identical fusion parameters\n(\u03bb = 0.35 and c = 0.05) used in [25] throughout.\nFor Switchboard, we use a two-layer RNN with embedding\ndimension 256, which is trained on the combined transcripts of\nthe Fisher and Switchboard datasets. We \ufb01nd the fusion pa-\nrameters via grid search by measuring performance on RT-03\n(LDC2007S10). We discuss the fusion parameters used in indi-\nvidual experiments in section 4.2.\n4. Experiments\nIn this section, we describe our experiments on LibriSpeech and\nSwitchboard with SpecAugment. We report state-of-the-art re-\nsults that out-perform heavily engineered hybrid systems.\n4.1. LibriSpeech 960h\nFor LibriSpeech, we use the same setup as [25], where we use\n80-dimensional \ufb01lter banks with delta and delta-delta accelera-\ntion, and a 16k word piece model [26].\nThe three networks LAS-4-1024, LAS-6-1024 and LAS-6-\n1280 are trained on LibriSpeech 960h with a combination of\naugmentation policies (None, LB, LD) and training schedules\n(B/D). Label smoothing was not applied in these experiments.\nThe experiments were run with peak learning rate of 0.001 and\nbatch size of 512, on 32 Google Cloud TPU chips for 7 days.\nOther than the augmentation policies and learning rate sched-\nules, all other hyperparameters were \ufb01xed, and no additional\ntuning was applied. We report test set numbers validated by the\ndev-other set in Table 2. We see that augmentation consistently\nimproves the performance of the trained network, and that the\nbene\ufb01t of a larger network and a longer learning rate schedule\nis more apparent with harsher augmentation.\nWe take the largest network, LAS-6-1280, and use sched-\nule L (with training time \u223c24 days) and policy LD to train\nthe network to maximize performance. We turn label smooth-\ning on for time steps < 140k as noted before. The test set\nperformance is reported by evaluating the checkpoint with best\ndev-other performance. State of the art performance is achieved\nby the LAS-6-1280 model, even without a language model. We\nTable 2: LibriSpeech test WER (%) evaluated for varying net-\nworks, schedules and policies. First row from [25].\nNetwork\nSch\nPol\nNo LM\nWith LM\nclean\nother\nclean\nother\nLAS-4-1024 [25]\nB\n-\n4.7\n13.4\n3.6\n10.3\nLAS-4-1024\nB\nLB\n3.7\n10.0\n3.4\n8.3\nB\nLD\n3.6\n9.2\n2.8\n7.5\nD\n-\n4.4\n13.3\n3.5\n10.4\nD\nLB\n3.4\n9.2\n2.7\n7.3\nD\nLD\n3.4\n8.3\n2.8\n6.8\nLAS-6-1024\nD\n-\n4.5\n13.1\n3.6\n10.3\nD\nLB\n3.4\n8.6\n2.6\n6.7\nD\nLD\n3.2\n8.0\n2.6\n6.5\nLAS-6-1280\nD\n-\n4.3\n12.9\n3.5\n10.5\nD\nLB\n3.4\n8.7\n2.8\n7.1\nD\nLD\n3.2\n7.7\n2.7\n6.5\ncan incorporate an LM using shallow fusion to further improve\nperformance. The results are presented in Table 3.\nTable 3: LibriSpeech 960h WERs (%).\nMethod\nNo LM\nWith LM\nclean\nother\nclean\nother\nHMM\nPanayotov et al., (2015) [20]\n5.51\n13.97\nPovey et al., (2016) [30]\n4.28\nHan et al., (2017) [31]\n3.51\n8.58\nYang et al. (2018) [32]\n2.97\n7.50\nCTC/ASG\nCollobert et al., (2016) [33]\n7.2\nLiptchinsky et al., (2017) [34]\n6.7\n20.8\n4.8\n14.5\nZhou et al., (2018) [35]\n5.42\n14.70\nZeghidour et al., (2018) [36]\n3.44\n11.24\nLi et al., (2019) [37]\n3.86\n11.95\n2.95\n8.79\nLAS\nZeyer et al., (2018) [24]\n4.87\n15.39\n3.82\n12.76\nZeyer et al., (2018) [38]\n4.70\n15.20\nIrie et al., (2019) [25]\n4.7\n13.4\n3.6\n10.3\nSabour et al., (2019) [39]\n4.5\n13.3\nOur Work\nLAS\n4.1\n12.5\n3.2\n9.8\nLAS + SpecAugment\n2.8\n6.8\n2.5\n5.8\n4.2. Switchboard 300h\nFor Switchboard 300h, we use the Kaldi [40] \u201cs5c\u201d recipe to\nprocess our data, but we adapt the recipe to use 80-dimensional\n\ufb01lter banks with delta and delta-delta acceleration. We use a 1k\nWPM [26] to tokenize the output, constructed using the com-\nbined vocabulary of the Switchboard and Fisher transcripts.\nWe train LAS-4-1024 with policies (None, SM, SS) and\nschedule B. As before, we set the peak learning rate to 0.001\nand total batch size to 512, and train using 32 Google Cloud\nTPU chips. Here the experiments are run with and without la-\nbel smoothing. Not having a canonical development set, we\nchoose to evaluate the checkpoint at the end point of the train-\ning schedule, which we choose to be 100k steps for schedule B.\nWe note that the training curve relaxes after the decay schedule\nis completed (step sf), and the performance of the network does\nnot vary much. The performance of various augmentation poli-\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "SpecAugment: A Simple Data Augmentation Method\nfor Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art results even with-\nout the use of Language Models (LMs).\nOn LibriSpeech\n[20], we achieve 2.8% Word Error Rate (WER) on the test-\nclean set and 6.8% WER on the test-other set, without the\nuse of an LM. Upon shallow fusion [21] with an LM trained\non the LibriSpeech LM corpus, we are able to better our per-\nformance (2.5% WER on test-clean and 5.8% WER on test-\nother), improving the current state of the art on test-other by\n22% relatively. On Switchboard 300h (LDC97S62) [22], we\nobtain 7.2% WER on the Switchboard portion of the Hub5\u201900\n(LDC2002S09, LDC2003T02) test set, and 14.6% on the Call-\nHome portion, without using an LM. Upon shallow fusion\nwith an LM trained on the combined transcript of the Switch-\nboard and Fisher (LDC200{4,5}T19) [23] corpora, we obtain\n6.8%/14.1% WER on the Switchboard/Callhome portion.\n2. Augmentation Policy\nWe aim to construct an augmentation policy that acts on the log\nmel spectrogram directly, which helps the network learn use-\nful features. Motivated by the goal that these features should\nbe robust to deformations in the time direction, partial loss of\nfrequency information and partial loss of small segments of\nspeech, we have chosen the following deformations to make up\na policy:\n1. Time\nwarping\nis\napplied\nvia\nthe\nfunction\nsparse image warp of tensorflow.\nGiven\na log mel spectrogram with \u03c4 time steps, we view it\nas an image where the time axis is horizontal and the\nfrequency axis is vertical.\nA random point along the\nhorizontal line passing through the center of the image\nwithin the time steps (W, \u03c4 \u2212W) is to be warped either\nto the left or right by a distance w chosen from a uniform\ndistribution from 0 to the time warp parameter W along\nthat line. We \ufb01x six anchor points on the boundary\u2014the\nfour corners and the mid-points of the vertical edges.\n2. Frequency masking is applied so that f consecutive mel\nfrequency channels [f0, f0 + f) are masked, where f is\n\ufb01rst chosen from a uniform distribution from 0 to the\narXiv:1904.08779v3  [eess.AS]  3 Dec 2019\nFigure 1: Augmentations applied to the base input, given at the\ntop. From top to bottom, the \ufb01gures depict the log mel spectro-\ngram of the base input with no augmentation, time warp, fre-\nquency masking and time masking applied.\nFigure 2: Augmentation policies applied to the base input. From\ntop to bottom, the \ufb01gures depict the log mel spectrogram of the\nbase input with policies None, LB and LD applied.\nfrequency mask parameter F, and f0 is chosen from\n[0, \u03bd \u2212f). \u03bd is the number of mel frequency channels.\n3. Time masking is applied so that t consecutive time steps\n[t0, t0 + t) are masked, where t is \ufb01rst chosen from a\nuniform distribution from 0 to the time mask parameter\nT, and t0 is chosen from [0, \u03c4 \u2212t).\n\u2022 We introduce an upper bound on the time mask so\nthat a time mask cannot be wider than p times the\nnumber of time steps.\nFigure 1 shows examples of the individual augmentations ap-\nplied to a single input. The log mel spectrograms are normal-\nized to have zero mean value, and thus setting the masked value\nto zero is equivalent to setting it to the mean value.\nWe can consider policies where multiple frequency and\ntime masks are applied. The multiple masks may overlap. In\nthis work, we mainly consider a series of hand-crafted policies,\nLibriSpeech basic (LB), LibriSpeech double (LD), Switchboard\nmild (SM) and Switchboard strong (SS) whose parameters are\nsummarized in Table 1. In Figure 2, we show an example of a\nlog mel spectrogram augmented with policies LB and LD.\nTable 1: Augmentation parameters for policies. mF and mT\ndenote the number of frequency and time masks applied.\nPolicy\nW\nF\nmF\nT\np\nmT\nNone\n0\n0\n-\n0\n-\n-\nLB\n80\n27\n1\n100\n1.0\n1\nLD\n80\n27\n2\n100\n1.0\n2\nSM\n40\n15\n2\n70\n0.2\n2\nSS\n40\n27\n2\n70\n0.2\n2\n3. Model\nWe use Listen, Attend and Spell (LAS) networks [6] for our\nASR tasks. These models, being end-to-end, are simple to train\nand have the added bene\ufb01t of having well-documented bench-\nmarks [24, 25] that we are able to build upon to get our results.\nIn this section, we review LAS networks and introduce some\nnotation to parameterize them. We also introduce the learning\nrate schedules we use to train the networks, as they turn out\nto be an important factor in determining performance. We end\nwith reviewing shallow fusion [21], which we have used to in-\ncorporate language models for further gains in performance.\n3.1. LAS Network Architectures\nWe use Listen, Attend and Spell (LAS) networks [6] for end-to-\nend ASR studied in [25], for which we use the notation LAS-\nd-w. The input log mel spectrogram is passed in to a 2-layer\nConvolutional Neural Network (CNN) with max-pooling and\nstride of 2. The output of the CNN is passes through an en-\ncoder consisting of d stacked bi-directional LSTMs with cell\nsize w to yield a series of attention vectors. The attention vec-\ntors are fed into a 2-layer RNN decoder of cell dimension w,\nwhich yields the tokens for the transcript. The text is tokenized\nusing a Word Piece Model (WPM) [26] of vocabulary size 16k\nfor LibriSpeech and 1k for Switchboard. The WPM for Lib-\nriSpeech 960h is constructed using the training set transcripts.\nFor the Switchboard 300h task, transcripts from the training set\nare combined with those of the Fisher corpus to construct the\nWPM. The \ufb01nal transcripts are obtained by a beam search with\nbeam size 8. For comparison with [25], we note that their \u201clarge\nmodel\u201d in our notation is LAS-4-1024.\n3.2. Learning Rate Schedules\nThe learning rate schedule turns out to be an important factor\nin determining the performance of ASR networks, especially\nso when augmentation is present. Here, we introduce training\nschedules that serve two purposes. First, we use these schedules\nto verify that a longer schedule improves the \ufb01nal performance\nof the network, even more so with augmentation (Table 2). Sec-\nond, based on this, we introduce very long schedules that are\nused to maximize the performance of the networks.\nWe use a learning rate schedule in which we ramp-up, hold,\nthen exponentially decay the learning rate until it reaches 1/100\nof its maximum value. The learning rate is kept constant beyond\nthis point. This schedule is parameterized by three time stamps\n(sr, si, sf) \u2013 the step sr where the ramp-up (from zero learning\nrate) is complete, the step si where exponential decay starts, and\nthe step sf where the exponential decay stops.\nThere are two more factors that introduce time scales in our\nexperiment. First, we turn on a variational weight noise [27]\nof standard deviation 0.075 at step snoise and keep it constant\nthroughout training. Weight noise is introduced in the step in-\nterval (sr, si), i.e., during the high plateau of the learning rate.\nSecond, we introduce uniform label smoothing [28] with\nuncertainty 0.1, i.e., the correct class label is assigned the con\ufb01-\ndence 0.9, while the con\ufb01dence of the other labels are increased\naccordingly. As is commented on again later on, label smooth-\ning can destabilize training for smaller learning rates, and we\nsometimes choose to turn it on only at the beginning of training\nand off when the learning rate starts to decay.\nThe two basic schedules we use, are given as the following:\n1. B(asic): (sr, snoise, si, sf) = (0.5k, 10k, 20k, 80k)\n2. D(ouble): (sr, snoise, si, sf) = (1k, 20k, 40k, 160k)\nAs discussed further in section 5, we can improve the perfor-\nmance of the trained network by using a longer schedule. We\nthus introduce the following schedule:\n3. L(ong): (sr, snoise, si, sf) = (1k, 20k, 140k, 320k)\nwhich we use to train the largest model to improve performance.\nWhen using schedule L, label smoothing with uncertainty 0.1 is\nintroduced for time steps < si = 140k for LibriSpeech 960h,\nand is subsequently turned off. For Switchboard 300h, label\nsmoothing is turned on throughout training.\n3.3. Shallow Fusion with Language Models\nWhile we are able to get state-of-the-art results with augmen-\ntation, we can get further improvements by using a language\nmodel. We thus incorporate an RNN language model by shal-\nlow fusion for both tasks. In shallow fusion, the \u201cnext token\u201d\ny\u2217in the decoding process is determined by\ny\u2217= argmax\ny\n(log P(y|x) + \u03bb log PLM(y)) ,\n(1)\ni.e., by jointly scoring the token using the base ASR model and\nthe language model. We also use a coverage penalty c [29].\nFor LibriSpeech, we use a two-layer RNN with embedding\ndimension 1024 used in [25] for the LM, which is trained on\nthe LibriSpeech LM corpus. We use identical fusion parameters\n(\u03bb = 0.35 and c = 0.05) used in [25] throughout.\nFor Switchboard, we use a two-layer RNN with embedding\ndimension 256, which is trained on the combined transcripts of\nthe Fisher and Switchboard datasets. We \ufb01nd the fusion pa-\nrameters via grid search by measuring performance on RT-03\n(LDC2007S10). We discuss the fusion parameters used in indi-\nvidual experiments in section 4.2.\n4. Experiments\nIn this section, we describe our experiments on LibriSpeech and\nSwitchboard with SpecAugment. We report state-of-the-art re-\nsults that out-perform heavily engineered hybrid systems.\n4.1. LibriSpeech 960h\nFor LibriSpeech, we use the same setup as [25], where we use\n80-dimensional \ufb01lter banks with delta and delta-delta accelera-\ntion, and a 16k word piece model [26].\nThe three networks LAS-4-1024, LAS-6-1024 and LAS-6-\n1280 are trained on LibriSpeech 960h with a combination of\naugmentation policies (None, LB, LD) and training schedules\n(B/D). Label smoothing was not applied in these experiments.\nThe experiments were run with peak learning rate of 0.001 and\nbatch size of 512, on 32 Google Cloud TPU chips for 7 days.\nOther than the augmentation policies and learning rate sched-\nules, all other hyperparameters were \ufb01xed, and no additional\ntuning was applied. We report test set numbers validated by the\ndev-other set in Table 2. We see that augmentation consistently\nimproves the performance of the trained network, and that the\nbene\ufb01t of a larger network and a longer learning rate schedule\nis more apparent with harsher augmentation.\nWe take the largest network, LAS-6-1280, and use sched-\nule L (with training time \u223c24 days) and policy LD to train\nthe network to maximize performance. We turn label smooth-\ning on for time steps < 140k as noted before. The test set\nperformance is reported by evaluating the checkpoint with best\ndev-other performance. State of the art performance is achieved\nby the LAS-6-1280 model, even without a language model. We\nTable 2: LibriSpeech test WER (%) evaluated for varying net-\nworks, schedules and policies. First row from [25].\nNetwork\nSch\nPol\nNo LM\nWith LM\nclean\nother\nclean\nother\nLAS-4-1024 [25]\nB\n-\n4.7\n13.4\n3.6\n10.3\nLAS-4-1024\nB\nLB\n3.7\n10.0\n3.4\n8.3\nB\nLD\n3.6\n9.2\n2.8\n7.5\nD\n-\n4.4\n13.3\n3.5\n10.4\nD\nLB\n3.4\n9.2\n2.7\n7.3\nD\nLD\n3.4\n8.3\n2.8\n6.8\nLAS-6-1024\nD\n-\n4.5\n13.1\n3.6\n10.3\nD\nLB\n3.4\n8.6\n2.6\n6.7\nD\nLD\n3.2\n8.0\n2.6\n6.5\nLAS-6-1280\nD\n-\n4.3\n12.9\n3.5\n10.5\nD\nLB\n3.4\n8.7\n2.8\n7.1\nD\nLD\n3.2\n7.7\n2.7\n6.5\ncan incorporate an LM using shallow fusion to further improve\nperformance. The results are presented in Table 3.\nTable 3: LibriSpeech 960h WERs (%).\nMethod\nNo LM\nWith LM\nclean\nother\nclean\nother\nHMM\nPanayotov et al., (2015) [20]\n5.51\n13.97\nPovey et al., (2016) [30]\n4.28\nHan et al., (2017) [31]\n3.51\n8.58\nYang et al. (2018) [32]\n2.97\n7.50\nCTC/ASG\nCollobert et al., (2016) [33]\n7.2\nLiptchinsky et al., (2017) [34]\n6.7\n20.8\n4.8\n14.5\nZhou et al., (2018) [35]\n5.42\n14.70\nZeghidour et al., (2018) [36]\n3.44\n11.24\nLi et al., (2019) [37]\n3.86\n11.95\n2.95\n8.79\nLAS\nZeyer et al., (2018) [24]\n4.87\n15.39\n3.82\n12.76\nZeyer et al., (2018) [38]\n4.70\n15.20\nIrie et al., (2019) [25]\n4.7\n13.4\n3.6\n10.3\nSabour et al., (2019) [39]\n4.5\n13.3\nOur Work\nLAS\n4.1\n12.5\n3.2\n9.8\nLAS + SpecAugment\n2.8\n6.8\n2.5\n5.8\n4.2. Switchboard 300h\nFor Switchboard 300h, we use the Kaldi [40] \u201cs5c\u201d recipe to\nprocess our data, but we adapt the recipe to use 80-dimensional\n\ufb01lter banks with delta and delta-delta acceleration. We use a 1k\nWPM [26] to tokenize the output, constructed using the com-\nbined vocabulary of the Switchboard and Fisher transcripts.\nWe train LAS-4-1024 with policies (None, SM, SS) and\nschedule B. As before, we set the peak learning rate to 0.001\nand total batch size to 512, and train using 32 Google Cloud\nTPU chips. Here the experiments are run with and without la-\nbel smoothing. Not having a canonical development set, we\nchoose to evaluate the checkpoint at the end point of the train-\ning schedule, which we choose to be 100k steps for schedule B.\nWe note that the training curve relaxes after the decay schedule\nis completed (step sf), and the performance of the network does\nnot vary much. The performance of various augmentation poli-\ncies with and without label smoothing for Switchboard 300h is\nshown in Table 4. We see that label smoothing and augmenta-\ntion have an additive effect for this corpus.\nTable 4: Switchboard 300h WER (%) evaluated for LAS-4-1024\ntrained with schedule B with varying augmentation and Label\nSmoothing (LS) policies. No LMs have been used.\nPolicy\nLS\nSWBD\nCH\n-\n\u00d7\n12.1\n22.6\n\u25e6\n11.2\n21.6\nSM\n\u00d7\n9.5\n18.8\n\u25e6\n8.5\n16.1\nSS\n\u00d7\n9.7\n18.2\n\u25e6\n8.6\n16.3\nAs with LibriSpeech 960h, we train LAS-6-1280 on the\nSwitchboard 300h training set with schedule L (training time\n\u223c24 days) to get state of the art performance. In this case, we\n\ufb01nd that turning label smoothing on throughout training ben-\ne\ufb01ts the \ufb01nal performance. We report the performance at the\nend of training time at 340k steps. We present our results in the\ncontext of other work in Table 5. We also apply shallow fusion\nwith an LM trained on Fisher-Switchboard, whose fusion pa-\nrameters are obtained by evaluating performance on the RT-03\ncorpus. Unlike the case for LibriSpeech, the fusion parameters\ndo not transfer well between networks trained differently\u2014the\nthree entries in Table 5 were obtained by using fusion parame-\nters (\u03bb, c) = (0.3, 0.05), (0.2, 0.0125) and (0.1, 0.025) respec-\ntively.\nTable 5: Switchboard 300h WERs (%).\nMethod\nNo LM\nWith LM\nSWBD\nCH\nSWBD\nCH\nHMM\nVesel\u00b4y et al., (2013) [41]\n12.9\n24.5\nPovey et al., (2016) [30]\n9.6\n19.3\nHadian et al., (2018) [42]\n9.3\n18.9\nZeyer et al., (2018) [24]\n8.3\n17.3\nCTC\nZweig et al., (2017) [43]\n24.7\n37.1\n14.0\n25.3\nAudhkhasi et al., (2018) [44]\n20.8\n30.4\nAudhkhasi et al., (2018) [45]\n14.6\n23.6\nLAS\nLu et al., (2016) [46]\n26.8\n48.2\n25.8\n46.0\nToshniwal et al., (2017) [47]\n23.1\n40.8\nZeyer et al., (2018) [24]\n13.1\n26.1\n11.8\n25.7\nWeng et al., (2018) [48]\n12.2\n23.3\nZeyer et al., (2018) [38]\n11.9\n23.7\n11.0\n23.1\nOur Work\nLAS\n11.2\n21.6\n10.9\n19.4\nLAS + SpecAugment (SM)\n7.2\n14.6\n6.8\n14.1\nLAS + SpecAugment (SS)\n7.3\n14.4\n7.1\n14.0\n5. Discussion\nTime warping contributes, but is not a major factor in im-\nproving performance. In Table 6, we present three training\nresults for which time warping, time masking and frequency\nmasking have been turned off, respectively. We see that the ef-\nfect time warping, while small, is still existent. Time warping,\nbeing the most expensive as well as the least in\ufb02uential of the\naugmentations discussed in this work, should be the \ufb01rst aug-\nmentation to be dropped given any budgetary limitations.\nTable 6: Test set WER (%) evaluated without LM for network\nLAS-4-1024 trained with schedule B.\nW\nF\nmF\nT\np\nmT\ntest-other\ntest\n80\n27\n1\n100\n1.0\n1\n10.0\n3.7\n0\n27\n1\n100\n1.0\n1\n10.1\n3.8\n80\n0\n-\n100\n1.0\n1\n11.0\n4.0\n80\n27\n1\n0\n-\n-\n10.9\n4.1\nLabel smoothing introduces instability to training. We have\nnoticed that the proportion of unstable training runs increases\nfor LibriSpeech when label smoothing is applied with aug-\nmentation.\nThis becomes more conspicuous while learning\nrate is being decayed, thus our introduction of a label smooth-\ning schedule for training LibriSpeech, where labels are only\nsmoothed in the initial phases of the learning rate schedule.\nAugmentation converts an over-\ufb01tting problem into an\nunder-\ufb01tting problem. As can be observed from the training\ncurves of the networks in Figure 3, the networks during training\nnot only under-\ufb01t the loss and WER on the augmented training\nset, but also on the training set itself when trained on augmented\ndata. This is in stark contrast to the usual situation where net-\nworks tend to over-\ufb01t to the training data. This is the major\nbene\ufb01t of training with augmentation, as explained below.\nFigure 3: LAS-6-1280 on LibriSpeech with schedule D.\nCommon methods of addressing under-\ufb01tting yield im-\nprovements. We were able to make signi\ufb01cant gains in per-\nformance by standard approaches to alleviate under-\ufb01tting\u2014\nmaking larger networks and training longer. The current re-\nported performance was obtained by the recursive process of\napplying a harsh augmentation policy, and then making wider,\ndeeper networks and training them with longer schedules to ad-\ndress the under-\ufb01tting.\nRemark on related works. We note that an augmentation sim-\nilar to frequency masking has been studied in the context of\nCNN acoustic models in [49]. There, blocks of adjacent fre-\nquencies are pre-grouped into bins, which are randomly zeroed-\nout per minibatch. On the other hand, both the size and position\nof the frequency masks in SpecAugment are chosen stochasti-\ncally, and differ for every input in the minibatch. More ideas for\nstructurally omitting frequency data of spectrograms have been\ndiscussed in [50].\n6. Conclusions\nSpecAugment greatly improves the performance of ASR net-\nworks. We are able to obtain state-of-the-art results on the Lib-\nriSpeech 960h and Switchboard 300h tasks on end-to-end LAS\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "SpecAugment: A Simple Data Augmentation Method\nfor Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art results even with-\nout the use of Language Models (LMs).\nOn LibriSpeech\n[20], we achieve 2.8% Word Error Rate (WER) on the test-\nclean set and 6.8% WER on the test-other set, without the\nuse of an LM. Upon shallow fusion [21] with an LM trained\non the LibriSpeech LM corpus, we are able to better our per-\nformance (2.5% WER on test-clean and 5.8% WER on test-\nother), improving the current state of the art on test-other by\n22% relatively. On Switchboard 300h (LDC97S62) [22], we\nobtain 7.2% WER on the Switchboard portion of the Hub5\u201900\n(LDC2002S09, LDC2003T02) test set, and 14.6% on the Call-\nHome portion, without using an LM. Upon shallow fusion\nwith an LM trained on the combined transcript of the Switch-\nboard and Fisher (LDC200{4,5}T19) [23] corpora, we obtain\n6.8%/14.1% WER on the Switchboard/Callhome portion.\n2. Augmentation Policy\nWe aim to construct an augmentation policy that acts on the log\nmel spectrogram directly, which helps the network learn use-\nful features. Motivated by the goal that these features should\nbe robust to deformations in the time direction, partial loss of\nfrequency information and partial loss of small segments of\nspeech, we have chosen the following deformations to make up\na policy:\n1. Time\nwarping\nis\napplied\nvia\nthe\nfunction\nsparse image warp of tensorflow.\nGiven\na log mel spectrogram with \u03c4 time steps, we view it\nas an image where the time axis is horizontal and the\nfrequency axis is vertical.\nA random point along the\nhorizontal line passing through the center of the image\nwithin the time steps (W, \u03c4 \u2212W) is to be warped either\nto the left or right by a distance w chosen from a uniform\ndistribution from 0 to the time warp parameter W along\nthat line. We \ufb01x six anchor points on the boundary\u2014the\nfour corners and the mid-points of the vertical edges.\n2. Frequency masking is applied so that f consecutive mel\nfrequency channels [f0, f0 + f) are masked, where f is\n\ufb01rst chosen from a uniform distribution from 0 to the\narXiv:1904.08779v3  [eess.AS]  3 Dec 2019\nFigure 1: Augmentations applied to the base input, given at the\ntop. From top to bottom, the \ufb01gures depict the log mel spectro-\ngram of the base input with no augmentation, time warp, fre-\nquency masking and time masking applied.\nFigure 2: Augmentation policies applied to the base input. From\ntop to bottom, the \ufb01gures depict the log mel spectrogram of the\nbase input with policies None, LB and LD applied.\nfrequency mask parameter F, and f0 is chosen from\n[0, \u03bd \u2212f). \u03bd is the number of mel frequency channels.\n3. Time masking is applied so that t consecutive time steps\n[t0, t0 + t) are masked, where t is \ufb01rst chosen from a\nuniform distribution from 0 to the time mask parameter\nT, and t0 is chosen from [0, \u03c4 \u2212t).\n\u2022 We introduce an upper bound on the time mask so\nthat a time mask cannot be wider than p times the\nnumber of time steps.\nFigure 1 shows examples of the individual augmentations ap-\nplied to a single input. The log mel spectrograms are normal-\nized to have zero mean value, and thus setting the masked value\nto zero is equivalent to setting it to the mean value.\nWe can consider policies where multiple frequency and\ntime masks are applied. The multiple masks may overlap. In\nthis work, we mainly consider a series of hand-crafted policies,\nLibriSpeech basic (LB), LibriSpeech double (LD), Switchboard\nmild (SM) and Switchboard strong (SS) whose parameters are\nsummarized in Table 1. In Figure 2, we show an example of a\nlog mel spectrogram augmented with policies LB and LD.\nTable 1: Augmentation parameters for policies. mF and mT\ndenote the number of frequency and time masks applied.\nPolicy\nW\nF\nmF\nT\np\nmT\nNone\n0\n0\n-\n0\n-\n-\nLB\n80\n27\n1\n100\n1.0\n1\nLD\n80\n27\n2\n100\n1.0\n2\nSM\n40\n15\n2\n70\n0.2\n2\nSS\n40\n27\n2\n70\n0.2\n2\n3. Model\nWe use Listen, Attend and Spell (LAS) networks [6] for our\nASR tasks. These models, being end-to-end, are simple to train\nand have the added bene\ufb01t of having well-documented bench-\nmarks [24, 25] that we are able to build upon to get our results.\nIn this section, we review LAS networks and introduce some\nnotation to parameterize them. We also introduce the learning\nrate schedules we use to train the networks, as they turn out\nto be an important factor in determining performance. We end\nwith reviewing shallow fusion [21], which we have used to in-\ncorporate language models for further gains in performance.\n3.1. LAS Network Architectures\nWe use Listen, Attend and Spell (LAS) networks [6] for end-to-\nend ASR studied in [25], for which we use the notation LAS-\nd-w. The input log mel spectrogram is passed in to a 2-layer\nConvolutional Neural Network (CNN) with max-pooling and\nstride of 2. The output of the CNN is passes through an en-\ncoder consisting of d stacked bi-directional LSTMs with cell\nsize w to yield a series of attention vectors. The attention vec-\ntors are fed into a 2-layer RNN decoder of cell dimension w,\nwhich yields the tokens for the transcript. The text is tokenized\nusing a Word Piece Model (WPM) [26] of vocabulary size 16k\nfor LibriSpeech and 1k for Switchboard. The WPM for Lib-\nriSpeech 960h is constructed using the training set transcripts.\nFor the Switchboard 300h task, transcripts from the training set\nare combined with those of the Fisher corpus to construct the\nWPM. The \ufb01nal transcripts are obtained by a beam search with\nbeam size 8. For comparison with [25], we note that their \u201clarge\nmodel\u201d in our notation is LAS-4-1024.\n3.2. Learning Rate Schedules\nThe learning rate schedule turns out to be an important factor\nin determining the performance of ASR networks, especially\nso when augmentation is present. Here, we introduce training\nschedules that serve two purposes. First, we use these schedules\nto verify that a longer schedule improves the \ufb01nal performance\nof the network, even more so with augmentation (Table 2). Sec-\nond, based on this, we introduce very long schedules that are\nused to maximize the performance of the networks.\nWe use a learning rate schedule in which we ramp-up, hold,\nthen exponentially decay the learning rate until it reaches 1/100\nof its maximum value. The learning rate is kept constant beyond\nthis point. This schedule is parameterized by three time stamps\n(sr, si, sf) \u2013 the step sr where the ramp-up (from zero learning\nrate) is complete, the step si where exponential decay starts, and\nthe step sf where the exponential decay stops.\nThere are two more factors that introduce time scales in our\nexperiment. First, we turn on a variational weight noise [27]\nof standard deviation 0.075 at step snoise and keep it constant\nthroughout training. Weight noise is introduced in the step in-\nterval (sr, si), i.e., during the high plateau of the learning rate.\nSecond, we introduce uniform label smoothing [28] with\nuncertainty 0.1, i.e., the correct class label is assigned the con\ufb01-\ndence 0.9, while the con\ufb01dence of the other labels are increased\naccordingly. As is commented on again later on, label smooth-\ning can destabilize training for smaller learning rates, and we\nsometimes choose to turn it on only at the beginning of training\nand off when the learning rate starts to decay.\nThe two basic schedules we use, are given as the following:\n1. B(asic): (sr, snoise, si, sf) = (0.5k, 10k, 20k, 80k)\n2. D(ouble): (sr, snoise, si, sf) = (1k, 20k, 40k, 160k)\nAs discussed further in section 5, we can improve the perfor-\nmance of the trained network by using a longer schedule. We\nthus introduce the following schedule:\n3. L(ong): (sr, snoise, si, sf) = (1k, 20k, 140k, 320k)\nwhich we use to train the largest model to improve performance.\nWhen using schedule L, label smoothing with uncertainty 0.1 is\nintroduced for time steps < si = 140k for LibriSpeech 960h,\nand is subsequently turned off. For Switchboard 300h, label\nsmoothing is turned on throughout training.\n3.3. Shallow Fusion with Language Models\nWhile we are able to get state-of-the-art results with augmen-\ntation, we can get further improvements by using a language\nmodel. We thus incorporate an RNN language model by shal-\nlow fusion for both tasks. In shallow fusion, the \u201cnext token\u201d\ny\u2217in the decoding process is determined by\ny\u2217= argmax\ny\n(log P(y|x) + \u03bb log PLM(y)) ,\n(1)\ni.e., by jointly scoring the token using the base ASR model and\nthe language model. We also use a coverage penalty c [29].\nFor LibriSpeech, we use a two-layer RNN with embedding\ndimension 1024 used in [25] for the LM, which is trained on\nthe LibriSpeech LM corpus. We use identical fusion parameters\n(\u03bb = 0.35 and c = 0.05) used in [25] throughout.\nFor Switchboard, we use a two-layer RNN with embedding\ndimension 256, which is trained on the combined transcripts of\nthe Fisher and Switchboard datasets. We \ufb01nd the fusion pa-\nrameters via grid search by measuring performance on RT-03\n(LDC2007S10). We discuss the fusion parameters used in indi-\nvidual experiments in section 4.2.\n4. Experiments\nIn this section, we describe our experiments on LibriSpeech and\nSwitchboard with SpecAugment. We report state-of-the-art re-\nsults that out-perform heavily engineered hybrid systems.\n4.1. LibriSpeech 960h\nFor LibriSpeech, we use the same setup as [25], where we use\n80-dimensional \ufb01lter banks with delta and delta-delta accelera-\ntion, and a 16k word piece model [26].\nThe three networks LAS-4-1024, LAS-6-1024 and LAS-6-\n1280 are trained on LibriSpeech 960h with a combination of\naugmentation policies (None, LB, LD) and training schedules\n(B/D). Label smoothing was not applied in these experiments.\nThe experiments were run with peak learning rate of 0.001 and\nbatch size of 512, on 32 Google Cloud TPU chips for 7 days.\nOther than the augmentation policies and learning rate sched-\nules, all other hyperparameters were \ufb01xed, and no additional\ntuning was applied. We report test set numbers validated by the\ndev-other set in Table 2. We see that augmentation consistently\nimproves the performance of the trained network, and that the\nbene\ufb01t of a larger network and a longer learning rate schedule\nis more apparent with harsher augmentation.\nWe take the largest network, LAS-6-1280, and use sched-\nule L (with training time \u223c24 days) and policy LD to train\nthe network to maximize performance. We turn label smooth-\ning on for time steps < 140k as noted before. The test set\nperformance is reported by evaluating the checkpoint with best\ndev-other performance. State of the art performance is achieved\nby the LAS-6-1280 model, even without a language model. We\nTable 2: LibriSpeech test WER (%) evaluated for varying net-\nworks, schedules and policies. First row from [25].\nNetwork\nSch\nPol\nNo LM\nWith LM\nclean\nother\nclean\nother\nLAS-4-1024 [25]\nB\n-\n4.7\n13.4\n3.6\n10.3\nLAS-4-1024\nB\nLB\n3.7\n10.0\n3.4\n8.3\nB\nLD\n3.6\n9.2\n2.8\n7.5\nD\n-\n4.4\n13.3\n3.5\n10.4\nD\nLB\n3.4\n9.2\n2.7\n7.3\nD\nLD\n3.4\n8.3\n2.8\n6.8\nLAS-6-1024\nD\n-\n4.5\n13.1\n3.6\n10.3\nD\nLB\n3.4\n8.6\n2.6\n6.7\nD\nLD\n3.2\n8.0\n2.6\n6.5\nLAS-6-1280\nD\n-\n4.3\n12.9\n3.5\n10.5\nD\nLB\n3.4\n8.7\n2.8\n7.1\nD\nLD\n3.2\n7.7\n2.7\n6.5\ncan incorporate an LM using shallow fusion to further improve\nperformance. The results are presented in Table 3.\nTable 3: LibriSpeech 960h WERs (%).\nMethod\nNo LM\nWith LM\nclean\nother\nclean\nother\nHMM\nPanayotov et al., (2015) [20]\n5.51\n13.97\nPovey et al., (2016) [30]\n4.28\nHan et al., (2017) [31]\n3.51\n8.58\nYang et al. (2018) [32]\n2.97\n7.50\nCTC/ASG\nCollobert et al., (2016) [33]\n7.2\nLiptchinsky et al., (2017) [34]\n6.7\n20.8\n4.8\n14.5\nZhou et al., (2018) [35]\n5.42\n14.70\nZeghidour et al., (2018) [36]\n3.44\n11.24\nLi et al., (2019) [37]\n3.86\n11.95\n2.95\n8.79\nLAS\nZeyer et al., (2018) [24]\n4.87\n15.39\n3.82\n12.76\nZeyer et al., (2018) [38]\n4.70\n15.20\nIrie et al., (2019) [25]\n4.7\n13.4\n3.6\n10.3\nSabour et al., (2019) [39]\n4.5\n13.3\nOur Work\nLAS\n4.1\n12.5\n3.2\n9.8\nLAS + SpecAugment\n2.8\n6.8\n2.5\n5.8\n4.2. Switchboard 300h\nFor Switchboard 300h, we use the Kaldi [40] \u201cs5c\u201d recipe to\nprocess our data, but we adapt the recipe to use 80-dimensional\n\ufb01lter banks with delta and delta-delta acceleration. We use a 1k\nWPM [26] to tokenize the output, constructed using the com-\nbined vocabulary of the Switchboard and Fisher transcripts.\nWe train LAS-4-1024 with policies (None, SM, SS) and\nschedule B. As before, we set the peak learning rate to 0.001\nand total batch size to 512, and train using 32 Google Cloud\nTPU chips. Here the experiments are run with and without la-\nbel smoothing. Not having a canonical development set, we\nchoose to evaluate the checkpoint at the end point of the train-\ning schedule, which we choose to be 100k steps for schedule B.\nWe note that the training curve relaxes after the decay schedule\nis completed (step sf), and the performance of the network does\nnot vary much. The performance of various augmentation poli-\ncies with and without label smoothing for Switchboard 300h is\nshown in Table 4. We see that label smoothing and augmenta-\ntion have an additive effect for this corpus.\nTable 4: Switchboard 300h WER (%) evaluated for LAS-4-1024\ntrained with schedule B with varying augmentation and Label\nSmoothing (LS) policies. No LMs have been used.\nPolicy\nLS\nSWBD\nCH\n-\n\u00d7\n12.1\n22.6\n\u25e6\n11.2\n21.6\nSM\n\u00d7\n9.5\n18.8\n\u25e6\n8.5\n16.1\nSS\n\u00d7\n9.7\n18.2\n\u25e6\n8.6\n16.3\nAs with LibriSpeech 960h, we train LAS-6-1280 on the\nSwitchboard 300h training set with schedule L (training time\n\u223c24 days) to get state of the art performance. In this case, we\n\ufb01nd that turning label smoothing on throughout training ben-\ne\ufb01ts the \ufb01nal performance. We report the performance at the\nend of training time at 340k steps. We present our results in the\ncontext of other work in Table 5. We also apply shallow fusion\nwith an LM trained on Fisher-Switchboard, whose fusion pa-\nrameters are obtained by evaluating performance on the RT-03\ncorpus. Unlike the case for LibriSpeech, the fusion parameters\ndo not transfer well between networks trained differently\u2014the\nthree entries in Table 5 were obtained by using fusion parame-\nters (\u03bb, c) = (0.3, 0.05), (0.2, 0.0125) and (0.1, 0.025) respec-\ntively.\nTable 5: Switchboard 300h WERs (%).\nMethod\nNo LM\nWith LM\nSWBD\nCH\nSWBD\nCH\nHMM\nVesel\u00b4y et al., (2013) [41]\n12.9\n24.5\nPovey et al., (2016) [30]\n9.6\n19.3\nHadian et al., (2018) [42]\n9.3\n18.9\nZeyer et al., (2018) [24]\n8.3\n17.3\nCTC\nZweig et al., (2017) [43]\n24.7\n37.1\n14.0\n25.3\nAudhkhasi et al., (2018) [44]\n20.8\n30.4\nAudhkhasi et al., (2018) [45]\n14.6\n23.6\nLAS\nLu et al., (2016) [46]\n26.8\n48.2\n25.8\n46.0\nToshniwal et al., (2017) [47]\n23.1\n40.8\nZeyer et al., (2018) [24]\n13.1\n26.1\n11.8\n25.7\nWeng et al., (2018) [48]\n12.2\n23.3\nZeyer et al., (2018) [38]\n11.9\n23.7\n11.0\n23.1\nOur Work\nLAS\n11.2\n21.6\n10.9\n19.4\nLAS + SpecAugment (SM)\n7.2\n14.6\n6.8\n14.1\nLAS + SpecAugment (SS)\n7.3\n14.4\n7.1\n14.0\n5. Discussion\nTime warping contributes, but is not a major factor in im-\nproving performance. In Table 6, we present three training\nresults for which time warping, time masking and frequency\nmasking have been turned off, respectively. We see that the ef-\nfect time warping, while small, is still existent. Time warping,\nbeing the most expensive as well as the least in\ufb02uential of the\naugmentations discussed in this work, should be the \ufb01rst aug-\nmentation to be dropped given any budgetary limitations.\nTable 6: Test set WER (%) evaluated without LM for network\nLAS-4-1024 trained with schedule B.\nW\nF\nmF\nT\np\nmT\ntest-other\ntest\n80\n27\n1\n100\n1.0\n1\n10.0\n3.7\n0\n27\n1\n100\n1.0\n1\n10.1\n3.8\n80\n0\n-\n100\n1.0\n1\n11.0\n4.0\n80\n27\n1\n0\n-\n-\n10.9\n4.1\nLabel smoothing introduces instability to training. We have\nnoticed that the proportion of unstable training runs increases\nfor LibriSpeech when label smoothing is applied with aug-\nmentation.\nThis becomes more conspicuous while learning\nrate is being decayed, thus our introduction of a label smooth-\ning schedule for training LibriSpeech, where labels are only\nsmoothed in the initial phases of the learning rate schedule.\nAugmentation converts an over-\ufb01tting problem into an\nunder-\ufb01tting problem. As can be observed from the training\ncurves of the networks in Figure 3, the networks during training\nnot only under-\ufb01t the loss and WER on the augmented training\nset, but also on the training set itself when trained on augmented\ndata. This is in stark contrast to the usual situation where net-\nworks tend to over-\ufb01t to the training data. This is the major\nbene\ufb01t of training with augmentation, as explained below.\nFigure 3: LAS-6-1280 on LibriSpeech with schedule D.\nCommon methods of addressing under-\ufb01tting yield im-\nprovements. We were able to make signi\ufb01cant gains in per-\nformance by standard approaches to alleviate under-\ufb01tting\u2014\nmaking larger networks and training longer. The current re-\nported performance was obtained by the recursive process of\napplying a harsh augmentation policy, and then making wider,\ndeeper networks and training them with longer schedules to ad-\ndress the under-\ufb01tting.\nRemark on related works. We note that an augmentation sim-\nilar to frequency masking has been studied in the context of\nCNN acoustic models in [49]. There, blocks of adjacent fre-\nquencies are pre-grouped into bins, which are randomly zeroed-\nout per minibatch. On the other hand, both the size and position\nof the frequency masks in SpecAugment are chosen stochasti-\ncally, and differ for every input in the minibatch. More ideas for\nstructurally omitting frequency data of spectrograms have been\ndiscussed in [50].\n6. Conclusions\nSpecAugment greatly improves the performance of ASR net-\nworks. We are able to obtain state-of-the-art results on the Lib-\nriSpeech 960h and Switchboard 300h tasks on end-to-end LAS\nnetworks by augmenting the training set using simple hand-\ncrafted policies, surpassing the performance of hybrid systems\neven without the aid of a language model. SpecAugment con-\nverts ASR from an over-\ufb01tting to an under-\ufb01tting problem, and\nwe were able to gain performance by using bigger networks and\ntraining longer.\nAcknowledgements:\nWe would like to thank Yuan Cao,\nCiprian Chelba, Kazuki Irie, Ye Jia, Anjuli Kannan, Patrick\nNguyen, Vijay Peddinti, Rohit Prabhavalkar, Yonghui Wu and\nShuyuan Zhang for useful discussions. We also thank Gy\u00a8orgy\nKov\u00b4acs for introducing us to the works [49, 50].\n7. References\n[1] G. Hinton, L. Deng, D. Yu, G. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V. Vanhoucke, P. Nguyen, B. Kingsbury et al., \u201cDeep\nneural networks for acoustic modeling in speech recognition,\u201d\nIEEE Signal processing magazine, vol. 29, 2012.\n[2] G. Dahl, D. Yu, L. Deng, and A. Acero, \u201cContext-Dependent\nPre-Trained Deep Neural Networks for Large-Vocabulary Speech\nRecognition,\u201d IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, vol. 20, Jan 2012.\n[3] T. Sainath, A. rahman Mohamed, B. Kingsbury, and B. Ramab-\nhadran, \u201cDeep Convolutional Neural Networks for LVCSR,\u201d in\nICASSP, 2013.\n[4] A. Graves, A. rahman Mohamed, and G. Hinton, \u201cSpeech Recog-\nnition with Deep Recurrent Neural Networks,\u201d in ICASSP, 2013.\n[5] A. Graves and N. Jaitly, \u201cTowards End-to-End Speech Recogni-\ntion with Recurrent Neural Networks,\u201d in ICML, 2014.\n[6] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, \u201cListen, Attend and\nSpell: A Neural Network for Large Vocabulary Conversational\nSpeech Recognition,\u201d in ICASSP, 2016.\n[7] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,\n\u201cEnd-to-End Attention-based Large Vocabulary Speech Recogni-\ntion,\u201d in ICASSP, 2016.\n[8] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, N. Jaitly,\nB. Li, J. Chorowski, and M. Bacchiani, \u201cState-of-the-art Speech\nRecognition With Sequence-to-Sequence Models,\u201d in ICASSP,\n2018.\n[9] N. Kanda, R. Takeda, and Y. Obuchi, \u201cElastic spectral distortion\nfor low resource speech recognition with deep neural networks,\u201d\nin ASRU, 2013.\n[10] A. Ragni, K. M. Knill, S. P. Rath, and M. J. F. Gales, \u201cData aug-\nmentation for low resource languages,\u201d in INTERSPEECH, 2014.\n[11] N. Jaitly and G. Hinton, \u201cVocal Tract Length Perturbation (VTLP)\nimproves speech recognition,\u201d in ICML Workshop on Deep Learn-\ning for Audio, Speech and Language Processing, 2013.\n[12] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos,\nE. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and\nA. Ng, \u201cDeep Speech: Scaling up end-to-end speech recognition,\u201d\nin arXiv, 2014.\n[13] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, \u201cAudio Augmen-\ntation for Speech Recognition,\u201d in INTERSPEECH, 2015.\n[14] C. Kim, A. Misra, K. Chin, T. Hughes, A. Narayanan, T. Sainath,\nand M. Bacchiani, \u201cGeneration of large-scale simulated utterances\nin virtual rooms to train deep-neural networks for far-\ufb01eld speech\nrecognition in Google Home,\u201d in INTERSPEECH, 2017.\n[15] R. Prabhavalkar, R. Alvarez, C. Parada, P. Nakkiran, and T. N.\nSainath, \u201cAutomatic gain control and multi-style training for ro-\nbust small-footprint keyword spotting with deep neural networks,\u201d\nin ICASSP, 2015.\n[16] A. Raju, S. Panchapagesan, X. Liu, A. Mandal, and N. Strom,\n\u201cData Augmentation for Robust Keyword Spotting under Play-\nback Interference,\u201d in arXiv, 2018.\n[17] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based\nfusion for Multistream ASR,\u201d in ICASSP, 2016.\n[18] E. D. Cubuk, B. Zoph, D. Man\u00b4e, V. Vasudevan, and Q. V. Le, \u201cAu-\ntoaugment: Learning augmentation policies from data,\u201d in CVPR,\n2019.\n[19] T. DeVries and G. Taylor, \u201cImproved Regularization of Convolu-\ntional Neural Networks with Cutout,\u201d in arXiv, 2017.\n[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech: An ASR corpus based on public domain audio books,\u201d\nin ICASSP, 2015.\n[21] C\u00b8 . G\u00a8ulc\u00b8ehre, O. Firat, K. Xu, K. Cho, L. Barrault, H. Lin,\nF. Bougares, H. Schwenk, and Y. Bengio, \u201cOn using monolingual\ncorpora in neural machine translation,\u201d in arxiv, 2015.\n[22] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSWITCHBOARD:\ntelephone speech corpus for research and development,\u201d in\nICASSP, 1992.\n[23] C. Cieri, D. Miller, and K. Walker, \u201cThe \ufb01sher corpus: a resource\nfor the next generations of speech-to-text,\u201d in LREC, 2004.\n[24] A. Zeyer, K. Irie, R. Schl\u00a8uter, and H. Ney, \u201cImproved training of\nend-to-end attention models for speech recognition,\u201d in INTER-\nSPEECH, 2018.\n[25] K. Irie, R. Prabhavalkar, A. Kannan, A. Bruguier, D. Rybach, and\nP. Nguyen, \u201cModel Unit Exploration for Sequence-to-Sequence\nSpeech Recognition,\u201d in arXiv, 2019.\n[26] M. Schuster and K. Nakajima, \u201cJapanese and korean voice\nsearch,\u201d in ICASSP, 2012.\n[27] A. Graves, \u201cPractical Variational Inference for Neural Networks,\u201d\nin NIPS, 2011.\n[28] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRe-\nthinking the inception architecture for computer vision,\u201d in CVPR,\n2016.\n[29] J. Chorowski and N. Jaitly, \u201cTowards better decoding and lan-\nguage model integration in sequence to sequence models,\u201d in IN-\nTERSPEECH, 2017.\n[30] D. Povey, V. Peddinti, D. Galvez, P. Ghahrmani, V. Manohar,\nX. Na, Y. Wang, and S. Khudanpur, \u201cPurely sequence-trained\nneural networks for ASR based on lattice-free MMI,\u201d in INTER-\nSPEECH, 2016.\n[31] K. J. Han, A. Chandrashekaran, J. Kim, and I. Lane, \u201cThe CA-\nPIO 2017 Conversational Speech Recognition System,\u201d in arXiv,\n2017.\n[32] X. Yang, J. Li, and X. Zhou, \u201cA novel pyramidal-FSMN archi-\ntecture with lattice-free MMI for speech recognition,\u201d in arXiv,\n2018.\n[33] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2Letter: an End-\nto-End ConvNet-based Speech Recognition System,\u201d in arXiv,\n2016.\n[34] V. Liptchinsky, G. Synnaeve, and R. Collobert, \u201cLetter-Based\nSpeech Recognition with Gated ConvNets,\u201d in arXiv, 2017.\n[35] Y. Zhou, C. Xiong, and R. Socher, \u201cImproving End-to-End\nSpeech Recognition with Policy Learning,\u201d in ICASSP, 2018.\n[36] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve,\nand R. Collobert, \u201cFully Convolutional Speech Recognition,\u201d in\narXiv, 2018.\n[37] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M.\nCohen, H. Nguyen, and R. T. Gadde, \u201cJasper: An End-to-End\nConvolutional Neural Acoustic Model,\u201d in arXiv, 2019.\n[38] A. Zeyer, A. Merboldt, R. Schl\u00a8uter, and H. Ney, \u201cA comprehen-\nsive analysis on attention models,\u201d in NIPS: Workshop IRASL,\n2018.\n[39] S. Sabour, W. Chan, and M. Norouzi, \u201cOptimal Completion Dis-\ntillation for Sequence Learning,\u201d in ICLR, 2019.\n[40] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz,\nJ. Silovsky, G. Stemmer, and K. Vesely, \u201cThe Kaldi Speech\nRecognition Toolkit,\u201d in ASRU, 2011.\n"
    },
    {
        "pdf_file": "paper2.pdf",
        "text": "SpecAugment: A Simple Data Augmentation Method\nfor Automatic Speech Recognition\nDaniel S. Park\u2217, William Chan, Yu Zhang, Chung-Cheng Chiu,\nBarret Zoph, Ekin D. Cubuk, Quoc V. Le\nGoogle Brain\n{danielspark, williamchan, ngyuzh, chungchengc, barretzoph, cubuk, qvl}@google.com\nAbstract\nWe present SpecAugment, a simple data augmentation method\nfor speech recognition.\nSpecAugment is applied directly to\nthe feature inputs of a neural network (i.e., \ufb01lter bank coef-\n\ufb01cients).\nThe augmentation policy consists of warping the\nfeatures, masking blocks of frequency channels, and masking\nblocks of time steps. We apply SpecAugment on Listen, Attend\nand Spell networks for end-to-end speech recognition tasks. We\nachieve state-of-the-art performance on the LibriSpeech 960h\nand Swichboard 300h tasks, outperforming all prior work. On\nLibriSpeech, we achieve 6.8% WER on test-other without the\nuse of a language model, and 5.8% WER with shallow fusion\nwith a language model. This compares to the previous state-\nof-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of\nthe Hub5\u201900 test set without the use of a language model, and\n6.8%/14.1% with shallow fusion, which compares to the previ-\nous state-of-the-art hybrid system at 8.3%/17.3% WER.\nIndex Terms: end-to-end speech recognition, data augmenta-\ntion\n1. Introduction\nDeep Learning has been applied successfully to Automatic\nSpeech Recognition (ASR) [1], where the main focus of re-\nsearch has been designing better network architectures, for ex-\nample, DNNs [2], CNNs [3], RNNs [4] and end-to-end models\n[5, 6, 7]. However, these models tend to over\ufb01t easily and re-\nquire large amounts of training data [8].\nData augmentation has been proposed as a method to gen-\nerate additional training data for ASR. For example, in [9, 10],\narti\ufb01cial data was augmented for low resource speech recogni-\ntion tasks. Vocal Tract Length Normalization has been adapted\nfor data augmentation in [11]. Noisy audio has been synthe-\nsised via superimposing clean audio with a noisy audio signal\nin [12]. Speed perturbation has been applied on raw audio for\nLVSCR tasks in [13]. The use of an acoustic room simulator has\nbeen explored in [14]. Data augmentation for keyword spotting\nhas been studied in [15, 16]. Feature drop-outs have been em-\nployed for training multi-stream ASR systems [17]. More gen-\nerally, learned augmentation techniques have explored different\nsequences of augmentation transformations that have achieved\nstate-of-the-art performance in the image domain [18].\nInspired by the recent success of augmentation in the\nspeech and vision domains, we propose SpecAugment, an aug-\nmentation method that operates on the log mel spectrogram of\nthe input audio, rather than the raw audio itself. This method\nis simple and computationally cheap to apply, as it directly acts\non the log mel spectrogram as if it were an image, and does not\n\u2217Work done as a member of the Google AI Residency Program.\nrequire any additional data. We are thus able to apply SpecAug-\nment online during training. SpecAugment consists of three\nkinds of deformations of the log mel spectrogram. The \ufb01rst\nis time warping, a deformation of the time-series in the time\ndirection. The other two augmentations, inspired by \u201cCutout\u201d,\nproposed in computer vision [19], are time and frequency mask-\ning, where we mask a block of consecutive time steps or mel\nfrequency channels.\nThis approach while rudimentary, is remarkably effective\nand allows us to train end-to-end ASR networks, called Lis-\nten Attend and Spell (LAS) [6], to surpass more complicated\nhybrid systems, and achieve state-of-the-art results even with-\nout the use of Language Models (LMs).\nOn LibriSpeech\n[20], we achieve 2.8% Word Error Rate (WER) on the test-\nclean set and 6.8% WER on the test-other set, without the\nuse of an LM. Upon shallow fusion [21] with an LM trained\non the LibriSpeech LM corpus, we are able to better our per-\nformance (2.5% WER on test-clean and 5.8% WER on test-\nother), improving the current state of the art on test-other by\n22% relatively. On Switchboard 300h (LDC97S62) [22], we\nobtain 7.2% WER on the Switchboard portion of the Hub5\u201900\n(LDC2002S09, LDC2003T02) test set, and 14.6% on the Call-\nHome portion, without using an LM. Upon shallow fusion\nwith an LM trained on the combined transcript of the Switch-\nboard and Fisher (LDC200{4,5}T19) [23] corpora, we obtain\n6.8%/14.1% WER on the Switchboard/Callhome portion.\n2. Augmentation Policy\nWe aim to construct an augmentation policy that acts on the log\nmel spectrogram directly, which helps the network learn use-\nful features. Motivated by the goal that these features should\nbe robust to deformations in the time direction, partial loss of\nfrequency information and partial loss of small segments of\nspeech, we have chosen the following deformations to make up\na policy:\n1. Time\nwarping\nis\napplied\nvia\nthe\nfunction\nsparse image warp of tensorflow.\nGiven\na log mel spectrogram with \u03c4 time steps, we view it\nas an image where the time axis is horizontal and the\nfrequency axis is vertical.\nA random point along the\nhorizontal line passing through the center of the image\nwithin the time steps (W, \u03c4 \u2212W) is to be warped either\nto the left or right by a distance w chosen from a uniform\ndistribution from 0 to the time warp parameter W along\nthat line. We \ufb01x six anchor points on the boundary\u2014the\nfour corners and the mid-points of the vertical edges.\n2. Frequency masking is applied so that f consecutive mel\nfrequency channels [f0, f0 + f) are masked, where f is\n\ufb01rst chosen from a uniform distribution from 0 to the\narXiv:1904.08779v3  [eess.AS]  3 Dec 2019\nFigure 1: Augmentations applied to the base input, given at the\ntop. From top to bottom, the \ufb01gures depict the log mel spectro-\ngram of the base input with no augmentation, time warp, fre-\nquency masking and time masking applied.\nFigure 2: Augmentation policies applied to the base input. From\ntop to bottom, the \ufb01gures depict the log mel spectrogram of the\nbase input with policies None, LB and LD applied.\nfrequency mask parameter F, and f0 is chosen from\n[0, \u03bd \u2212f). \u03bd is the number of mel frequency channels.\n3. Time masking is applied so that t consecutive time steps\n[t0, t0 + t) are masked, where t is \ufb01rst chosen from a\nuniform distribution from 0 to the time mask parameter\nT, and t0 is chosen from [0, \u03c4 \u2212t).\n\u2022 We introduce an upper bound on the time mask so\nthat a time mask cannot be wider than p times the\nnumber of time steps.\nFigure 1 shows examples of the individual augmentations ap-\nplied to a single input. The log mel spectrograms are normal-\nized to have zero mean value, and thus setting the masked value\nto zero is equivalent to setting it to the mean value.\nWe can consider policies where multiple frequency and\ntime masks are applied. The multiple masks may overlap. In\nthis work, we mainly consider a series of hand-crafted policies,\nLibriSpeech basic (LB), LibriSpeech double (LD), Switchboard\nmild (SM) and Switchboard strong (SS) whose parameters are\nsummarized in Table 1. In Figure 2, we show an example of a\nlog mel spectrogram augmented with policies LB and LD.\nTable 1: Augmentation parameters for policies. mF and mT\ndenote the number of frequency and time masks applied.\nPolicy\nW\nF\nmF\nT\np\nmT\nNone\n0\n0\n-\n0\n-\n-\nLB\n80\n27\n1\n100\n1.0\n1\nLD\n80\n27\n2\n100\n1.0\n2\nSM\n40\n15\n2\n70\n0.2\n2\nSS\n40\n27\n2\n70\n0.2\n2\n3. Model\nWe use Listen, Attend and Spell (LAS) networks [6] for our\nASR tasks. These models, being end-to-end, are simple to train\nand have the added bene\ufb01t of having well-documented bench-\nmarks [24, 25] that we are able to build upon to get our results.\nIn this section, we review LAS networks and introduce some\nnotation to parameterize them. We also introduce the learning\nrate schedules we use to train the networks, as they turn out\nto be an important factor in determining performance. We end\nwith reviewing shallow fusion [21], which we have used to in-\ncorporate language models for further gains in performance.\n3.1. LAS Network Architectures\nWe use Listen, Attend and Spell (LAS) networks [6] for end-to-\nend ASR studied in [25], for which we use the notation LAS-\nd-w. The input log mel spectrogram is passed in to a 2-layer\nConvolutional Neural Network (CNN) with max-pooling and\nstride of 2. The output of the CNN is passes through an en-\ncoder consisting of d stacked bi-directional LSTMs with cell\nsize w to yield a series of attention vectors. The attention vec-\ntors are fed into a 2-layer RNN decoder of cell dimension w,\nwhich yields the tokens for the transcript. The text is tokenized\nusing a Word Piece Model (WPM) [26] of vocabulary size 16k\nfor LibriSpeech and 1k for Switchboard. The WPM for Lib-\nriSpeech 960h is constructed using the training set transcripts.\nFor the Switchboard 300h task, transcripts from the training set\nare combined with those of the Fisher corpus to construct the\nWPM. The \ufb01nal transcripts are obtained by a beam search with\nbeam size 8. For comparison with [25], we note that their \u201clarge\nmodel\u201d in our notation is LAS-4-1024.\n3.2. Learning Rate Schedules\nThe learning rate schedule turns out to be an important factor\nin determining the performance of ASR networks, especially\nso when augmentation is present. Here, we introduce training\nschedules that serve two purposes. First, we use these schedules\nto verify that a longer schedule improves the \ufb01nal performance\nof the network, even more so with augmentation (Table 2). Sec-\nond, based on this, we introduce very long schedules that are\nused to maximize the performance of the networks.\nWe use a learning rate schedule in which we ramp-up, hold,\nthen exponentially decay the learning rate until it reaches 1/100\nof its maximum value. The learning rate is kept constant beyond\nthis point. This schedule is parameterized by three time stamps\n(sr, si, sf) \u2013 the step sr where the ramp-up (from zero learning\nrate) is complete, the step si where exponential decay starts, and\nthe step sf where the exponential decay stops.\nThere are two more factors that introduce time scales in our\nexperiment. First, we turn on a variational weight noise [27]\nof standard deviation 0.075 at step snoise and keep it constant\nthroughout training. Weight noise is introduced in the step in-\nterval (sr, si), i.e., during the high plateau of the learning rate.\nSecond, we introduce uniform label smoothing [28] with\nuncertainty 0.1, i.e., the correct class label is assigned the con\ufb01-\ndence 0.9, while the con\ufb01dence of the other labels are increased\naccordingly. As is commented on again later on, label smooth-\ning can destabilize training for smaller learning rates, and we\nsometimes choose to turn it on only at the beginning of training\nand off when the learning rate starts to decay.\nThe two basic schedules we use, are given as the following:\n1. B(asic): (sr, snoise, si, sf) = (0.5k, 10k, 20k, 80k)\n2. D(ouble): (sr, snoise, si, sf) = (1k, 20k, 40k, 160k)\nAs discussed further in section 5, we can improve the perfor-\nmance of the trained network by using a longer schedule. We\nthus introduce the following schedule:\n3. L(ong): (sr, snoise, si, sf) = (1k, 20k, 140k, 320k)\nwhich we use to train the largest model to improve performance.\nWhen using schedule L, label smoothing with uncertainty 0.1 is\nintroduced for time steps < si = 140k for LibriSpeech 960h,\nand is subsequently turned off. For Switchboard 300h, label\nsmoothing is turned on throughout training.\n3.3. Shallow Fusion with Language Models\nWhile we are able to get state-of-the-art results with augmen-\ntation, we can get further improvements by using a language\nmodel. We thus incorporate an RNN language model by shal-\nlow fusion for both tasks. In shallow fusion, the \u201cnext token\u201d\ny\u2217in the decoding process is determined by\ny\u2217= argmax\ny\n(log P(y|x) + \u03bb log PLM(y)) ,\n(1)\ni.e., by jointly scoring the token using the base ASR model and\nthe language model. We also use a coverage penalty c [29].\nFor LibriSpeech, we use a two-layer RNN with embedding\ndimension 1024 used in [25] for the LM, which is trained on\nthe LibriSpeech LM corpus. We use identical fusion parameters\n(\u03bb = 0.35 and c = 0.05) used in [25] throughout.\nFor Switchboard, we use a two-layer RNN with embedding\ndimension 256, which is trained on the combined transcripts of\nthe Fisher and Switchboard datasets. We \ufb01nd the fusion pa-\nrameters via grid search by measuring performance on RT-03\n(LDC2007S10). We discuss the fusion parameters used in indi-\nvidual experiments in section 4.2.\n4. Experiments\nIn this section, we describe our experiments on LibriSpeech and\nSwitchboard with SpecAugment. We report state-of-the-art re-\nsults that out-perform heavily engineered hybrid systems.\n4.1. LibriSpeech 960h\nFor LibriSpeech, we use the same setup as [25], where we use\n80-dimensional \ufb01lter banks with delta and delta-delta accelera-\ntion, and a 16k word piece model [26].\nThe three networks LAS-4-1024, LAS-6-1024 and LAS-6-\n1280 are trained on LibriSpeech 960h with a combination of\naugmentation policies (None, LB, LD) and training schedules\n(B/D). Label smoothing was not applied in these experiments.\nThe experiments were run with peak learning rate of 0.001 and\nbatch size of 512, on 32 Google Cloud TPU chips for 7 days.\nOther than the augmentation policies and learning rate sched-\nules, all other hyperparameters were \ufb01xed, and no additional\ntuning was applied. We report test set numbers validated by the\ndev-other set in Table 2. We see that augmentation consistently\nimproves the performance of the trained network, and that the\nbene\ufb01t of a larger network and a longer learning rate schedule\nis more apparent with harsher augmentation.\nWe take the largest network, LAS-6-1280, and use sched-\nule L (with training time \u223c24 days) and policy LD to train\nthe network to maximize performance. We turn label smooth-\ning on for time steps < 140k as noted before. The test set\nperformance is reported by evaluating the checkpoint with best\ndev-other performance. State of the art performance is achieved\nby the LAS-6-1280 model, even without a language model. We\nTable 2: LibriSpeech test WER (%) evaluated for varying net-\nworks, schedules and policies. First row from [25].\nNetwork\nSch\nPol\nNo LM\nWith LM\nclean\nother\nclean\nother\nLAS-4-1024 [25]\nB\n-\n4.7\n13.4\n3.6\n10.3\nLAS-4-1024\nB\nLB\n3.7\n10.0\n3.4\n8.3\nB\nLD\n3.6\n9.2\n2.8\n7.5\nD\n-\n4.4\n13.3\n3.5\n10.4\nD\nLB\n3.4\n9.2\n2.7\n7.3\nD\nLD\n3.4\n8.3\n2.8\n6.8\nLAS-6-1024\nD\n-\n4.5\n13.1\n3.6\n10.3\nD\nLB\n3.4\n8.6\n2.6\n6.7\nD\nLD\n3.2\n8.0\n2.6\n6.5\nLAS-6-1280\nD\n-\n4.3\n12.9\n3.5\n10.5\nD\nLB\n3.4\n8.7\n2.8\n7.1\nD\nLD\n3.2\n7.7\n2.7\n6.5\ncan incorporate an LM using shallow fusion to further improve\nperformance. The results are presented in Table 3.\nTable 3: LibriSpeech 960h WERs (%).\nMethod\nNo LM\nWith LM\nclean\nother\nclean\nother\nHMM\nPanayotov et al., (2015) [20]\n5.51\n13.97\nPovey et al., (2016) [30]\n4.28\nHan et al., (2017) [31]\n3.51\n8.58\nYang et al. (2018) [32]\n2.97\n7.50\nCTC/ASG\nCollobert et al., (2016) [33]\n7.2\nLiptchinsky et al., (2017) [34]\n6.7\n20.8\n4.8\n14.5\nZhou et al., (2018) [35]\n5.42\n14.70\nZeghidour et al., (2018) [36]\n3.44\n11.24\nLi et al., (2019) [37]\n3.86\n11.95\n2.95\n8.79\nLAS\nZeyer et al., (2018) [24]\n4.87\n15.39\n3.82\n12.76\nZeyer et al., (2018) [38]\n4.70\n15.20\nIrie et al., (2019) [25]\n4.7\n13.4\n3.6\n10.3\nSabour et al., (2019) [39]\n4.5\n13.3\nOur Work\nLAS\n4.1\n12.5\n3.2\n9.8\nLAS + SpecAugment\n2.8\n6.8\n2.5\n5.8\n4.2. Switchboard 300h\nFor Switchboard 300h, we use the Kaldi [40] \u201cs5c\u201d recipe to\nprocess our data, but we adapt the recipe to use 80-dimensional\n\ufb01lter banks with delta and delta-delta acceleration. We use a 1k\nWPM [26] to tokenize the output, constructed using the com-\nbined vocabulary of the Switchboard and Fisher transcripts.\nWe train LAS-4-1024 with policies (None, SM, SS) and\nschedule B. As before, we set the peak learning rate to 0.001\nand total batch size to 512, and train using 32 Google Cloud\nTPU chips. Here the experiments are run with and without la-\nbel smoothing. Not having a canonical development set, we\nchoose to evaluate the checkpoint at the end point of the train-\ning schedule, which we choose to be 100k steps for schedule B.\nWe note that the training curve relaxes after the decay schedule\nis completed (step sf), and the performance of the network does\nnot vary much. The performance of various augmentation poli-\ncies with and without label smoothing for Switchboard 300h is\nshown in Table 4. We see that label smoothing and augmenta-\ntion have an additive effect for this corpus.\nTable 4: Switchboard 300h WER (%) evaluated for LAS-4-1024\ntrained with schedule B with varying augmentation and Label\nSmoothing (LS) policies. No LMs have been used.\nPolicy\nLS\nSWBD\nCH\n-\n\u00d7\n12.1\n22.6\n\u25e6\n11.2\n21.6\nSM\n\u00d7\n9.5\n18.8\n\u25e6\n8.5\n16.1\nSS\n\u00d7\n9.7\n18.2\n\u25e6\n8.6\n16.3\nAs with LibriSpeech 960h, we train LAS-6-1280 on the\nSwitchboard 300h training set with schedule L (training time\n\u223c24 days) to get state of the art performance. In this case, we\n\ufb01nd that turning label smoothing on throughout training ben-\ne\ufb01ts the \ufb01nal performance. We report the performance at the\nend of training time at 340k steps. We present our results in the\ncontext of other work in Table 5. We also apply shallow fusion\nwith an LM trained on Fisher-Switchboard, whose fusion pa-\nrameters are obtained by evaluating performance on the RT-03\ncorpus. Unlike the case for LibriSpeech, the fusion parameters\ndo not transfer well between networks trained differently\u2014the\nthree entries in Table 5 were obtained by using fusion parame-\nters (\u03bb, c) = (0.3, 0.05), (0.2, 0.0125) and (0.1, 0.025) respec-\ntively.\nTable 5: Switchboard 300h WERs (%).\nMethod\nNo LM\nWith LM\nSWBD\nCH\nSWBD\nCH\nHMM\nVesel\u00b4y et al., (2013) [41]\n12.9\n24.5\nPovey et al., (2016) [30]\n9.6\n19.3\nHadian et al., (2018) [42]\n9.3\n18.9\nZeyer et al., (2018) [24]\n8.3\n17.3\nCTC\nZweig et al., (2017) [43]\n24.7\n37.1\n14.0\n25.3\nAudhkhasi et al., (2018) [44]\n20.8\n30.4\nAudhkhasi et al., (2018) [45]\n14.6\n23.6\nLAS\nLu et al., (2016) [46]\n26.8\n48.2\n25.8\n46.0\nToshniwal et al., (2017) [47]\n23.1\n40.8\nZeyer et al., (2018) [24]\n13.1\n26.1\n11.8\n25.7\nWeng et al., (2018) [48]\n12.2\n23.3\nZeyer et al., (2018) [38]\n11.9\n23.7\n11.0\n23.1\nOur Work\nLAS\n11.2\n21.6\n10.9\n19.4\nLAS + SpecAugment (SM)\n7.2\n14.6\n6.8\n14.1\nLAS + SpecAugment (SS)\n7.3\n14.4\n7.1\n14.0\n5. Discussion\nTime warping contributes, but is not a major factor in im-\nproving performance. In Table 6, we present three training\nresults for which time warping, time masking and frequency\nmasking have been turned off, respectively. We see that the ef-\nfect time warping, while small, is still existent. Time warping,\nbeing the most expensive as well as the least in\ufb02uential of the\naugmentations discussed in this work, should be the \ufb01rst aug-\nmentation to be dropped given any budgetary limitations.\nTable 6: Test set WER (%) evaluated without LM for network\nLAS-4-1024 trained with schedule B.\nW\nF\nmF\nT\np\nmT\ntest-other\ntest\n80\n27\n1\n100\n1.0\n1\n10.0\n3.7\n0\n27\n1\n100\n1.0\n1\n10.1\n3.8\n80\n0\n-\n100\n1.0\n1\n11.0\n4.0\n80\n27\n1\n0\n-\n-\n10.9\n4.1\nLabel smoothing introduces instability to training. We have\nnoticed that the proportion of unstable training runs increases\nfor LibriSpeech when label smoothing is applied with aug-\nmentation.\nThis becomes more conspicuous while learning\nrate is being decayed, thus our introduction of a label smooth-\ning schedule for training LibriSpeech, where labels are only\nsmoothed in the initial phases of the learning rate schedule.\nAugmentation converts an over-\ufb01tting problem into an\nunder-\ufb01tting problem. As can be observed from the training\ncurves of the networks in Figure 3, the networks during training\nnot only under-\ufb01t the loss and WER on the augmented training\nset, but also on the training set itself when trained on augmented\ndata. This is in stark contrast to the usual situation where net-\nworks tend to over-\ufb01t to the training data. This is the major\nbene\ufb01t of training with augmentation, as explained below.\nFigure 3: LAS-6-1280 on LibriSpeech with schedule D.\nCommon methods of addressing under-\ufb01tting yield im-\nprovements. We were able to make signi\ufb01cant gains in per-\nformance by standard approaches to alleviate under-\ufb01tting\u2014\nmaking larger networks and training longer. The current re-\nported performance was obtained by the recursive process of\napplying a harsh augmentation policy, and then making wider,\ndeeper networks and training them with longer schedules to ad-\ndress the under-\ufb01tting.\nRemark on related works. We note that an augmentation sim-\nilar to frequency masking has been studied in the context of\nCNN acoustic models in [49]. There, blocks of adjacent fre-\nquencies are pre-grouped into bins, which are randomly zeroed-\nout per minibatch. On the other hand, both the size and position\nof the frequency masks in SpecAugment are chosen stochasti-\ncally, and differ for every input in the minibatch. More ideas for\nstructurally omitting frequency data of spectrograms have been\ndiscussed in [50].\n6. Conclusions\nSpecAugment greatly improves the performance of ASR net-\nworks. We are able to obtain state-of-the-art results on the Lib-\nriSpeech 960h and Switchboard 300h tasks on end-to-end LAS\nnetworks by augmenting the training set using simple hand-\ncrafted policies, surpassing the performance of hybrid systems\neven without the aid of a language model. SpecAugment con-\nverts ASR from an over-\ufb01tting to an under-\ufb01tting problem, and\nwe were able to gain performance by using bigger networks and\ntraining longer.\nAcknowledgements:\nWe would like to thank Yuan Cao,\nCiprian Chelba, Kazuki Irie, Ye Jia, Anjuli Kannan, Patrick\nNguyen, Vijay Peddinti, Rohit Prabhavalkar, Yonghui Wu and\nShuyuan Zhang for useful discussions. We also thank Gy\u00a8orgy\nKov\u00b4acs for introducing us to the works [49, 50].\n7. References\n[1] G. Hinton, L. Deng, D. Yu, G. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V. Vanhoucke, P. Nguyen, B. Kingsbury et al., \u201cDeep\nneural networks for acoustic modeling in speech recognition,\u201d\nIEEE Signal processing magazine, vol. 29, 2012.\n[2] G. Dahl, D. Yu, L. Deng, and A. Acero, \u201cContext-Dependent\nPre-Trained Deep Neural Networks for Large-Vocabulary Speech\nRecognition,\u201d IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, vol. 20, Jan 2012.\n[3] T. Sainath, A. rahman Mohamed, B. Kingsbury, and B. Ramab-\nhadran, \u201cDeep Convolutional Neural Networks for LVCSR,\u201d in\nICASSP, 2013.\n[4] A. Graves, A. rahman Mohamed, and G. Hinton, \u201cSpeech Recog-\nnition with Deep Recurrent Neural Networks,\u201d in ICASSP, 2013.\n[5] A. Graves and N. Jaitly, \u201cTowards End-to-End Speech Recogni-\ntion with Recurrent Neural Networks,\u201d in ICML, 2014.\n[6] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, \u201cListen, Attend and\nSpell: A Neural Network for Large Vocabulary Conversational\nSpeech Recognition,\u201d in ICASSP, 2016.\n[7] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,\n\u201cEnd-to-End Attention-based Large Vocabulary Speech Recogni-\ntion,\u201d in ICASSP, 2016.\n[8] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, N. Jaitly,\nB. Li, J. Chorowski, and M. Bacchiani, \u201cState-of-the-art Speech\nRecognition With Sequence-to-Sequence Models,\u201d in ICASSP,\n2018.\n[9] N. Kanda, R. Takeda, and Y. Obuchi, \u201cElastic spectral distortion\nfor low resource speech recognition with deep neural networks,\u201d\nin ASRU, 2013.\n[10] A. Ragni, K. M. Knill, S. P. Rath, and M. J. F. Gales, \u201cData aug-\nmentation for low resource languages,\u201d in INTERSPEECH, 2014.\n[11] N. Jaitly and G. Hinton, \u201cVocal Tract Length Perturbation (VTLP)\nimproves speech recognition,\u201d in ICML Workshop on Deep Learn-\ning for Audio, Speech and Language Processing, 2013.\n[12] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos,\nE. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and\nA. Ng, \u201cDeep Speech: Scaling up end-to-end speech recognition,\u201d\nin arXiv, 2014.\n[13] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, \u201cAudio Augmen-\ntation for Speech Recognition,\u201d in INTERSPEECH, 2015.\n[14] C. Kim, A. Misra, K. Chin, T. Hughes, A. Narayanan, T. Sainath,\nand M. Bacchiani, \u201cGeneration of large-scale simulated utterances\nin virtual rooms to train deep-neural networks for far-\ufb01eld speech\nrecognition in Google Home,\u201d in INTERSPEECH, 2017.\n[15] R. Prabhavalkar, R. Alvarez, C. Parada, P. Nakkiran, and T. N.\nSainath, \u201cAutomatic gain control and multi-style training for ro-\nbust small-footprint keyword spotting with deep neural networks,\u201d\nin ICASSP, 2015.\n[16] A. Raju, S. Panchapagesan, X. Liu, A. Mandal, and N. Strom,\n\u201cData Augmentation for Robust Keyword Spotting under Play-\nback Interference,\u201d in arXiv, 2018.\n[17] S. H. Mallidi and H. Hermansky, \u201cNovel neural network based\nfusion for Multistream ASR,\u201d in ICASSP, 2016.\n[18] E. D. Cubuk, B. Zoph, D. Man\u00b4e, V. Vasudevan, and Q. V. Le, \u201cAu-\ntoaugment: Learning augmentation policies from data,\u201d in CVPR,\n2019.\n[19] T. DeVries and G. Taylor, \u201cImproved Regularization of Convolu-\ntional Neural Networks with Cutout,\u201d in arXiv, 2017.\n[20] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech: An ASR corpus based on public domain audio books,\u201d\nin ICASSP, 2015.\n[21] C\u00b8 . G\u00a8ulc\u00b8ehre, O. Firat, K. Xu, K. Cho, L. Barrault, H. Lin,\nF. Bougares, H. Schwenk, and Y. Bengio, \u201cOn using monolingual\ncorpora in neural machine translation,\u201d in arxiv, 2015.\n[22] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSWITCHBOARD:\ntelephone speech corpus for research and development,\u201d in\nICASSP, 1992.\n[23] C. Cieri, D. Miller, and K. Walker, \u201cThe \ufb01sher corpus: a resource\nfor the next generations of speech-to-text,\u201d in LREC, 2004.\n[24] A. Zeyer, K. Irie, R. Schl\u00a8uter, and H. Ney, \u201cImproved training of\nend-to-end attention models for speech recognition,\u201d in INTER-\nSPEECH, 2018.\n[25] K. Irie, R. Prabhavalkar, A. Kannan, A. Bruguier, D. Rybach, and\nP. Nguyen, \u201cModel Unit Exploration for Sequence-to-Sequence\nSpeech Recognition,\u201d in arXiv, 2019.\n[26] M. Schuster and K. Nakajima, \u201cJapanese and korean voice\nsearch,\u201d in ICASSP, 2012.\n[27] A. Graves, \u201cPractical Variational Inference for Neural Networks,\u201d\nin NIPS, 2011.\n[28] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRe-\nthinking the inception architecture for computer vision,\u201d in CVPR,\n2016.\n[29] J. Chorowski and N. Jaitly, \u201cTowards better decoding and lan-\nguage model integration in sequence to sequence models,\u201d in IN-\nTERSPEECH, 2017.\n[30] D. Povey, V. Peddinti, D. Galvez, P. Ghahrmani, V. Manohar,\nX. Na, Y. Wang, and S. Khudanpur, \u201cPurely sequence-trained\nneural networks for ASR based on lattice-free MMI,\u201d in INTER-\nSPEECH, 2016.\n[31] K. J. Han, A. Chandrashekaran, J. Kim, and I. Lane, \u201cThe CA-\nPIO 2017 Conversational Speech Recognition System,\u201d in arXiv,\n2017.\n[32] X. Yang, J. Li, and X. Zhou, \u201cA novel pyramidal-FSMN archi-\ntecture with lattice-free MMI for speech recognition,\u201d in arXiv,\n2018.\n[33] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2Letter: an End-\nto-End ConvNet-based Speech Recognition System,\u201d in arXiv,\n2016.\n[34] V. Liptchinsky, G. Synnaeve, and R. Collobert, \u201cLetter-Based\nSpeech Recognition with Gated ConvNets,\u201d in arXiv, 2017.\n[35] Y. Zhou, C. Xiong, and R. Socher, \u201cImproving End-to-End\nSpeech Recognition with Policy Learning,\u201d in ICASSP, 2018.\n[36] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve,\nand R. Collobert, \u201cFully Convolutional Speech Recognition,\u201d in\narXiv, 2018.\n[37] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M.\nCohen, H. Nguyen, and R. T. Gadde, \u201cJasper: An End-to-End\nConvolutional Neural Acoustic Model,\u201d in arXiv, 2019.\n[38] A. Zeyer, A. Merboldt, R. Schl\u00a8uter, and H. Ney, \u201cA comprehen-\nsive analysis on attention models,\u201d in NIPS: Workshop IRASL,\n2018.\n[39] S. Sabour, W. Chan, and M. Norouzi, \u201cOptimal Completion Dis-\ntillation for Sequence Learning,\u201d in ICLR, 2019.\n[40] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz,\nJ. Silovsky, G. Stemmer, and K. Vesely, \u201cThe Kaldi Speech\nRecognition Toolkit,\u201d in ASRU, 2011.\n[41] K. Vesely, A. Ghoshal, L. Burger, and D. Povey, \u201cSequence-\ndiscriminative training of deep neural networks,\u201d in INTER-\nSPEECH, 2013.\n[42] H. Hadian, H. Sameti, D. Povey, and S. Khudanpur, \u201cEnd-to-end\nspeech recognition using lattice-free MMI,\u201d in INTERSPEECH,\n2018.\n[43] G. Zweig, C. Yu, J. Droppo, and A. Stolcke, \u201cAdvances in All-\nNeural Speech Recognition,\u201d in ICASSP, 2017.\n[44] K. Audhkhasi, B. Ramabhadran, G. Saon, M. Picheny, and D. Na-\nhamoo, \u201cDirect Acoustics-to-Word Models for English Conversa-\ntional Speech Recognition,\u201d in INTERSPEECH, 2018.\n[45] K. Audhkhasi, B. Kingsbury, B. Ramabhadran, G. Saon, and\nM. Picheny, \u201cBuilding competitive direct acoustics-to-word mod-\nels for english conversational speech recognition,\u201d in ICASSP,\n2018.\n[46] L. Lu, X. Zhang, and S. Renals, \u201cOn training the recurrent neural\nnetwork encoder-decoder for large vocabulary end-to-end speech\nrecognition,\u201d in ICASSP, 2016.\n[47] S. Toshniwal, H. Tang, L. Lu, and K. Livescu, \u201cMultitask Learn-\ning with Low-Level Auxiliary Tasks for Encoder-Decoder Based\nSpeech Recognition,\u201d in INTERSPEECH, 2017.\n[48] C. Weng, J. Cui, G. Wang, J. Wang, C. Yu, D. Su, and D. Yu, \u201cIm-\nproving Attention Based Sequence-to-Sequence Models for End-\nto-End English Conversational Speech Recognition,\u201d in INTER-\nSPEECH, 2018.\n[49] G. Kov\u00b4acs, L. T\u00b4oth, D. Van Compernolle, and S. Ganapathy, \u201cIn-\ncreasing the robustness of cnn acoustic models using autoregres-\nsive moving average spectrogram features and channel dropout,\u201d\nPattern Recognition Letters, vol. 100, pp. 44\u201350, 2017.\n[50] L. T\u00b4oth, G. Kov\u00b4acs, and D. Van Compernolle, \u201cA perceptually\ninspired data augmentation method for noise robust cnn acoustic\nmodels,\u201d in SPECOM, 2018.\n"
    },
    {
        "pdf_file": "paper3.pdf",
        "text": "SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS\nAlex Graves, Abdel-rahman Mohamed and Geoffrey Hinton\nDepartment of Computer Science, University of Toronto\nABSTRACT\nRecurrent neural networks (RNNs) are a powerful model for\nsequential data. End-to-end training methods such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art results in cur-\nsive handwriting recognition. However RNN performance in\nspeech recognition has so far been disappointing, with better\nresults returned by deep feedforward networks. This paper in-\nvestigates deep recurrent neural networks, which combine the\nmultiple levels of representation that have proved so effective\nin deep networks with the \ufb02exible use of long range context\nthat empowers RNNs. When trained end-to-end with suit-\nable regularisation, we \ufb01nd that deep Long Short-term Mem-\nory RNNs achieve a test set error of 17.7% on the TIMIT\nphoneme recognition benchmark, which to our knowledge is\nthe best recorded score.\nIndex Terms\u2014 recurrent neural networks, deep neural\nnetworks, speech recognition\n1. INTRODUCTION\nNeural networks have a long history in speech recognition,\nusually in combination with hidden Markov models [1, 2].\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-\nral networks (RNNs) as an alternative model. HMM-RNN\nsystems [5] have also seen a recent revival [6, 7], but do not\ncurrently perform as well as deep networks.\nInstead of combining RNNs with HMMs, it is possible\nto train RNNs \u2018end-to-end\u2019 for speech recognition [8, 9, 10].\nThis approach exploits the larger state-space and richer dy-\nnamics of RNNs compared to HMMs, and avoids the prob-\nlem of using potentially incorrect alignments as training tar-\ngets. The combination of Long Short-term Memory [11], an\nRNN architecture with an improved memory, with end-to-end\ntraining has proved especially effective for cursive handwrit-\ning recognition [12, 13]. However it has so far made little\nimpact on speech recognition.\nRNNs are inherently deep in time, since their hidden state\nis a function of all previous hidden states. The question that\ninspired this paper was whether RNNs could also bene\ufb01t from\ndepth in space; that is from stacking multiple recurrent hid-\nden layers on top of each other, just as feedforward layers are\nstacked in conventional deep networks. To answer this ques-\ntion we introduce deep Long Short-term Memory RNNs and\nassess their potential for speech recognition. We also present\nan enhancement to a recently introduced end-to-end learning\nmethod that jointly trains two separate RNNs as acoustic and\nlinguistic models [10]. Sections 2 and 3 describe the network\narchitectures and training methods, Section 4 provides exper-\nimental results and concluding remarks are given in Section 5.\n2. RECURRENT NEURAL NETWORKS\nGiven an input sequence x = (x1, . . . , xT ), a standard recur-\nrent neural network (RNN) computes the hidden vector se-\nquence h = (h1, . . . , hT ) and output vector sequence y =\n(y1, . . . , yT ) by iterating the following equations from t = 1\nto T:\nht = H (Wxhxt + Whhht\u22121 + bh)\n(1)\nyt = Whyht + by\n(2)\nwhere the W terms denote weight matrices (e.g. Wxh is the\ninput-hidden weight matrix), the b terms denote bias vectors\n(e.g. bh is hidden bias vector) and H is the hidden layer func-\ntion.\nH is usually an elementwise application of a sigmoid\nfunction. However we have found that the Long Short-Term\nMemory (LSTM) architecture [11], which uses purpose-built\nmemory cells to store information, is better at \ufb01nding and ex-\nploiting long range context. Fig. 1 illustrates a single LSTM\nmemory cell. For the version of LSTM used in this paper [14]\nH is implemented by the following composite function:\nit = \u03c3 (Wxixt + Whiht\u22121 + Wcict\u22121 + bi)\n(3)\nft = \u03c3 (Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf)\n(4)\nct = ftct\u22121 + it tanh (Wxcxt + Whcht\u22121 + bc)\n(5)\not = \u03c3 (Wxoxt + Whoht\u22121 + Wcoct + bo)\n(6)\nht = ot tanh(ct)\n(7)\nwhere \u03c3 is the logistic sigmoid function, and i, f, o and c\nare respectively the input gate, forget gate, output gate and\narXiv:1303.5778v1  [cs.NE]  22 Mar 2013\n"
    },
    {
        "pdf_file": "paper3.pdf",
        "text": "SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS\nAlex Graves, Abdel-rahman Mohamed and Geoffrey Hinton\nDepartment of Computer Science, University of Toronto\nABSTRACT\nRecurrent neural networks (RNNs) are a powerful model for\nsequential data. End-to-end training methods such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art results in cur-\nsive handwriting recognition. However RNN performance in\nspeech recognition has so far been disappointing, with better\nresults returned by deep feedforward networks. This paper in-\nvestigates deep recurrent neural networks, which combine the\nmultiple levels of representation that have proved so effective\nin deep networks with the \ufb02exible use of long range context\nthat empowers RNNs. When trained end-to-end with suit-\nable regularisation, we \ufb01nd that deep Long Short-term Mem-\nory RNNs achieve a test set error of 17.7% on the TIMIT\nphoneme recognition benchmark, which to our knowledge is\nthe best recorded score.\nIndex Terms\u2014 recurrent neural networks, deep neural\nnetworks, speech recognition\n1. INTRODUCTION\nNeural networks have a long history in speech recognition,\nusually in combination with hidden Markov models [1, 2].\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-\nral networks (RNNs) as an alternative model. HMM-RNN\nsystems [5] have also seen a recent revival [6, 7], but do not\ncurrently perform as well as deep networks.\nInstead of combining RNNs with HMMs, it is possible\nto train RNNs \u2018end-to-end\u2019 for speech recognition [8, 9, 10].\nThis approach exploits the larger state-space and richer dy-\nnamics of RNNs compared to HMMs, and avoids the prob-\nlem of using potentially incorrect alignments as training tar-\ngets. The combination of Long Short-term Memory [11], an\nRNN architecture with an improved memory, with end-to-end\ntraining has proved especially effective for cursive handwrit-\ning recognition [12, 13]. However it has so far made little\nimpact on speech recognition.\nRNNs are inherently deep in time, since their hidden state\nis a function of all previous hidden states. The question that\ninspired this paper was whether RNNs could also bene\ufb01t from\ndepth in space; that is from stacking multiple recurrent hid-\nden layers on top of each other, just as feedforward layers are\nstacked in conventional deep networks. To answer this ques-\ntion we introduce deep Long Short-term Memory RNNs and\nassess their potential for speech recognition. We also present\nan enhancement to a recently introduced end-to-end learning\nmethod that jointly trains two separate RNNs as acoustic and\nlinguistic models [10]. Sections 2 and 3 describe the network\narchitectures and training methods, Section 4 provides exper-\nimental results and concluding remarks are given in Section 5.\n2. RECURRENT NEURAL NETWORKS\nGiven an input sequence x = (x1, . . . , xT ), a standard recur-\nrent neural network (RNN) computes the hidden vector se-\nquence h = (h1, . . . , hT ) and output vector sequence y =\n(y1, . . . , yT ) by iterating the following equations from t = 1\nto T:\nht = H (Wxhxt + Whhht\u22121 + bh)\n(1)\nyt = Whyht + by\n(2)\nwhere the W terms denote weight matrices (e.g. Wxh is the\ninput-hidden weight matrix), the b terms denote bias vectors\n(e.g. bh is hidden bias vector) and H is the hidden layer func-\ntion.\nH is usually an elementwise application of a sigmoid\nfunction. However we have found that the Long Short-Term\nMemory (LSTM) architecture [11], which uses purpose-built\nmemory cells to store information, is better at \ufb01nding and ex-\nploiting long range context. Fig. 1 illustrates a single LSTM\nmemory cell. For the version of LSTM used in this paper [14]\nH is implemented by the following composite function:\nit = \u03c3 (Wxixt + Whiht\u22121 + Wcict\u22121 + bi)\n(3)\nft = \u03c3 (Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf)\n(4)\nct = ftct\u22121 + it tanh (Wxcxt + Whcht\u22121 + bc)\n(5)\not = \u03c3 (Wxoxt + Whoht\u22121 + Wcoct + bo)\n(6)\nht = ot tanh(ct)\n(7)\nwhere \u03c3 is the logistic sigmoid function, and i, f, o and c\nare respectively the input gate, forget gate, output gate and\narXiv:1303.5778v1  [cs.NE]  22 Mar 2013\nFig. 1. Long Short-term Memory Cell\nFig. 2. Bidirectional RNN\ncell activation vectors, all of which are the same size as the\nhidden vector h. The weight matrices from the cell to gate\nvectors (e.g. Wsi) are diagonal, so element m in each gate\nvector only receives input from element m of the cell vector.\nOne shortcoming of conventional RNNs is that they are\nonly able to make use of previous context. In speech recog-\nnition, where whole utterances are transcribed at once, there\nis no reason not to exploit future context as well. Bidirec-\ntional RNNs (BRNNs) [15] do this by processing the data in\nboth directions with two separate hidden layers, which are\nthen fed forwards to the same output layer. As illustrated in\nFig. 2, a BRNN computes the forward hidden sequence \u2212\n\u2192\nh ,\nthe backward hidden sequence \u2190\n\u2212\nh and the output sequence y\nby iterating the backward layer from t = T to 1, the forward\nlayer from t = 1 to T and then updating the output layer:\n\u2212\u2192h t = H\n\u0010\nWx\u2212\n\u2192\nh xt + W\u2212\n\u2192\nh \u2212\n\u2192\nh\n\u2212\u2192h t\u22121 + b\u2212\n\u2192\nh\n\u0011\n(8)\n\u2190\u2212h t = H\n\u0010\nWx\u2190\n\u2212\nh xt + W\u2190\n\u2212\nh \u2190\n\u2212\nh\n\u2190\u2212h t+1 + b\u2190\n\u2212\nh\n\u0011\n(9)\nyt = W\u2212\n\u2192\nh y\n\u2212\u2192h t + W\u2190\n\u2212\nh y\n\u2190\u2212h t + by\n(10)\nCombing BRNNs with LSTM gives bidirectional LSTM [16],\nwhich can access long-range context in both input directions.\nA crucial element of the recent success of hybrid HMM-\nneural network systems is the use of deep architectures, which\nare able to build up progressively higher level representations\nof acoustic data. Deep RNNs can be created by stacking mul-\ntiple RNN hidden layers on top of each other, with the out-\nput sequence of one layer forming the input sequence for the\nnext. Assuming the same hidden layer function is used for\nall N layers in the stack, the hidden vector sequences hn are\niteratively computed from n = 1 to N and t = 1 to T:\nhn\nt = H\n\u0000Whn\u22121hnhn\u22121\nt\n+ Whnhnhn\nt\u22121 + bn\nh\n\u0001\n(11)\nwhere we de\ufb01ne h0 = x. The network outputs yt are\nyt = WhNyhN\nt + by\n(12)\nDeep bidirectional RNNs can be implemented by replacing\neach hidden sequence hn with the forward and backward se-\nquences \u2212\n\u2192\nh n and \u2190\n\u2212\nh n, and ensuring that every hidden layer\nreceives input from both the forward and backward layers at\nthe level below. If LSTM is used for the hidden layers we get\ndeep bidirectional LSTM, the main architecture used in this\npaper. As far as we are aware this is the \ufb01rst time deep LSTM\nhas been applied to speech recognition, and we \ufb01nd that it\nyields a dramatic improvement over single-layer LSTM.\n3. NETWORK TRAINING\nWe focus on end-to-end training, where RNNs learn to map\ndirectly from acoustic to phonetic sequences. One advantage\nof this approach is that it removes the need for a prede\ufb01ned\n(and error-prone) alignment to create the training targets. The\n\ufb01rst step is to to use the network outputs to parameterise a\ndifferentiable distribution Pr(y|x) over all possible phonetic\noutput sequences y given an acoustic input sequence x. The\nlog-probability log Pr(z|x) of the target output sequence z\ncan then be differentiated with respect to the network weights\nusing backpropagation through time [17], and the whole sys-\ntem can be optimised with gradient descent. We now describe\ntwo ways to de\ufb01ne the output distribution and hence train the\nnetwork. We refer throughout to the length of x as T, the\nlength of z as U, and the number of possible phonemes as K.\n3.1. Connectionist Temporal Classi\ufb01cation\nThe \ufb01rst method, known as Connectionist Temporal Classi-\n\ufb01cation (CTC) [8, 9], uses a softmax layer to de\ufb01ne a sepa-\nrate output distribution Pr(k|t) at every step t along the in-\nput sequence. This distribution covers the K phonemes plus\nan extra blank symbol \u2205which represents a non-output (the\nsoftmax layer is therefore size K + 1). Intuitively the net-\nwork decides whether to emit any label, or no label, at every\ntimestep. Taken together these decisions de\ufb01ne a distribu-\ntion over alignments between the input and target sequences.\nCTC then uses a forward-backward algorithm to sum over all\n"
    },
    {
        "pdf_file": "paper3.pdf",
        "text": "SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS\nAlex Graves, Abdel-rahman Mohamed and Geoffrey Hinton\nDepartment of Computer Science, University of Toronto\nABSTRACT\nRecurrent neural networks (RNNs) are a powerful model for\nsequential data. End-to-end training methods such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art results in cur-\nsive handwriting recognition. However RNN performance in\nspeech recognition has so far been disappointing, with better\nresults returned by deep feedforward networks. This paper in-\nvestigates deep recurrent neural networks, which combine the\nmultiple levels of representation that have proved so effective\nin deep networks with the \ufb02exible use of long range context\nthat empowers RNNs. When trained end-to-end with suit-\nable regularisation, we \ufb01nd that deep Long Short-term Mem-\nory RNNs achieve a test set error of 17.7% on the TIMIT\nphoneme recognition benchmark, which to our knowledge is\nthe best recorded score.\nIndex Terms\u2014 recurrent neural networks, deep neural\nnetworks, speech recognition\n1. INTRODUCTION\nNeural networks have a long history in speech recognition,\nusually in combination with hidden Markov models [1, 2].\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-\nral networks (RNNs) as an alternative model. HMM-RNN\nsystems [5] have also seen a recent revival [6, 7], but do not\ncurrently perform as well as deep networks.\nInstead of combining RNNs with HMMs, it is possible\nto train RNNs \u2018end-to-end\u2019 for speech recognition [8, 9, 10].\nThis approach exploits the larger state-space and richer dy-\nnamics of RNNs compared to HMMs, and avoids the prob-\nlem of using potentially incorrect alignments as training tar-\ngets. The combination of Long Short-term Memory [11], an\nRNN architecture with an improved memory, with end-to-end\ntraining has proved especially effective for cursive handwrit-\ning recognition [12, 13]. However it has so far made little\nimpact on speech recognition.\nRNNs are inherently deep in time, since their hidden state\nis a function of all previous hidden states. The question that\ninspired this paper was whether RNNs could also bene\ufb01t from\ndepth in space; that is from stacking multiple recurrent hid-\nden layers on top of each other, just as feedforward layers are\nstacked in conventional deep networks. To answer this ques-\ntion we introduce deep Long Short-term Memory RNNs and\nassess their potential for speech recognition. We also present\nan enhancement to a recently introduced end-to-end learning\nmethod that jointly trains two separate RNNs as acoustic and\nlinguistic models [10]. Sections 2 and 3 describe the network\narchitectures and training methods, Section 4 provides exper-\nimental results and concluding remarks are given in Section 5.\n2. RECURRENT NEURAL NETWORKS\nGiven an input sequence x = (x1, . . . , xT ), a standard recur-\nrent neural network (RNN) computes the hidden vector se-\nquence h = (h1, . . . , hT ) and output vector sequence y =\n(y1, . . . , yT ) by iterating the following equations from t = 1\nto T:\nht = H (Wxhxt + Whhht\u22121 + bh)\n(1)\nyt = Whyht + by\n(2)\nwhere the W terms denote weight matrices (e.g. Wxh is the\ninput-hidden weight matrix), the b terms denote bias vectors\n(e.g. bh is hidden bias vector) and H is the hidden layer func-\ntion.\nH is usually an elementwise application of a sigmoid\nfunction. However we have found that the Long Short-Term\nMemory (LSTM) architecture [11], which uses purpose-built\nmemory cells to store information, is better at \ufb01nding and ex-\nploiting long range context. Fig. 1 illustrates a single LSTM\nmemory cell. For the version of LSTM used in this paper [14]\nH is implemented by the following composite function:\nit = \u03c3 (Wxixt + Whiht\u22121 + Wcict\u22121 + bi)\n(3)\nft = \u03c3 (Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf)\n(4)\nct = ftct\u22121 + it tanh (Wxcxt + Whcht\u22121 + bc)\n(5)\not = \u03c3 (Wxoxt + Whoht\u22121 + Wcoct + bo)\n(6)\nht = ot tanh(ct)\n(7)\nwhere \u03c3 is the logistic sigmoid function, and i, f, o and c\nare respectively the input gate, forget gate, output gate and\narXiv:1303.5778v1  [cs.NE]  22 Mar 2013\nFig. 1. Long Short-term Memory Cell\nFig. 2. Bidirectional RNN\ncell activation vectors, all of which are the same size as the\nhidden vector h. The weight matrices from the cell to gate\nvectors (e.g. Wsi) are diagonal, so element m in each gate\nvector only receives input from element m of the cell vector.\nOne shortcoming of conventional RNNs is that they are\nonly able to make use of previous context. In speech recog-\nnition, where whole utterances are transcribed at once, there\nis no reason not to exploit future context as well. Bidirec-\ntional RNNs (BRNNs) [15] do this by processing the data in\nboth directions with two separate hidden layers, which are\nthen fed forwards to the same output layer. As illustrated in\nFig. 2, a BRNN computes the forward hidden sequence \u2212\n\u2192\nh ,\nthe backward hidden sequence \u2190\n\u2212\nh and the output sequence y\nby iterating the backward layer from t = T to 1, the forward\nlayer from t = 1 to T and then updating the output layer:\n\u2212\u2192h t = H\n\u0010\nWx\u2212\n\u2192\nh xt + W\u2212\n\u2192\nh \u2212\n\u2192\nh\n\u2212\u2192h t\u22121 + b\u2212\n\u2192\nh\n\u0011\n(8)\n\u2190\u2212h t = H\n\u0010\nWx\u2190\n\u2212\nh xt + W\u2190\n\u2212\nh \u2190\n\u2212\nh\n\u2190\u2212h t+1 + b\u2190\n\u2212\nh\n\u0011\n(9)\nyt = W\u2212\n\u2192\nh y\n\u2212\u2192h t + W\u2190\n\u2212\nh y\n\u2190\u2212h t + by\n(10)\nCombing BRNNs with LSTM gives bidirectional LSTM [16],\nwhich can access long-range context in both input directions.\nA crucial element of the recent success of hybrid HMM-\nneural network systems is the use of deep architectures, which\nare able to build up progressively higher level representations\nof acoustic data. Deep RNNs can be created by stacking mul-\ntiple RNN hidden layers on top of each other, with the out-\nput sequence of one layer forming the input sequence for the\nnext. Assuming the same hidden layer function is used for\nall N layers in the stack, the hidden vector sequences hn are\niteratively computed from n = 1 to N and t = 1 to T:\nhn\nt = H\n\u0000Whn\u22121hnhn\u22121\nt\n+ Whnhnhn\nt\u22121 + bn\nh\n\u0001\n(11)\nwhere we de\ufb01ne h0 = x. The network outputs yt are\nyt = WhNyhN\nt + by\n(12)\nDeep bidirectional RNNs can be implemented by replacing\neach hidden sequence hn with the forward and backward se-\nquences \u2212\n\u2192\nh n and \u2190\n\u2212\nh n, and ensuring that every hidden layer\nreceives input from both the forward and backward layers at\nthe level below. If LSTM is used for the hidden layers we get\ndeep bidirectional LSTM, the main architecture used in this\npaper. As far as we are aware this is the \ufb01rst time deep LSTM\nhas been applied to speech recognition, and we \ufb01nd that it\nyields a dramatic improvement over single-layer LSTM.\n3. NETWORK TRAINING\nWe focus on end-to-end training, where RNNs learn to map\ndirectly from acoustic to phonetic sequences. One advantage\nof this approach is that it removes the need for a prede\ufb01ned\n(and error-prone) alignment to create the training targets. The\n\ufb01rst step is to to use the network outputs to parameterise a\ndifferentiable distribution Pr(y|x) over all possible phonetic\noutput sequences y given an acoustic input sequence x. The\nlog-probability log Pr(z|x) of the target output sequence z\ncan then be differentiated with respect to the network weights\nusing backpropagation through time [17], and the whole sys-\ntem can be optimised with gradient descent. We now describe\ntwo ways to de\ufb01ne the output distribution and hence train the\nnetwork. We refer throughout to the length of x as T, the\nlength of z as U, and the number of possible phonemes as K.\n3.1. Connectionist Temporal Classi\ufb01cation\nThe \ufb01rst method, known as Connectionist Temporal Classi-\n\ufb01cation (CTC) [8, 9], uses a softmax layer to de\ufb01ne a sepa-\nrate output distribution Pr(k|t) at every step t along the in-\nput sequence. This distribution covers the K phonemes plus\nan extra blank symbol \u2205which represents a non-output (the\nsoftmax layer is therefore size K + 1). Intuitively the net-\nwork decides whether to emit any label, or no label, at every\ntimestep. Taken together these decisions de\ufb01ne a distribu-\ntion over alignments between the input and target sequences.\nCTC then uses a forward-backward algorithm to sum over all\npossible alignments and determine the normalised probability\nPr(z|x) of the target sequence given the input sequence [8].\nSimilar procedures have been used elsewhere in speech and\nhandwriting recognition to integrate out over possible seg-\nmentations [18, 19]; however CTC differs in that it ignores\nsegmentation altogether and sums over single-timestep label\ndecisions instead.\nRNNs trained with CTC are generally bidirectional, to en-\nsure that every Pr(k|t) depends on the entire input sequence,\nand not just the inputs up to t. In this work we focus on deep\nbidirectional networks, with Pr(k|t) de\ufb01ned as follows:\nyt = W\u2212\n\u2192\nh Ny\n\u2212\u2192h N\nt + W\u2190\n\u2212\nh Ny\n\u2190\u2212h N\nt + by\n(13)\nPr(k|t) =\nexp(yt[k])\nPK\nk\u2032=1 exp(yt[k\u2032])\n,\n(14)\nwhere yt[k] is the kth element of the length K + 1 unnor-\nmalised output vector yt, and N is the number of bidirectional\nlevels.\n3.2. RNN Transducer\nCTC de\ufb01nes a distribution over phoneme sequences that de-\npends only on the acoustic input sequence x. It is therefore\nan acoustic-only model. A recent augmentation, known as an\nRNN transducer [10] combines a CTC-like network with a\nseparate RNN that predicts each phoneme given the previous\nones, thereby yielding a jointly trained acoustic and language\nmodel. Joint LM-acoustic training has proved bene\ufb01cial in\nthe past for speech recognition [20, 21].\nWhereas CTC determines an output distribution at every\ninput timestep, an RNN transducer determines a separate dis-\ntribution Pr(k|t, u) for every combination of input timestep t\nand output timestep u. As with CTC, each distribution cov-\ners the K phonemes plus \u2205.\nIntuitively the network \u2018de-\ncides\u2019 what to output depending both on where it is in the\ninput sequence and the outputs it has already emitted. For a\nlength U target sequence z, the complete set of TU decisions\njointly determines a distribution over all possible alignments\nbetween x and z, which can then be integrated out with a\nforward-backward algorithm to determine log Pr(z|x) [10].\nIn the original formulation Pr(k|t, u) was de\ufb01ned by tak-\ning an \u2018acoustic\u2019 distribution Pr(k|t) from the CTC network,\na \u2018linguistic\u2019 distribution Pr(k|u) from the prediction net-\nwork, then multiplying the two together and renormalising.\nAn improvement introduced in this paper is to instead feed\nthe hidden activations of both networks into a separate feed-\nforward output network, whose outputs are then normalised\nwith a softmax function to yield Pr(k|t, u). This allows a\nricher set of possibilities for combining linguistic and acous-\ntic information, and appears to lead to better generalisation.\nIn particular we have found that the number of deletion errors\nencountered during decoding is reduced.\nDenote by \u2212\n\u2192\nh N and \u2190\n\u2212\nh N the uppermost forward and\nbackward hidden sequences of the CTC network, and by p\nthe hidden sequence of the prediction network. At each t, u\nthe output network is implemented by feeding \u2212\n\u2192\nh N and \u2190\n\u2212\nh N\nto a linear layer to generate the vector lt, then feeding lt and\npu to a tanh hidden layer to yield ht,u, and \ufb01nally feeding\nht,u to a size K + 1 softmax layer to determine Pr(k|t, u):\nlt = W\u2212\n\u2192\nh Nl\n\u2212\u2192h N\nt + W\u2190\n\u2212\nh Nl\n\u2190\u2212h N\nt + bl\n(15)\nht,u = tanh (Wlhlt,u + Wpbpu + bh)\n(16)\nyt,u = Whyht,u + by\n(17)\nPr(k|t, u) =\nexp(yt,u[k])\nPK\nk\u2032=1 exp(yt,u[k\u2032])\n,\n(18)\nwhere yt,u[k] is the kth element of the length K + 1 unnor-\nmalised output vector. For simplicity we constrained all non-\noutput layers to be the same size (|\u2212\u2192h n\nt | = |\u2190\u2212h n\nt | = |pu| =\n|lt| = |ht,u|); however they could be varied independently.\nRNN transducers can be trained from random initial\nweights. However they appear to work better when initialised\nwith the weights of a pretrained CTC network and a pre-\ntrained next-step prediction network (so that only the output\nnetwork starts from random weights). The output layers (and\nall associated weights) used by the networks during pretrain-\ning are removed during retraining. In this work we pretrain\nthe prediction network on the phonetic transcriptions of the\naudio training data; however for large-scale applications it\nwould make more sense to pretrain on a separate text corpus.\n3.3. Decoding\nRNN transducers can be decoded with beam search [10] to\nyield an n-best list of candidate transcriptions. In the past\nCTC networks have been decoded using either a form of best-\n\ufb01rst decoding known as pre\ufb01x search, or by simply taking the\nmost active output at every timestep [8]. In this work however\nwe exploit the same beam search as the transducer, with the\nmodi\ufb01cation that the output label probabilities Pr(k|t, u) do\nnot depend on the previous outputs (so Pr(k|t, u) = Pr(k|t)).\nWe \ufb01nd beam search both faster and more effective than pre-\n\ufb01x search for CTC. Note the n-best list from the transducer\nwas originally sorted by the length normalised log-probabilty\nlog Pr(y)/|y|; in the current work we dispense with the nor-\nmalisation (which only helps when there are many more dele-\ntions than insertions) and sort by Pr(y).\n3.4. Regularisation\nRegularisation is vital for good performance with RNNs, as\ntheir \ufb02exibility makes them prone to over\ufb01tting. Two regu-\nlarisers were used in this paper: early stopping and weight\nnoise (the addition of Gaussian noise to the network weights\nduring training [22]). Weight noise was added once per train-\ning sequence, rather than at every timestep. Weight noise\n"
    },
    {
        "pdf_file": "paper3.pdf",
        "text": "SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS\nAlex Graves, Abdel-rahman Mohamed and Geoffrey Hinton\nDepartment of Computer Science, University of Toronto\nABSTRACT\nRecurrent neural networks (RNNs) are a powerful model for\nsequential data. End-to-end training methods such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art results in cur-\nsive handwriting recognition. However RNN performance in\nspeech recognition has so far been disappointing, with better\nresults returned by deep feedforward networks. This paper in-\nvestigates deep recurrent neural networks, which combine the\nmultiple levels of representation that have proved so effective\nin deep networks with the \ufb02exible use of long range context\nthat empowers RNNs. When trained end-to-end with suit-\nable regularisation, we \ufb01nd that deep Long Short-term Mem-\nory RNNs achieve a test set error of 17.7% on the TIMIT\nphoneme recognition benchmark, which to our knowledge is\nthe best recorded score.\nIndex Terms\u2014 recurrent neural networks, deep neural\nnetworks, speech recognition\n1. INTRODUCTION\nNeural networks have a long history in speech recognition,\nusually in combination with hidden Markov models [1, 2].\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-\nral networks (RNNs) as an alternative model. HMM-RNN\nsystems [5] have also seen a recent revival [6, 7], but do not\ncurrently perform as well as deep networks.\nInstead of combining RNNs with HMMs, it is possible\nto train RNNs \u2018end-to-end\u2019 for speech recognition [8, 9, 10].\nThis approach exploits the larger state-space and richer dy-\nnamics of RNNs compared to HMMs, and avoids the prob-\nlem of using potentially incorrect alignments as training tar-\ngets. The combination of Long Short-term Memory [11], an\nRNN architecture with an improved memory, with end-to-end\ntraining has proved especially effective for cursive handwrit-\ning recognition [12, 13]. However it has so far made little\nimpact on speech recognition.\nRNNs are inherently deep in time, since their hidden state\nis a function of all previous hidden states. The question that\ninspired this paper was whether RNNs could also bene\ufb01t from\ndepth in space; that is from stacking multiple recurrent hid-\nden layers on top of each other, just as feedforward layers are\nstacked in conventional deep networks. To answer this ques-\ntion we introduce deep Long Short-term Memory RNNs and\nassess their potential for speech recognition. We also present\nan enhancement to a recently introduced end-to-end learning\nmethod that jointly trains two separate RNNs as acoustic and\nlinguistic models [10]. Sections 2 and 3 describe the network\narchitectures and training methods, Section 4 provides exper-\nimental results and concluding remarks are given in Section 5.\n2. RECURRENT NEURAL NETWORKS\nGiven an input sequence x = (x1, . . . , xT ), a standard recur-\nrent neural network (RNN) computes the hidden vector se-\nquence h = (h1, . . . , hT ) and output vector sequence y =\n(y1, . . . , yT ) by iterating the following equations from t = 1\nto T:\nht = H (Wxhxt + Whhht\u22121 + bh)\n(1)\nyt = Whyht + by\n(2)\nwhere the W terms denote weight matrices (e.g. Wxh is the\ninput-hidden weight matrix), the b terms denote bias vectors\n(e.g. bh is hidden bias vector) and H is the hidden layer func-\ntion.\nH is usually an elementwise application of a sigmoid\nfunction. However we have found that the Long Short-Term\nMemory (LSTM) architecture [11], which uses purpose-built\nmemory cells to store information, is better at \ufb01nding and ex-\nploiting long range context. Fig. 1 illustrates a single LSTM\nmemory cell. For the version of LSTM used in this paper [14]\nH is implemented by the following composite function:\nit = \u03c3 (Wxixt + Whiht\u22121 + Wcict\u22121 + bi)\n(3)\nft = \u03c3 (Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf)\n(4)\nct = ftct\u22121 + it tanh (Wxcxt + Whcht\u22121 + bc)\n(5)\not = \u03c3 (Wxoxt + Whoht\u22121 + Wcoct + bo)\n(6)\nht = ot tanh(ct)\n(7)\nwhere \u03c3 is the logistic sigmoid function, and i, f, o and c\nare respectively the input gate, forget gate, output gate and\narXiv:1303.5778v1  [cs.NE]  22 Mar 2013\nFig. 1. Long Short-term Memory Cell\nFig. 2. Bidirectional RNN\ncell activation vectors, all of which are the same size as the\nhidden vector h. The weight matrices from the cell to gate\nvectors (e.g. Wsi) are diagonal, so element m in each gate\nvector only receives input from element m of the cell vector.\nOne shortcoming of conventional RNNs is that they are\nonly able to make use of previous context. In speech recog-\nnition, where whole utterances are transcribed at once, there\nis no reason not to exploit future context as well. Bidirec-\ntional RNNs (BRNNs) [15] do this by processing the data in\nboth directions with two separate hidden layers, which are\nthen fed forwards to the same output layer. As illustrated in\nFig. 2, a BRNN computes the forward hidden sequence \u2212\n\u2192\nh ,\nthe backward hidden sequence \u2190\n\u2212\nh and the output sequence y\nby iterating the backward layer from t = T to 1, the forward\nlayer from t = 1 to T and then updating the output layer:\n\u2212\u2192h t = H\n\u0010\nWx\u2212\n\u2192\nh xt + W\u2212\n\u2192\nh \u2212\n\u2192\nh\n\u2212\u2192h t\u22121 + b\u2212\n\u2192\nh\n\u0011\n(8)\n\u2190\u2212h t = H\n\u0010\nWx\u2190\n\u2212\nh xt + W\u2190\n\u2212\nh \u2190\n\u2212\nh\n\u2190\u2212h t+1 + b\u2190\n\u2212\nh\n\u0011\n(9)\nyt = W\u2212\n\u2192\nh y\n\u2212\u2192h t + W\u2190\n\u2212\nh y\n\u2190\u2212h t + by\n(10)\nCombing BRNNs with LSTM gives bidirectional LSTM [16],\nwhich can access long-range context in both input directions.\nA crucial element of the recent success of hybrid HMM-\nneural network systems is the use of deep architectures, which\nare able to build up progressively higher level representations\nof acoustic data. Deep RNNs can be created by stacking mul-\ntiple RNN hidden layers on top of each other, with the out-\nput sequence of one layer forming the input sequence for the\nnext. Assuming the same hidden layer function is used for\nall N layers in the stack, the hidden vector sequences hn are\niteratively computed from n = 1 to N and t = 1 to T:\nhn\nt = H\n\u0000Whn\u22121hnhn\u22121\nt\n+ Whnhnhn\nt\u22121 + bn\nh\n\u0001\n(11)\nwhere we de\ufb01ne h0 = x. The network outputs yt are\nyt = WhNyhN\nt + by\n(12)\nDeep bidirectional RNNs can be implemented by replacing\neach hidden sequence hn with the forward and backward se-\nquences \u2212\n\u2192\nh n and \u2190\n\u2212\nh n, and ensuring that every hidden layer\nreceives input from both the forward and backward layers at\nthe level below. If LSTM is used for the hidden layers we get\ndeep bidirectional LSTM, the main architecture used in this\npaper. As far as we are aware this is the \ufb01rst time deep LSTM\nhas been applied to speech recognition, and we \ufb01nd that it\nyields a dramatic improvement over single-layer LSTM.\n3. NETWORK TRAINING\nWe focus on end-to-end training, where RNNs learn to map\ndirectly from acoustic to phonetic sequences. One advantage\nof this approach is that it removes the need for a prede\ufb01ned\n(and error-prone) alignment to create the training targets. The\n\ufb01rst step is to to use the network outputs to parameterise a\ndifferentiable distribution Pr(y|x) over all possible phonetic\noutput sequences y given an acoustic input sequence x. The\nlog-probability log Pr(z|x) of the target output sequence z\ncan then be differentiated with respect to the network weights\nusing backpropagation through time [17], and the whole sys-\ntem can be optimised with gradient descent. We now describe\ntwo ways to de\ufb01ne the output distribution and hence train the\nnetwork. We refer throughout to the length of x as T, the\nlength of z as U, and the number of possible phonemes as K.\n3.1. Connectionist Temporal Classi\ufb01cation\nThe \ufb01rst method, known as Connectionist Temporal Classi-\n\ufb01cation (CTC) [8, 9], uses a softmax layer to de\ufb01ne a sepa-\nrate output distribution Pr(k|t) at every step t along the in-\nput sequence. This distribution covers the K phonemes plus\nan extra blank symbol \u2205which represents a non-output (the\nsoftmax layer is therefore size K + 1). Intuitively the net-\nwork decides whether to emit any label, or no label, at every\ntimestep. Taken together these decisions de\ufb01ne a distribu-\ntion over alignments between the input and target sequences.\nCTC then uses a forward-backward algorithm to sum over all\npossible alignments and determine the normalised probability\nPr(z|x) of the target sequence given the input sequence [8].\nSimilar procedures have been used elsewhere in speech and\nhandwriting recognition to integrate out over possible seg-\nmentations [18, 19]; however CTC differs in that it ignores\nsegmentation altogether and sums over single-timestep label\ndecisions instead.\nRNNs trained with CTC are generally bidirectional, to en-\nsure that every Pr(k|t) depends on the entire input sequence,\nand not just the inputs up to t. In this work we focus on deep\nbidirectional networks, with Pr(k|t) de\ufb01ned as follows:\nyt = W\u2212\n\u2192\nh Ny\n\u2212\u2192h N\nt + W\u2190\n\u2212\nh Ny\n\u2190\u2212h N\nt + by\n(13)\nPr(k|t) =\nexp(yt[k])\nPK\nk\u2032=1 exp(yt[k\u2032])\n,\n(14)\nwhere yt[k] is the kth element of the length K + 1 unnor-\nmalised output vector yt, and N is the number of bidirectional\nlevels.\n3.2. RNN Transducer\nCTC de\ufb01nes a distribution over phoneme sequences that de-\npends only on the acoustic input sequence x. It is therefore\nan acoustic-only model. A recent augmentation, known as an\nRNN transducer [10] combines a CTC-like network with a\nseparate RNN that predicts each phoneme given the previous\nones, thereby yielding a jointly trained acoustic and language\nmodel. Joint LM-acoustic training has proved bene\ufb01cial in\nthe past for speech recognition [20, 21].\nWhereas CTC determines an output distribution at every\ninput timestep, an RNN transducer determines a separate dis-\ntribution Pr(k|t, u) for every combination of input timestep t\nand output timestep u. As with CTC, each distribution cov-\ners the K phonemes plus \u2205.\nIntuitively the network \u2018de-\ncides\u2019 what to output depending both on where it is in the\ninput sequence and the outputs it has already emitted. For a\nlength U target sequence z, the complete set of TU decisions\njointly determines a distribution over all possible alignments\nbetween x and z, which can then be integrated out with a\nforward-backward algorithm to determine log Pr(z|x) [10].\nIn the original formulation Pr(k|t, u) was de\ufb01ned by tak-\ning an \u2018acoustic\u2019 distribution Pr(k|t) from the CTC network,\na \u2018linguistic\u2019 distribution Pr(k|u) from the prediction net-\nwork, then multiplying the two together and renormalising.\nAn improvement introduced in this paper is to instead feed\nthe hidden activations of both networks into a separate feed-\nforward output network, whose outputs are then normalised\nwith a softmax function to yield Pr(k|t, u). This allows a\nricher set of possibilities for combining linguistic and acous-\ntic information, and appears to lead to better generalisation.\nIn particular we have found that the number of deletion errors\nencountered during decoding is reduced.\nDenote by \u2212\n\u2192\nh N and \u2190\n\u2212\nh N the uppermost forward and\nbackward hidden sequences of the CTC network, and by p\nthe hidden sequence of the prediction network. At each t, u\nthe output network is implemented by feeding \u2212\n\u2192\nh N and \u2190\n\u2212\nh N\nto a linear layer to generate the vector lt, then feeding lt and\npu to a tanh hidden layer to yield ht,u, and \ufb01nally feeding\nht,u to a size K + 1 softmax layer to determine Pr(k|t, u):\nlt = W\u2212\n\u2192\nh Nl\n\u2212\u2192h N\nt + W\u2190\n\u2212\nh Nl\n\u2190\u2212h N\nt + bl\n(15)\nht,u = tanh (Wlhlt,u + Wpbpu + bh)\n(16)\nyt,u = Whyht,u + by\n(17)\nPr(k|t, u) =\nexp(yt,u[k])\nPK\nk\u2032=1 exp(yt,u[k\u2032])\n,\n(18)\nwhere yt,u[k] is the kth element of the length K + 1 unnor-\nmalised output vector. For simplicity we constrained all non-\noutput layers to be the same size (|\u2212\u2192h n\nt | = |\u2190\u2212h n\nt | = |pu| =\n|lt| = |ht,u|); however they could be varied independently.\nRNN transducers can be trained from random initial\nweights. However they appear to work better when initialised\nwith the weights of a pretrained CTC network and a pre-\ntrained next-step prediction network (so that only the output\nnetwork starts from random weights). The output layers (and\nall associated weights) used by the networks during pretrain-\ning are removed during retraining. In this work we pretrain\nthe prediction network on the phonetic transcriptions of the\naudio training data; however for large-scale applications it\nwould make more sense to pretrain on a separate text corpus.\n3.3. Decoding\nRNN transducers can be decoded with beam search [10] to\nyield an n-best list of candidate transcriptions. In the past\nCTC networks have been decoded using either a form of best-\n\ufb01rst decoding known as pre\ufb01x search, or by simply taking the\nmost active output at every timestep [8]. In this work however\nwe exploit the same beam search as the transducer, with the\nmodi\ufb01cation that the output label probabilities Pr(k|t, u) do\nnot depend on the previous outputs (so Pr(k|t, u) = Pr(k|t)).\nWe \ufb01nd beam search both faster and more effective than pre-\n\ufb01x search for CTC. Note the n-best list from the transducer\nwas originally sorted by the length normalised log-probabilty\nlog Pr(y)/|y|; in the current work we dispense with the nor-\nmalisation (which only helps when there are many more dele-\ntions than insertions) and sort by Pr(y).\n3.4. Regularisation\nRegularisation is vital for good performance with RNNs, as\ntheir \ufb02exibility makes them prone to over\ufb01tting. Two regu-\nlarisers were used in this paper: early stopping and weight\nnoise (the addition of Gaussian noise to the network weights\nduring training [22]). Weight noise was added once per train-\ning sequence, rather than at every timestep. Weight noise\ntends to \u2018simplify\u2019 neural networks, in the sense of reducing\nthe amount of information required to transmit the parame-\nters [23, 24], which improves generalisation.\n4. EXPERIMENTS\nPhoneme recognition experiments were performed on the\nTIMIT corpus [25]. The standard 462 speaker set with all\nSA records removed was used for training, and a separate\ndevelopment set of 50 speakers was used for early stop-\nping. Results are reported for the 24-speaker core test set.\nThe audio data was encoded using a Fourier-transform-based\n\ufb01lter-bank with 40 coef\ufb01cients (plus energy) distributed on\na mel-scale, together with their \ufb01rst and second temporal\nderivatives. Each input vector was therefore size 123. The\ndata were normalised so that every element of the input vec-\ntors had zero mean and unit variance over the training set. All\n61 phoneme labels were used during training and decoding\n(so K = 61), then mapped to 39 classes for scoring [26].\nNote that all experiments were run only once, so the vari-\nance due to random weight initialisation and weight noise is\nunknown.\nAs shown in Table 1, nine RNNs were evaluated, vary-\ning along three main dimensions: the training method used\n(CTC, Transducer or pretrained Transducer), the number of\nhidden levels (1\u20135), and the number of LSTM cells in each\nhidden layer. Bidirectional LSTM was used for all networks\nexcept CTC-3l-500h-tanh, which had tanh units instead of\nLSTM cells, and CTC-3l-421h-uni where the LSTM layers\nwere unidirectional. All networks were trained using stochas-\ntic gradient descent, with learning rate 10\u22124, momentum 0.9\nand random initial weights drawn uniformly from [\u22120.1, 0.1].\nAll networks except CTC-3l-500h-tanh and PreTrans-3l-250h\nwere \ufb01rst trained with no noise and then, starting from the\npoint of highest log-probability on the development set, re-\ntrained with Gaussian weight noise (\u03c3 = 0.075) until the\npoint of lowest phoneme error rate on the development set.\nPreTrans-3l-250h was initialised with the weights of CTC-\n3l-250h, along with the weights of a phoneme prediction net-\nwork (which also had a hidden layer of 250 LSTM cells), both\nof which were trained without noise, retrained with noise, and\nstopped at the point of highest log-probability. PreTrans-3l-\n250h was trained from this point with noise added. CTC-3l-\n500h-tanh was entirely trained without weight noise because\nit failed to learn with noise added. Beam search decoding was\nused for all networks, with a beam width of 100.\nThe advantage of deep networks is immediately obvious,\nwith the error rate for CTC dropping from 23.9% to 18.4%\nas the number of hidden levels increases from one to \ufb01ve.\nThe four networks CTC-3l-500h-tanh, CTC-1l-622h, CTC-\n3l-421h-uni and CTC-3l-250h all had approximately the same\nnumber of weights, but give radically different results. The\nthree main conclusions we can draw from this are (a) LSTM\nworks much better than tanh for this task, (b) bidirectional\nTable 1. TIMIT Phoneme Recognition Results. \u2018Epochs\u2019 is\nthe number of passes through the training set before conver-\ngence. \u2018PER\u2019 is the phoneme error rate on the core test set.\nNETWORK\nWEIGHTS\nEPOCHS\nPER\nCTC-3L-500H-TANH\n3.7M\n107\n37.6%\nCTC-1L-250H\n0.8M\n82\n23.9%\nCTC-1L-622H\n3.8M\n87\n23.0%\nCTC-2L-250H\n2.3M\n55\n21.0%\nCTC-3L-421H-UNI\n3.8M\n115\n19.6%\nCTC-3L-250H\n3.8M\n124\n18.6%\nCTC-5L-250H\n6.8M\n150\n18.4%\nTRANS-3L-250H\n4.3M\n112\n18.3%\nPRETRANS-3L-250H\n4.3M\n144\n17.7%\nFig. 3. Input Sensitivity of a deep CTC RNN. The heatmap\n(top) shows the derivatives of the \u2018ah\u2019 and \u2018p\u2019 outputs printed\nin red with respect to the \ufb01lterbank inputs (bottom).\nThe\nTIMIT ground truth segmentation is shown below. Note that\nthe sensitivity extends to surrounding segments; this may be\nbecause CTC (which lacks an explicit language model) at-\ntempts to learn linguistic dependencies from the acoustic data.\nLSTM has a slight advantage over unidirectional LSTMand\n(c) depth is more important than layer size (which supports\nprevious \ufb01ndings for deep networks [3]). Although the advan-\ntage of the transducer is slight when the weights are randomly\ninitialised, it becomes more substantial when pretraining is\nused.\n5. CONCLUSIONS AND FUTURE WORK\nWe have shown that the combination of deep, bidirectional\nLong Short-term Memory RNNs with end-to-end training and\nweight noise gives state-of-the-art results in phoneme recog-\nnition on the TIMIT database. An obvious next step is to ex-\ntend the system to large vocabulary speech recognition. An-\nother interesting direction would be to combine frequency-\ndomain convolutional neural networks [27] with deep LSTM.\n"
    },
    {
        "pdf_file": "paper3.pdf",
        "text": "SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS\nAlex Graves, Abdel-rahman Mohamed and Geoffrey Hinton\nDepartment of Computer Science, University of Toronto\nABSTRACT\nRecurrent neural networks (RNNs) are a powerful model for\nsequential data. End-to-end training methods such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art results in cur-\nsive handwriting recognition. However RNN performance in\nspeech recognition has so far been disappointing, with better\nresults returned by deep feedforward networks. This paper in-\nvestigates deep recurrent neural networks, which combine the\nmultiple levels of representation that have proved so effective\nin deep networks with the \ufb02exible use of long range context\nthat empowers RNNs. When trained end-to-end with suit-\nable regularisation, we \ufb01nd that deep Long Short-term Mem-\nory RNNs achieve a test set error of 17.7% on the TIMIT\nphoneme recognition benchmark, which to our knowledge is\nthe best recorded score.\nIndex Terms\u2014 recurrent neural networks, deep neural\nnetworks, speech recognition\n1. INTRODUCTION\nNeural networks have a long history in speech recognition,\nusually in combination with hidden Markov models [1, 2].\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-\nral networks (RNNs) as an alternative model. HMM-RNN\nsystems [5] have also seen a recent revival [6, 7], but do not\ncurrently perform as well as deep networks.\nInstead of combining RNNs with HMMs, it is possible\nto train RNNs \u2018end-to-end\u2019 for speech recognition [8, 9, 10].\nThis approach exploits the larger state-space and richer dy-\nnamics of RNNs compared to HMMs, and avoids the prob-\nlem of using potentially incorrect alignments as training tar-\ngets. The combination of Long Short-term Memory [11], an\nRNN architecture with an improved memory, with end-to-end\ntraining has proved especially effective for cursive handwrit-\ning recognition [12, 13]. However it has so far made little\nimpact on speech recognition.\nRNNs are inherently deep in time, since their hidden state\nis a function of all previous hidden states. The question that\ninspired this paper was whether RNNs could also bene\ufb01t from\ndepth in space; that is from stacking multiple recurrent hid-\nden layers on top of each other, just as feedforward layers are\nstacked in conventional deep networks. To answer this ques-\ntion we introduce deep Long Short-term Memory RNNs and\nassess their potential for speech recognition. We also present\nan enhancement to a recently introduced end-to-end learning\nmethod that jointly trains two separate RNNs as acoustic and\nlinguistic models [10]. Sections 2 and 3 describe the network\narchitectures and training methods, Section 4 provides exper-\nimental results and concluding remarks are given in Section 5.\n2. RECURRENT NEURAL NETWORKS\nGiven an input sequence x = (x1, . . . , xT ), a standard recur-\nrent neural network (RNN) computes the hidden vector se-\nquence h = (h1, . . . , hT ) and output vector sequence y =\n(y1, . . . , yT ) by iterating the following equations from t = 1\nto T:\nht = H (Wxhxt + Whhht\u22121 + bh)\n(1)\nyt = Whyht + by\n(2)\nwhere the W terms denote weight matrices (e.g. Wxh is the\ninput-hidden weight matrix), the b terms denote bias vectors\n(e.g. bh is hidden bias vector) and H is the hidden layer func-\ntion.\nH is usually an elementwise application of a sigmoid\nfunction. However we have found that the Long Short-Term\nMemory (LSTM) architecture [11], which uses purpose-built\nmemory cells to store information, is better at \ufb01nding and ex-\nploiting long range context. Fig. 1 illustrates a single LSTM\nmemory cell. For the version of LSTM used in this paper [14]\nH is implemented by the following composite function:\nit = \u03c3 (Wxixt + Whiht\u22121 + Wcict\u22121 + bi)\n(3)\nft = \u03c3 (Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf)\n(4)\nct = ftct\u22121 + it tanh (Wxcxt + Whcht\u22121 + bc)\n(5)\not = \u03c3 (Wxoxt + Whoht\u22121 + Wcoct + bo)\n(6)\nht = ot tanh(ct)\n(7)\nwhere \u03c3 is the logistic sigmoid function, and i, f, o and c\nare respectively the input gate, forget gate, output gate and\narXiv:1303.5778v1  [cs.NE]  22 Mar 2013\nFig. 1. Long Short-term Memory Cell\nFig. 2. Bidirectional RNN\ncell activation vectors, all of which are the same size as the\nhidden vector h. The weight matrices from the cell to gate\nvectors (e.g. Wsi) are diagonal, so element m in each gate\nvector only receives input from element m of the cell vector.\nOne shortcoming of conventional RNNs is that they are\nonly able to make use of previous context. In speech recog-\nnition, where whole utterances are transcribed at once, there\nis no reason not to exploit future context as well. Bidirec-\ntional RNNs (BRNNs) [15] do this by processing the data in\nboth directions with two separate hidden layers, which are\nthen fed forwards to the same output layer. As illustrated in\nFig. 2, a BRNN computes the forward hidden sequence \u2212\n\u2192\nh ,\nthe backward hidden sequence \u2190\n\u2212\nh and the output sequence y\nby iterating the backward layer from t = T to 1, the forward\nlayer from t = 1 to T and then updating the output layer:\n\u2212\u2192h t = H\n\u0010\nWx\u2212\n\u2192\nh xt + W\u2212\n\u2192\nh \u2212\n\u2192\nh\n\u2212\u2192h t\u22121 + b\u2212\n\u2192\nh\n\u0011\n(8)\n\u2190\u2212h t = H\n\u0010\nWx\u2190\n\u2212\nh xt + W\u2190\n\u2212\nh \u2190\n\u2212\nh\n\u2190\u2212h t+1 + b\u2190\n\u2212\nh\n\u0011\n(9)\nyt = W\u2212\n\u2192\nh y\n\u2212\u2192h t + W\u2190\n\u2212\nh y\n\u2190\u2212h t + by\n(10)\nCombing BRNNs with LSTM gives bidirectional LSTM [16],\nwhich can access long-range context in both input directions.\nA crucial element of the recent success of hybrid HMM-\nneural network systems is the use of deep architectures, which\nare able to build up progressively higher level representations\nof acoustic data. Deep RNNs can be created by stacking mul-\ntiple RNN hidden layers on top of each other, with the out-\nput sequence of one layer forming the input sequence for the\nnext. Assuming the same hidden layer function is used for\nall N layers in the stack, the hidden vector sequences hn are\niteratively computed from n = 1 to N and t = 1 to T:\nhn\nt = H\n\u0000Whn\u22121hnhn\u22121\nt\n+ Whnhnhn\nt\u22121 + bn\nh\n\u0001\n(11)\nwhere we de\ufb01ne h0 = x. The network outputs yt are\nyt = WhNyhN\nt + by\n(12)\nDeep bidirectional RNNs can be implemented by replacing\neach hidden sequence hn with the forward and backward se-\nquences \u2212\n\u2192\nh n and \u2190\n\u2212\nh n, and ensuring that every hidden layer\nreceives input from both the forward and backward layers at\nthe level below. If LSTM is used for the hidden layers we get\ndeep bidirectional LSTM, the main architecture used in this\npaper. As far as we are aware this is the \ufb01rst time deep LSTM\nhas been applied to speech recognition, and we \ufb01nd that it\nyields a dramatic improvement over single-layer LSTM.\n3. NETWORK TRAINING\nWe focus on end-to-end training, where RNNs learn to map\ndirectly from acoustic to phonetic sequences. One advantage\nof this approach is that it removes the need for a prede\ufb01ned\n(and error-prone) alignment to create the training targets. The\n\ufb01rst step is to to use the network outputs to parameterise a\ndifferentiable distribution Pr(y|x) over all possible phonetic\noutput sequences y given an acoustic input sequence x. The\nlog-probability log Pr(z|x) of the target output sequence z\ncan then be differentiated with respect to the network weights\nusing backpropagation through time [17], and the whole sys-\ntem can be optimised with gradient descent. We now describe\ntwo ways to de\ufb01ne the output distribution and hence train the\nnetwork. We refer throughout to the length of x as T, the\nlength of z as U, and the number of possible phonemes as K.\n3.1. Connectionist Temporal Classi\ufb01cation\nThe \ufb01rst method, known as Connectionist Temporal Classi-\n\ufb01cation (CTC) [8, 9], uses a softmax layer to de\ufb01ne a sepa-\nrate output distribution Pr(k|t) at every step t along the in-\nput sequence. This distribution covers the K phonemes plus\nan extra blank symbol \u2205which represents a non-output (the\nsoftmax layer is therefore size K + 1). Intuitively the net-\nwork decides whether to emit any label, or no label, at every\ntimestep. Taken together these decisions de\ufb01ne a distribu-\ntion over alignments between the input and target sequences.\nCTC then uses a forward-backward algorithm to sum over all\npossible alignments and determine the normalised probability\nPr(z|x) of the target sequence given the input sequence [8].\nSimilar procedures have been used elsewhere in speech and\nhandwriting recognition to integrate out over possible seg-\nmentations [18, 19]; however CTC differs in that it ignores\nsegmentation altogether and sums over single-timestep label\ndecisions instead.\nRNNs trained with CTC are generally bidirectional, to en-\nsure that every Pr(k|t) depends on the entire input sequence,\nand not just the inputs up to t. In this work we focus on deep\nbidirectional networks, with Pr(k|t) de\ufb01ned as follows:\nyt = W\u2212\n\u2192\nh Ny\n\u2212\u2192h N\nt + W\u2190\n\u2212\nh Ny\n\u2190\u2212h N\nt + by\n(13)\nPr(k|t) =\nexp(yt[k])\nPK\nk\u2032=1 exp(yt[k\u2032])\n,\n(14)\nwhere yt[k] is the kth element of the length K + 1 unnor-\nmalised output vector yt, and N is the number of bidirectional\nlevels.\n3.2. RNN Transducer\nCTC de\ufb01nes a distribution over phoneme sequences that de-\npends only on the acoustic input sequence x. It is therefore\nan acoustic-only model. A recent augmentation, known as an\nRNN transducer [10] combines a CTC-like network with a\nseparate RNN that predicts each phoneme given the previous\nones, thereby yielding a jointly trained acoustic and language\nmodel. Joint LM-acoustic training has proved bene\ufb01cial in\nthe past for speech recognition [20, 21].\nWhereas CTC determines an output distribution at every\ninput timestep, an RNN transducer determines a separate dis-\ntribution Pr(k|t, u) for every combination of input timestep t\nand output timestep u. As with CTC, each distribution cov-\ners the K phonemes plus \u2205.\nIntuitively the network \u2018de-\ncides\u2019 what to output depending both on where it is in the\ninput sequence and the outputs it has already emitted. For a\nlength U target sequence z, the complete set of TU decisions\njointly determines a distribution over all possible alignments\nbetween x and z, which can then be integrated out with a\nforward-backward algorithm to determine log Pr(z|x) [10].\nIn the original formulation Pr(k|t, u) was de\ufb01ned by tak-\ning an \u2018acoustic\u2019 distribution Pr(k|t) from the CTC network,\na \u2018linguistic\u2019 distribution Pr(k|u) from the prediction net-\nwork, then multiplying the two together and renormalising.\nAn improvement introduced in this paper is to instead feed\nthe hidden activations of both networks into a separate feed-\nforward output network, whose outputs are then normalised\nwith a softmax function to yield Pr(k|t, u). This allows a\nricher set of possibilities for combining linguistic and acous-\ntic information, and appears to lead to better generalisation.\nIn particular we have found that the number of deletion errors\nencountered during decoding is reduced.\nDenote by \u2212\n\u2192\nh N and \u2190\n\u2212\nh N the uppermost forward and\nbackward hidden sequences of the CTC network, and by p\nthe hidden sequence of the prediction network. At each t, u\nthe output network is implemented by feeding \u2212\n\u2192\nh N and \u2190\n\u2212\nh N\nto a linear layer to generate the vector lt, then feeding lt and\npu to a tanh hidden layer to yield ht,u, and \ufb01nally feeding\nht,u to a size K + 1 softmax layer to determine Pr(k|t, u):\nlt = W\u2212\n\u2192\nh Nl\n\u2212\u2192h N\nt + W\u2190\n\u2212\nh Nl\n\u2190\u2212h N\nt + bl\n(15)\nht,u = tanh (Wlhlt,u + Wpbpu + bh)\n(16)\nyt,u = Whyht,u + by\n(17)\nPr(k|t, u) =\nexp(yt,u[k])\nPK\nk\u2032=1 exp(yt,u[k\u2032])\n,\n(18)\nwhere yt,u[k] is the kth element of the length K + 1 unnor-\nmalised output vector. For simplicity we constrained all non-\noutput layers to be the same size (|\u2212\u2192h n\nt | = |\u2190\u2212h n\nt | = |pu| =\n|lt| = |ht,u|); however they could be varied independently.\nRNN transducers can be trained from random initial\nweights. However they appear to work better when initialised\nwith the weights of a pretrained CTC network and a pre-\ntrained next-step prediction network (so that only the output\nnetwork starts from random weights). The output layers (and\nall associated weights) used by the networks during pretrain-\ning are removed during retraining. In this work we pretrain\nthe prediction network on the phonetic transcriptions of the\naudio training data; however for large-scale applications it\nwould make more sense to pretrain on a separate text corpus.\n3.3. Decoding\nRNN transducers can be decoded with beam search [10] to\nyield an n-best list of candidate transcriptions. In the past\nCTC networks have been decoded using either a form of best-\n\ufb01rst decoding known as pre\ufb01x search, or by simply taking the\nmost active output at every timestep [8]. In this work however\nwe exploit the same beam search as the transducer, with the\nmodi\ufb01cation that the output label probabilities Pr(k|t, u) do\nnot depend on the previous outputs (so Pr(k|t, u) = Pr(k|t)).\nWe \ufb01nd beam search both faster and more effective than pre-\n\ufb01x search for CTC. Note the n-best list from the transducer\nwas originally sorted by the length normalised log-probabilty\nlog Pr(y)/|y|; in the current work we dispense with the nor-\nmalisation (which only helps when there are many more dele-\ntions than insertions) and sort by Pr(y).\n3.4. Regularisation\nRegularisation is vital for good performance with RNNs, as\ntheir \ufb02exibility makes them prone to over\ufb01tting. Two regu-\nlarisers were used in this paper: early stopping and weight\nnoise (the addition of Gaussian noise to the network weights\nduring training [22]). Weight noise was added once per train-\ning sequence, rather than at every timestep. Weight noise\ntends to \u2018simplify\u2019 neural networks, in the sense of reducing\nthe amount of information required to transmit the parame-\nters [23, 24], which improves generalisation.\n4. EXPERIMENTS\nPhoneme recognition experiments were performed on the\nTIMIT corpus [25]. The standard 462 speaker set with all\nSA records removed was used for training, and a separate\ndevelopment set of 50 speakers was used for early stop-\nping. Results are reported for the 24-speaker core test set.\nThe audio data was encoded using a Fourier-transform-based\n\ufb01lter-bank with 40 coef\ufb01cients (plus energy) distributed on\na mel-scale, together with their \ufb01rst and second temporal\nderivatives. Each input vector was therefore size 123. The\ndata were normalised so that every element of the input vec-\ntors had zero mean and unit variance over the training set. All\n61 phoneme labels were used during training and decoding\n(so K = 61), then mapped to 39 classes for scoring [26].\nNote that all experiments were run only once, so the vari-\nance due to random weight initialisation and weight noise is\nunknown.\nAs shown in Table 1, nine RNNs were evaluated, vary-\ning along three main dimensions: the training method used\n(CTC, Transducer or pretrained Transducer), the number of\nhidden levels (1\u20135), and the number of LSTM cells in each\nhidden layer. Bidirectional LSTM was used for all networks\nexcept CTC-3l-500h-tanh, which had tanh units instead of\nLSTM cells, and CTC-3l-421h-uni where the LSTM layers\nwere unidirectional. All networks were trained using stochas-\ntic gradient descent, with learning rate 10\u22124, momentum 0.9\nand random initial weights drawn uniformly from [\u22120.1, 0.1].\nAll networks except CTC-3l-500h-tanh and PreTrans-3l-250h\nwere \ufb01rst trained with no noise and then, starting from the\npoint of highest log-probability on the development set, re-\ntrained with Gaussian weight noise (\u03c3 = 0.075) until the\npoint of lowest phoneme error rate on the development set.\nPreTrans-3l-250h was initialised with the weights of CTC-\n3l-250h, along with the weights of a phoneme prediction net-\nwork (which also had a hidden layer of 250 LSTM cells), both\nof which were trained without noise, retrained with noise, and\nstopped at the point of highest log-probability. PreTrans-3l-\n250h was trained from this point with noise added. CTC-3l-\n500h-tanh was entirely trained without weight noise because\nit failed to learn with noise added. Beam search decoding was\nused for all networks, with a beam width of 100.\nThe advantage of deep networks is immediately obvious,\nwith the error rate for CTC dropping from 23.9% to 18.4%\nas the number of hidden levels increases from one to \ufb01ve.\nThe four networks CTC-3l-500h-tanh, CTC-1l-622h, CTC-\n3l-421h-uni and CTC-3l-250h all had approximately the same\nnumber of weights, but give radically different results. The\nthree main conclusions we can draw from this are (a) LSTM\nworks much better than tanh for this task, (b) bidirectional\nTable 1. TIMIT Phoneme Recognition Results. \u2018Epochs\u2019 is\nthe number of passes through the training set before conver-\ngence. \u2018PER\u2019 is the phoneme error rate on the core test set.\nNETWORK\nWEIGHTS\nEPOCHS\nPER\nCTC-3L-500H-TANH\n3.7M\n107\n37.6%\nCTC-1L-250H\n0.8M\n82\n23.9%\nCTC-1L-622H\n3.8M\n87\n23.0%\nCTC-2L-250H\n2.3M\n55\n21.0%\nCTC-3L-421H-UNI\n3.8M\n115\n19.6%\nCTC-3L-250H\n3.8M\n124\n18.6%\nCTC-5L-250H\n6.8M\n150\n18.4%\nTRANS-3L-250H\n4.3M\n112\n18.3%\nPRETRANS-3L-250H\n4.3M\n144\n17.7%\nFig. 3. Input Sensitivity of a deep CTC RNN. The heatmap\n(top) shows the derivatives of the \u2018ah\u2019 and \u2018p\u2019 outputs printed\nin red with respect to the \ufb01lterbank inputs (bottom).\nThe\nTIMIT ground truth segmentation is shown below. Note that\nthe sensitivity extends to surrounding segments; this may be\nbecause CTC (which lacks an explicit language model) at-\ntempts to learn linguistic dependencies from the acoustic data.\nLSTM has a slight advantage over unidirectional LSTMand\n(c) depth is more important than layer size (which supports\nprevious \ufb01ndings for deep networks [3]). Although the advan-\ntage of the transducer is slight when the weights are randomly\ninitialised, it becomes more substantial when pretraining is\nused.\n5. CONCLUSIONS AND FUTURE WORK\nWe have shown that the combination of deep, bidirectional\nLong Short-term Memory RNNs with end-to-end training and\nweight noise gives state-of-the-art results in phoneme recog-\nnition on the TIMIT database. An obvious next step is to ex-\ntend the system to large vocabulary speech recognition. An-\nother interesting direction would be to combine frequency-\ndomain convolutional neural networks [27] with deep LSTM.\n6. REFERENCES\n[1] H.A. Bourlard and N. Morgan, Connnectionist Speech\nRecognition: A Hybrid Approach,\nKluwer Academic\nPublishers, 1994.\n[2] Qifeng Zhu, Barry Chen, Nelson Morgan, and Andreas\nStolcke, \u201cTandem connectionist feature extraction for\nconversational speech recognition,\u201d\nin International\nConference on Machine Learning for Multimodal Inter-\naction, Berlin, Heidelberg, 2005, MLMI\u201904, pp. 223\u2013\n231, Springer-Verlag.\n[3] A. Mohamed, G.E. Dahl, and G. Hinton,\n\u201cAcoustic\nmodeling using deep belief networks,\u201d Audio, Speech,\nand Language Processing, IEEE Transactions on, vol.\n20, no. 1, pp. 14 \u201322, jan. 2012.\n[4] G. Hinton, Li Deng, Dong Yu, G.E. Dahl, A. Mohamed,\nN. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T.N.\nSainath, and B. Kingsbury, \u201cDeep neural networks for\nacoustic modeling in speech recognition,\u201d Signal Pro-\ncessing Magazine, IEEE, vol. 29, no. 6, pp. 82 \u201397, nov.\n2012.\n[5] A. J. Robinson, \u201cAn Application of Recurrent Nets to\nPhone Probability Estimation,\u201d IEEE Transactions on\nNeural Networks, vol. 5, no. 2, pp. 298\u2013305, 1994.\n[6] Oriol Vinyals, Suman Ravuri, and Daniel Povey, \u201cRe-\nvisiting Recurrent Neural Networks for Robust ASR,\u201d\nin ICASSP, 2012.\n[7] A. Maas, Q. Le, T. O\u2019Neil, O. Vinyals, P. Nguyen, and\nA. Ng, \u201cRecurrent neural networks for noise reduction\nin robust asr,\u201d in INTERSPEECH, 2012.\n[8] A. Graves, S. Fern\u00b4andez, F. Gomez, and J. Schmidhuber,\n\u201cConnectionist Temporal Classi\ufb01cation: Labelling Un-\nsegmented Sequence Data with Recurrent Neural Net-\nworks,\u201d in ICML, Pittsburgh, USA, 2006.\n[9] A. Graves, Supervised sequence labelling with recurrent\nneural networks, vol. 385, Springer, 2012.\n[10] A. Graves, \u201cSequence transduction with recurrent neu-\nral networks,\u201d in ICML Representation Learning Work-\nsop, 2012.\n[11] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term\nMemory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u2013\n1780, 1997.\n[12] A. Graves, S. Fern\u00b4andez, M. Liwicki, H. Bunke, and\nJ. Schmidhuber,\n\u201cUnconstrained Online Handwriting\nRecognition with Recurrent Neural Networks,\u201d in NIPS.\n2008.\n[13] Alex Graves and Juergen Schmidhuber, \u201cOf\ufb02ine Hand-\nwriting Recognition with Multidimensional Recurrent\nNeural Networks,\u201d in NIPS. 2009.\n[14] F. Gers, N. Schraudolph, and J. Schmidhuber, \u201cLearning\nPrecise Timing with LSTM Recurrent Networks,\u201d Jour-\nnal of Machine Learning Research, vol. 3, pp. 115\u2013143,\n2002.\n[15] M. Schuster and K. K. Paliwal, \u201cBidirectional Recur-\nrent Neural Networks,\u201d IEEE Transactions on Signal\nProcessing, vol. 45, pp. 2673\u20132681, 1997.\n[16] A. Graves and J. Schmidhuber, \u201cFramewise Phoneme\nClassi\ufb01cation with Bidirectional LSTM and Other Neu-\nral Network Architectures,\u201d Neural Networks, vol. 18,\nno. 5-6, pp. 602\u2013610, June/July 2005.\n[17] David\nE.\nRumelhart,\nGeoffrey\nE.\nHinton,\nand\nRonald J. Williams, Learning representations by back-\npropagating errors, pp. 696\u2013699, MIT Press, 1988.\n[18] Georey Zweig and Patrick Nguyen, \u201cSCARF: A seg-\nmental CRF speech recognition system,\u201d\nTech. Rep.,\nMicrosoft Research, 2009.\n[19] Andrew W. Senior and Anthony J. Robinson, \u201cForward-\nbackward retraining of recurrent neural networks,\u201d in\nNIPS, 1995, pp. 743\u2013749.\n[20] Abdel rahman Mohamed, Dong Yu, and Li Deng, \u201cIn-\nvestigation of full-sequence training of deep belief net-\nworks for speech recognition,\u201d in in Interspeech, 2010.\n[21] M. Lehr and I. Shafran,\n\u201cDiscriminatively estimated\njoint acoustic, duration, and language model for speech\nrecognition,\u201d in ICASSP, 2010, pp. 5542 \u20135545.\n[22] Kam-Chuen Jim, C.L. Giles, and B.G. Horne, \u201cAn anal-\nysis of noise in recurrent neural networks: convergence\nand generalization,\u201d Neural Networks, IEEE Transac-\ntions on, vol. 7, no. 6, pp. 1424 \u20131438, nov 1996.\n[23] Geoffrey E. Hinton and Drew van Camp, \u201cKeeping the\nneural networks simple by minimizing the description\nlength of the weights,\u201d in COLT, 1993, pp. 5\u201313.\n[24] Alex Graves, \u201cPractical variational inference for neural\nnetworks,\u201d in NIPS, pp. 2348\u20132356. 2011.\n[25] DARPA-ISTO, The DARPA TIMIT Acoustic-Phonetic\nContinuous Speech Corpus (TIMIT), speech disc cd1-\n1.1 edition, 1990.\n[26] Kai fu Lee and Hsiao wuen Hon, \u201cSpeaker-independent\nphone recognition using hidden markov models,\u201d IEEE\nTransactions on Acoustics, Speech, and Signal Process-\ning, 1989.\n[27] O. Abdel-Hamid, A. Mohamed, Hui Jiang, and G. Penn,\n\u201cApplying convolutional neural networks concepts to\nhybrid nn-hmm model for speech recognition,\u201d\nin\nICASSP, march 2012, pp. 4277 \u20134280.\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\nschedule where training initially is conducted with un-transcribed speech and paired speech-text data,\nand unspoken text is utilized only after 20k steps. The joint training employing all three types of data\nlasts for another 100K steps.\nSupervised Training: We use two separate optimizers for the encoder parameters and the decoder\nparameters of the network [71]. For USM-CTC and USM-LAS, we train the model for 100k steps\nwith 2048 batch size. For in-domain experiments, the checkpoint is selected based on development\nset performance.\nTraining Large Models: We use the GShard [72] framework with the GSPMD backend [73] to\ntrain our large models on TPUs.\n3\nDatasets\n3.1\nAudio Data\nFigure 6: The video category and length distribution of YT-513-U.\nThe following audio datasets are used in this report to train our models:\n\u2022 YouTube SUPervised Plus (YT-SUP+):\n\u2013 YT-SUP: 90k hours of segmented, labeled audio across 75 languages.\n\u2013 YT-Pseudo-Labeled: 100k hours of segmented, pseudo-labeled en-US audio from YT-\nNTL-U. The pseudo-labels are generated by a 600M CTC model trained on YT-SUP\nen-US data.\n\u2022 YouTube Next Thousand Languages Unsupervised (YT-NTL-U): 12.1M hours of seg-\nmented, unlabeled audio, including:\n\u2013 YT-55-U: 12M hours of segmented, unlabeled audio on 55 rich resource languages\nidentified by YouTube production language id models.\n\u2013 YT-513-U: 100k hours of segmented, unlabeled audio across 513 tail languages not\ncovered by YouTube production language id models. These languages are identified by\nvendors.\nLet us expand upon how each dataset has been constructed.\nYT-SUP+: YT-SUP is a dataset with audio from videos that have user-uploaded transcripts from 75\nlanguages. We group consecutive segments into a longer unit similar to [57]. The maximal sequence\nlength for training is 30 seconds. The total amount of training data is 90k hours, ranging from English\n(en-US) (3.5k hours) to Amharic (Am-Et) (150 hours). We also introduce an additional 100k hours of\nen-US audio from YT-NTL-U to YT-SUP. We choose to generate pseudo-labels on this dataset using\na 600M-parameter CTC YT teacher model trained on YT-SUP. Each audio is randomly segmented\nbetween 5 to 15 seconds.\nYT-55-U: YT-55-U is built by first randomly collecting 3 million hours of audio from \"speech-heavy\"\nYouTube videos, filtered by language. The 3 million hours of audio is then further segmented by the\nYT teacher model. Instead of using a teacher model as in [3], the non-speech segments identified\nby a Voice Activity Detection (VAD) model are removed to yield approximately 1 million hours of\n10\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\nschedule where training initially is conducted with un-transcribed speech and paired speech-text data,\nand unspoken text is utilized only after 20k steps. The joint training employing all three types of data\nlasts for another 100K steps.\nSupervised Training: We use two separate optimizers for the encoder parameters and the decoder\nparameters of the network [71]. For USM-CTC and USM-LAS, we train the model for 100k steps\nwith 2048 batch size. For in-domain experiments, the checkpoint is selected based on development\nset performance.\nTraining Large Models: We use the GShard [72] framework with the GSPMD backend [73] to\ntrain our large models on TPUs.\n3\nDatasets\n3.1\nAudio Data\nFigure 6: The video category and length distribution of YT-513-U.\nThe following audio datasets are used in this report to train our models:\n\u2022 YouTube SUPervised Plus (YT-SUP+):\n\u2013 YT-SUP: 90k hours of segmented, labeled audio across 75 languages.\n\u2013 YT-Pseudo-Labeled: 100k hours of segmented, pseudo-labeled en-US audio from YT-\nNTL-U. The pseudo-labels are generated by a 600M CTC model trained on YT-SUP\nen-US data.\n\u2022 YouTube Next Thousand Languages Unsupervised (YT-NTL-U): 12.1M hours of seg-\nmented, unlabeled audio, including:\n\u2013 YT-55-U: 12M hours of segmented, unlabeled audio on 55 rich resource languages\nidentified by YouTube production language id models.\n\u2013 YT-513-U: 100k hours of segmented, unlabeled audio across 513 tail languages not\ncovered by YouTube production language id models. These languages are identified by\nvendors.\nLet us expand upon how each dataset has been constructed.\nYT-SUP+: YT-SUP is a dataset with audio from videos that have user-uploaded transcripts from 75\nlanguages. We group consecutive segments into a longer unit similar to [57]. The maximal sequence\nlength for training is 30 seconds. The total amount of training data is 90k hours, ranging from English\n(en-US) (3.5k hours) to Amharic (Am-Et) (150 hours). We also introduce an additional 100k hours of\nen-US audio from YT-NTL-U to YT-SUP. We choose to generate pseudo-labels on this dataset using\na 600M-parameter CTC YT teacher model trained on YT-SUP. Each audio is randomly segmented\nbetween 5 to 15 seconds.\nYT-55-U: YT-55-U is built by first randomly collecting 3 million hours of audio from \"speech-heavy\"\nYouTube videos, filtered by language. The 3 million hours of audio is then further segmented by the\nYT teacher model. Instead of using a teacher model as in [3], the non-speech segments identified\nby a Voice Activity Detection (VAD) model are removed to yield approximately 1 million hours of\n10\nunlabeled audio data. Later, we use a YouTube production language identification model to select 55\nlanguages from that audio.\nYT-513-U: We create an additional dataset called YT-513-U to ensure coverage of lower resource\nlanguages in our pre-training dataset. We reached out to vendors and native speakers to identify YT\nvideos containing speech in specific long tail languages, collecting a dataset of unlabeled speech in\n513 languages. Vendors were tasked with ensuring a variety of domains, voices, and content in the\nvideos that are collected in each language. These videos are segmented into speech segments using a\nVAD model, resulting in a total of 102k hours of speech. Our final YT-513-U dataset contains 88\nlanguages with over 500 hours of speech each, 237 languages with between 100-500 hours, and 188\nlanguages with less than 100 hours of data. The languages chosen for this collection are wide-ranging,\nwith a majority of our data corresponding to languages from South Asia, Southeast Asia, West Africa,\nand East Africa. The distribution of video categories and lengths in our dataset are depicted in\nFigure 6.\nIn addition to YouTube data, we also include public data for MOST training:\n\u2022 Public Unsupervised (Pub-U): Following [20], we use approximately 429k hours of\nunlabeled speech data in 51 languages. It includes: 372k hours of speech data spanning\n23 languages from VoxPopuli [74], read speech data in 25 languages drawn from the v6.1\nrelease of Common Voice [75], 50k hours of read books data in eight European languages\nfrom Multilingual LibriSpeech [76] and 1k hours of telephonic conversation data spanning\n17 African and Asian languages from BABEL [77].\n\u2022 Public Supervised (Pub-S): Similar to [20], our public supervised set includes approxi-\nmately 1.3k hours of speech and transcript data spanning 14 languages from VoxPopuli,\n10 hour training splits for each of the 8 MLS languages, and 1k hours of data spanning 17\nlanguages from the Babel ASR task.\nNote that the public data is only used for in-domain pre-training and is excluded for training the\ngeneric USM-LAS/CTC models. This allows us to treat the public task performance as out-of-domain\nbenchmarks for the USM-LAS/CTC models.\n3.2\nText Data\nWeb-NTL: For pre-training with unlabeled text, we use a web-crawled corpus of monolingual text\ncontaining over 28B sentences [78]. The dataset spans 1140 languages, 205 of which have over 1M\nsentences and 199 of which have between 100k and 1M sentences. We up-sample lower resource\nlanguages using temperature-based sampling [79] with T = 3.0. More details about the dataset and\nthe mining approach have been described in Section 2 of [78].\n3.3\nDownstream Benchmarks\n3.3.1\nSpeech Recognition (ASR)\nWe present our results on two public tasks, SpeechStew [2] and FLEURS [16], and an internal\nbenchmark on YouTube.\nThe SpeechStew [2] dataset is assembled by putting together seven public speech corpora\u2014AMI [80],\nCommon Voice [81], English Broadcast News3, LibriSpeech [82], Switchboard/Fisher4, TED-LIUM\nv3 [83,84] and Wall Street Journal5, which are all standard benchmarks [85\u201387] covering different\ndomains in en-US.\nThe FLEURS [16] dataset is a publicly available, multi-way parallel dataset of 10 hours of read\nspeech in 102 languages spanning 7 geo-groups. We restrict our use of the dataset to its ASR\nbenchmark. Among the 102 languages present in the FLEURS benchmark, we select 62 to serve as a\nsub-group to compare our generic ASR system with Whisper [1], as those languages are covered by\nthe training sets of both models. We also report full results for in-domain fine-tuning and adaptation.\nUnlike [16], we report both WER and CER metrics, as CER is inappropriate as an indicator of\n3Linguistic data consortium (LDC) datasets LDC97S44, LDC97T22, LDC98S71 and LDC98T28.\n4LDC datasets LDC2004T19, LDC2005T19, LDC2004S13, LDC2005S13 and LDC97S62.\n5LDC datasets LDC93S6B and LDC94S13B.\n11\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\nschedule where training initially is conducted with un-transcribed speech and paired speech-text data,\nand unspoken text is utilized only after 20k steps. The joint training employing all three types of data\nlasts for another 100K steps.\nSupervised Training: We use two separate optimizers for the encoder parameters and the decoder\nparameters of the network [71]. For USM-CTC and USM-LAS, we train the model for 100k steps\nwith 2048 batch size. For in-domain experiments, the checkpoint is selected based on development\nset performance.\nTraining Large Models: We use the GShard [72] framework with the GSPMD backend [73] to\ntrain our large models on TPUs.\n3\nDatasets\n3.1\nAudio Data\nFigure 6: The video category and length distribution of YT-513-U.\nThe following audio datasets are used in this report to train our models:\n\u2022 YouTube SUPervised Plus (YT-SUP+):\n\u2013 YT-SUP: 90k hours of segmented, labeled audio across 75 languages.\n\u2013 YT-Pseudo-Labeled: 100k hours of segmented, pseudo-labeled en-US audio from YT-\nNTL-U. The pseudo-labels are generated by a 600M CTC model trained on YT-SUP\nen-US data.\n\u2022 YouTube Next Thousand Languages Unsupervised (YT-NTL-U): 12.1M hours of seg-\nmented, unlabeled audio, including:\n\u2013 YT-55-U: 12M hours of segmented, unlabeled audio on 55 rich resource languages\nidentified by YouTube production language id models.\n\u2013 YT-513-U: 100k hours of segmented, unlabeled audio across 513 tail languages not\ncovered by YouTube production language id models. These languages are identified by\nvendors.\nLet us expand upon how each dataset has been constructed.\nYT-SUP+: YT-SUP is a dataset with audio from videos that have user-uploaded transcripts from 75\nlanguages. We group consecutive segments into a longer unit similar to [57]. The maximal sequence\nlength for training is 30 seconds. The total amount of training data is 90k hours, ranging from English\n(en-US) (3.5k hours) to Amharic (Am-Et) (150 hours). We also introduce an additional 100k hours of\nen-US audio from YT-NTL-U to YT-SUP. We choose to generate pseudo-labels on this dataset using\na 600M-parameter CTC YT teacher model trained on YT-SUP. Each audio is randomly segmented\nbetween 5 to 15 seconds.\nYT-55-U: YT-55-U is built by first randomly collecting 3 million hours of audio from \"speech-heavy\"\nYouTube videos, filtered by language. The 3 million hours of audio is then further segmented by the\nYT teacher model. Instead of using a teacher model as in [3], the non-speech segments identified\nby a Voice Activity Detection (VAD) model are removed to yield approximately 1 million hours of\n10\nunlabeled audio data. Later, we use a YouTube production language identification model to select 55\nlanguages from that audio.\nYT-513-U: We create an additional dataset called YT-513-U to ensure coverage of lower resource\nlanguages in our pre-training dataset. We reached out to vendors and native speakers to identify YT\nvideos containing speech in specific long tail languages, collecting a dataset of unlabeled speech in\n513 languages. Vendors were tasked with ensuring a variety of domains, voices, and content in the\nvideos that are collected in each language. These videos are segmented into speech segments using a\nVAD model, resulting in a total of 102k hours of speech. Our final YT-513-U dataset contains 88\nlanguages with over 500 hours of speech each, 237 languages with between 100-500 hours, and 188\nlanguages with less than 100 hours of data. The languages chosen for this collection are wide-ranging,\nwith a majority of our data corresponding to languages from South Asia, Southeast Asia, West Africa,\nand East Africa. The distribution of video categories and lengths in our dataset are depicted in\nFigure 6.\nIn addition to YouTube data, we also include public data for MOST training:\n\u2022 Public Unsupervised (Pub-U): Following [20], we use approximately 429k hours of\nunlabeled speech data in 51 languages. It includes: 372k hours of speech data spanning\n23 languages from VoxPopuli [74], read speech data in 25 languages drawn from the v6.1\nrelease of Common Voice [75], 50k hours of read books data in eight European languages\nfrom Multilingual LibriSpeech [76] and 1k hours of telephonic conversation data spanning\n17 African and Asian languages from BABEL [77].\n\u2022 Public Supervised (Pub-S): Similar to [20], our public supervised set includes approxi-\nmately 1.3k hours of speech and transcript data spanning 14 languages from VoxPopuli,\n10 hour training splits for each of the 8 MLS languages, and 1k hours of data spanning 17\nlanguages from the Babel ASR task.\nNote that the public data is only used for in-domain pre-training and is excluded for training the\ngeneric USM-LAS/CTC models. This allows us to treat the public task performance as out-of-domain\nbenchmarks for the USM-LAS/CTC models.\n3.2\nText Data\nWeb-NTL: For pre-training with unlabeled text, we use a web-crawled corpus of monolingual text\ncontaining over 28B sentences [78]. The dataset spans 1140 languages, 205 of which have over 1M\nsentences and 199 of which have between 100k and 1M sentences. We up-sample lower resource\nlanguages using temperature-based sampling [79] with T = 3.0. More details about the dataset and\nthe mining approach have been described in Section 2 of [78].\n3.3\nDownstream Benchmarks\n3.3.1\nSpeech Recognition (ASR)\nWe present our results on two public tasks, SpeechStew [2] and FLEURS [16], and an internal\nbenchmark on YouTube.\nThe SpeechStew [2] dataset is assembled by putting together seven public speech corpora\u2014AMI [80],\nCommon Voice [81], English Broadcast News3, LibriSpeech [82], Switchboard/Fisher4, TED-LIUM\nv3 [83,84] and Wall Street Journal5, which are all standard benchmarks [85\u201387] covering different\ndomains in en-US.\nThe FLEURS [16] dataset is a publicly available, multi-way parallel dataset of 10 hours of read\nspeech in 102 languages spanning 7 geo-groups. We restrict our use of the dataset to its ASR\nbenchmark. Among the 102 languages present in the FLEURS benchmark, we select 62 to serve as a\nsub-group to compare our generic ASR system with Whisper [1], as those languages are covered by\nthe training sets of both models. We also report full results for in-domain fine-tuning and adaptation.\nUnlike [16], we report both WER and CER metrics, as CER is inappropriate as an indicator of\n3Linguistic data consortium (LDC) datasets LDC97S44, LDC97T22, LDC98S71 and LDC98T28.\n4LDC datasets LDC2004T19, LDC2005T19, LDC2004S13, LDC2005S13 and LDC97S62.\n5LDC datasets LDC93S6B and LDC94S13B.\n11\nTable 3: WERs (%) across multiple tasks for multiple settings compared against pre-existing baselines,\nwith the exception of CoVoST 2, for which the BLEU score is presented. For the YouTube long-form\nset, we select the top-25 languages Whisper was trained on and exclude all languages for which\nWhisper produces > 40% WER to reduce the noise introduced by LAS hallucination in the Whisper\nmodel. For FLEURS, we report both the WER and the CER for our models. \u2020Results omitted for the\nWhisper-shortform model on the YouTube long-form dataset as the model has a high deletion problem\non this set. \u2021The Whisper-shortform model uses segmented decoding to reduce its hallucination\nproblem on CORAAL. \u00a7Our adapter setup adds about 2.3% of the total parameters while keeping the\nencoder frozen from pre-training.\nTask\nMultilingual Long-form ASR\nMultidomain en-US\nMultilingual ASR\nAST\nDataset\nYouTube\nCORAAL\nSpeechStew\nFLEURS\nCoVoST 2\nLangauges\nen-US\n18\n73\nen-US\nen-US\n62\n102\n21\nPrior Work (single model)\nWhisper-longform\n17.7\n27.8\n-\n23.9\n12.8\nWhisper-shortform\u2020\n-\n-\n-\n13.2\u2021\n11.5\n36.6\n-\n29.1\nOur Work (single model)\nUSM-LAS\n14.4\n19.0\n29.8\n11.2\n10.5\n12.5\n-\n-\nUSM-CTC\n13.7\n18.7\n26.7\n12.1\n10.8\n15.5\n-\n-\nPrior Work (in-domain fine-tuning)\nBigSSL [3]\n14.8\n-\n-\n-\n7.5\n-\n-\n-\nMaestro [67]\n7.2\n25.2\nMaestro-U [67]\n26.0 (8.7)\nOur Work (in-domain fine-tuning)\nUSM\n13.2\n-\n-\n-\n7.4\n13.5\n19.2 (6.9)\n28.7\nUSM-M\n12.5\n-\n-\n-\n7.0\n11.8\n17.4 (6.5)\n30.7\nOur Work (frozen encoder)\nUSM-M-adapter\u00a7\n-\n-\n-\n-\n7.5\n12.4\n17.6 (6.7)\n29.6\nperformance for some languages. When presenting the error rate metrics, we use CER for Chinese,\nJapanese, Thai, Lao, and Burmese to be consistent with Whisper [1].\nThe test set for the YouTube domain consists of utterances from 73 languages with an average of\n15 hours of audio per language, the audio length for each individual language ranging from 1 to 24\nhours. The audio is transcribed manually from popular YouTube videos, each with a duration of up to\n30 minutes.\n3.3.2\nSpeech Translation (AST)\nFollowing [20], we use CoVoST 2 [18] to benchmark multilingual speech translation. We evaluate\nthe multilingual XX-to-English task that covers translation from 21 source languages into English.\nDepending on the language, the training data ranges in size from 1 - 264 hours.\nBesides speech translation data, we also add text-to-text translation data for training the model as\nin [20]. This dataset includes the text translation data from CoVoST 2 combined with all data from\neither WMT or TED Talks, as available.\n4\nKey Results\n4.1\nRobust Speech Recognition for Massively Multilingual Tasks\nIn this section, we compare the performance of our models against public baselines, including\nWhisper large-v26 [1], which has been trained on 680k hours of weakly supervised data across 100\nlanguages.\nFor the massively multilingual speech recognition test dataset from YouTube, we observe that Whisper\nhallucinates in many languages, resulting in a WER exceeding 100%. For a reasonable comparison,\nwe restrict the language set on which we compare the performance USM against Whisper by first\nselecting the top-25 languages from the training data for Whisper and further excluding languages for\nwhich Whisper produces > 40% WER. We also use segmented decoding for Whisper with 30-second\nsegments to further reduce the effect of hallucinations. As shown in Table 3, our USM-LAS and\n6Whisper large-v2 on Github (https://github.com/openai/whisper.git, revision b4308c4) is used for evaluation.\n12\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\nschedule where training initially is conducted with un-transcribed speech and paired speech-text data,\nand unspoken text is utilized only after 20k steps. The joint training employing all three types of data\nlasts for another 100K steps.\nSupervised Training: We use two separate optimizers for the encoder parameters and the decoder\nparameters of the network [71]. For USM-CTC and USM-LAS, we train the model for 100k steps\nwith 2048 batch size. For in-domain experiments, the checkpoint is selected based on development\nset performance.\nTraining Large Models: We use the GShard [72] framework with the GSPMD backend [73] to\ntrain our large models on TPUs.\n3\nDatasets\n3.1\nAudio Data\nFigure 6: The video category and length distribution of YT-513-U.\nThe following audio datasets are used in this report to train our models:\n\u2022 YouTube SUPervised Plus (YT-SUP+):\n\u2013 YT-SUP: 90k hours of segmented, labeled audio across 75 languages.\n\u2013 YT-Pseudo-Labeled: 100k hours of segmented, pseudo-labeled en-US audio from YT-\nNTL-U. The pseudo-labels are generated by a 600M CTC model trained on YT-SUP\nen-US data.\n\u2022 YouTube Next Thousand Languages Unsupervised (YT-NTL-U): 12.1M hours of seg-\nmented, unlabeled audio, including:\n\u2013 YT-55-U: 12M hours of segmented, unlabeled audio on 55 rich resource languages\nidentified by YouTube production language id models.\n\u2013 YT-513-U: 100k hours of segmented, unlabeled audio across 513 tail languages not\ncovered by YouTube production language id models. These languages are identified by\nvendors.\nLet us expand upon how each dataset has been constructed.\nYT-SUP+: YT-SUP is a dataset with audio from videos that have user-uploaded transcripts from 75\nlanguages. We group consecutive segments into a longer unit similar to [57]. The maximal sequence\nlength for training is 30 seconds. The total amount of training data is 90k hours, ranging from English\n(en-US) (3.5k hours) to Amharic (Am-Et) (150 hours). We also introduce an additional 100k hours of\nen-US audio from YT-NTL-U to YT-SUP. We choose to generate pseudo-labels on this dataset using\na 600M-parameter CTC YT teacher model trained on YT-SUP. Each audio is randomly segmented\nbetween 5 to 15 seconds.\nYT-55-U: YT-55-U is built by first randomly collecting 3 million hours of audio from \"speech-heavy\"\nYouTube videos, filtered by language. The 3 million hours of audio is then further segmented by the\nYT teacher model. Instead of using a teacher model as in [3], the non-speech segments identified\nby a Voice Activity Detection (VAD) model are removed to yield approximately 1 million hours of\n10\nunlabeled audio data. Later, we use a YouTube production language identification model to select 55\nlanguages from that audio.\nYT-513-U: We create an additional dataset called YT-513-U to ensure coverage of lower resource\nlanguages in our pre-training dataset. We reached out to vendors and native speakers to identify YT\nvideos containing speech in specific long tail languages, collecting a dataset of unlabeled speech in\n513 languages. Vendors were tasked with ensuring a variety of domains, voices, and content in the\nvideos that are collected in each language. These videos are segmented into speech segments using a\nVAD model, resulting in a total of 102k hours of speech. Our final YT-513-U dataset contains 88\nlanguages with over 500 hours of speech each, 237 languages with between 100-500 hours, and 188\nlanguages with less than 100 hours of data. The languages chosen for this collection are wide-ranging,\nwith a majority of our data corresponding to languages from South Asia, Southeast Asia, West Africa,\nand East Africa. The distribution of video categories and lengths in our dataset are depicted in\nFigure 6.\nIn addition to YouTube data, we also include public data for MOST training:\n\u2022 Public Unsupervised (Pub-U): Following [20], we use approximately 429k hours of\nunlabeled speech data in 51 languages. It includes: 372k hours of speech data spanning\n23 languages from VoxPopuli [74], read speech data in 25 languages drawn from the v6.1\nrelease of Common Voice [75], 50k hours of read books data in eight European languages\nfrom Multilingual LibriSpeech [76] and 1k hours of telephonic conversation data spanning\n17 African and Asian languages from BABEL [77].\n\u2022 Public Supervised (Pub-S): Similar to [20], our public supervised set includes approxi-\nmately 1.3k hours of speech and transcript data spanning 14 languages from VoxPopuli,\n10 hour training splits for each of the 8 MLS languages, and 1k hours of data spanning 17\nlanguages from the Babel ASR task.\nNote that the public data is only used for in-domain pre-training and is excluded for training the\ngeneric USM-LAS/CTC models. This allows us to treat the public task performance as out-of-domain\nbenchmarks for the USM-LAS/CTC models.\n3.2\nText Data\nWeb-NTL: For pre-training with unlabeled text, we use a web-crawled corpus of monolingual text\ncontaining over 28B sentences [78]. The dataset spans 1140 languages, 205 of which have over 1M\nsentences and 199 of which have between 100k and 1M sentences. We up-sample lower resource\nlanguages using temperature-based sampling [79] with T = 3.0. More details about the dataset and\nthe mining approach have been described in Section 2 of [78].\n3.3\nDownstream Benchmarks\n3.3.1\nSpeech Recognition (ASR)\nWe present our results on two public tasks, SpeechStew [2] and FLEURS [16], and an internal\nbenchmark on YouTube.\nThe SpeechStew [2] dataset is assembled by putting together seven public speech corpora\u2014AMI [80],\nCommon Voice [81], English Broadcast News3, LibriSpeech [82], Switchboard/Fisher4, TED-LIUM\nv3 [83,84] and Wall Street Journal5, which are all standard benchmarks [85\u201387] covering different\ndomains in en-US.\nThe FLEURS [16] dataset is a publicly available, multi-way parallel dataset of 10 hours of read\nspeech in 102 languages spanning 7 geo-groups. We restrict our use of the dataset to its ASR\nbenchmark. Among the 102 languages present in the FLEURS benchmark, we select 62 to serve as a\nsub-group to compare our generic ASR system with Whisper [1], as those languages are covered by\nthe training sets of both models. We also report full results for in-domain fine-tuning and adaptation.\nUnlike [16], we report both WER and CER metrics, as CER is inappropriate as an indicator of\n3Linguistic data consortium (LDC) datasets LDC97S44, LDC97T22, LDC98S71 and LDC98T28.\n4LDC datasets LDC2004T19, LDC2005T19, LDC2004S13, LDC2005S13 and LDC97S62.\n5LDC datasets LDC93S6B and LDC94S13B.\n11\nTable 3: WERs (%) across multiple tasks for multiple settings compared against pre-existing baselines,\nwith the exception of CoVoST 2, for which the BLEU score is presented. For the YouTube long-form\nset, we select the top-25 languages Whisper was trained on and exclude all languages for which\nWhisper produces > 40% WER to reduce the noise introduced by LAS hallucination in the Whisper\nmodel. For FLEURS, we report both the WER and the CER for our models. \u2020Results omitted for the\nWhisper-shortform model on the YouTube long-form dataset as the model has a high deletion problem\non this set. \u2021The Whisper-shortform model uses segmented decoding to reduce its hallucination\nproblem on CORAAL. \u00a7Our adapter setup adds about 2.3% of the total parameters while keeping the\nencoder frozen from pre-training.\nTask\nMultilingual Long-form ASR\nMultidomain en-US\nMultilingual ASR\nAST\nDataset\nYouTube\nCORAAL\nSpeechStew\nFLEURS\nCoVoST 2\nLangauges\nen-US\n18\n73\nen-US\nen-US\n62\n102\n21\nPrior Work (single model)\nWhisper-longform\n17.7\n27.8\n-\n23.9\n12.8\nWhisper-shortform\u2020\n-\n-\n-\n13.2\u2021\n11.5\n36.6\n-\n29.1\nOur Work (single model)\nUSM-LAS\n14.4\n19.0\n29.8\n11.2\n10.5\n12.5\n-\n-\nUSM-CTC\n13.7\n18.7\n26.7\n12.1\n10.8\n15.5\n-\n-\nPrior Work (in-domain fine-tuning)\nBigSSL [3]\n14.8\n-\n-\n-\n7.5\n-\n-\n-\nMaestro [67]\n7.2\n25.2\nMaestro-U [67]\n26.0 (8.7)\nOur Work (in-domain fine-tuning)\nUSM\n13.2\n-\n-\n-\n7.4\n13.5\n19.2 (6.9)\n28.7\nUSM-M\n12.5\n-\n-\n-\n7.0\n11.8\n17.4 (6.5)\n30.7\nOur Work (frozen encoder)\nUSM-M-adapter\u00a7\n-\n-\n-\n-\n7.5\n12.4\n17.6 (6.7)\n29.6\nperformance for some languages. When presenting the error rate metrics, we use CER for Chinese,\nJapanese, Thai, Lao, and Burmese to be consistent with Whisper [1].\nThe test set for the YouTube domain consists of utterances from 73 languages with an average of\n15 hours of audio per language, the audio length for each individual language ranging from 1 to 24\nhours. The audio is transcribed manually from popular YouTube videos, each with a duration of up to\n30 minutes.\n3.3.2\nSpeech Translation (AST)\nFollowing [20], we use CoVoST 2 [18] to benchmark multilingual speech translation. We evaluate\nthe multilingual XX-to-English task that covers translation from 21 source languages into English.\nDepending on the language, the training data ranges in size from 1 - 264 hours.\nBesides speech translation data, we also add text-to-text translation data for training the model as\nin [20]. This dataset includes the text translation data from CoVoST 2 combined with all data from\neither WMT or TED Talks, as available.\n4\nKey Results\n4.1\nRobust Speech Recognition for Massively Multilingual Tasks\nIn this section, we compare the performance of our models against public baselines, including\nWhisper large-v26 [1], which has been trained on 680k hours of weakly supervised data across 100\nlanguages.\nFor the massively multilingual speech recognition test dataset from YouTube, we observe that Whisper\nhallucinates in many languages, resulting in a WER exceeding 100%. For a reasonable comparison,\nwe restrict the language set on which we compare the performance USM against Whisper by first\nselecting the top-25 languages from the training data for Whisper and further excluding languages for\nwhich Whisper produces > 40% WER. We also use segmented decoding for Whisper with 30-second\nsegments to further reduce the effect of hallucinations. As shown in Table 3, our USM-LAS and\n6Whisper large-v2 on Github (https://github.com/openai/whisper.git, revision b4308c4) is used for evaluation.\n12\nUSM-CTC models outperform Whisper by a wide margin on YouTube en-US, despite training on\nsignificantly less supervised data (3.5k hours versus Whisper\u2019s 400k hours [1]). While the USM-LAS\nmodel also requires segmented decoding to reduce long-form degradation as discussed in section\n2.4, it is far more robust, out-performing Whisper by a relative 30% WER on those 18 languages.\nUSM-CTC does not exhibit long-form performance degradation and achieves the best performance\non YouTube.\nOn the out-of-domain long-form CORAAL set, both USM-CTC and USM-LAS outperform Whisper\nby more than 10% relative WER. USM-CTC and USM-LAS similary outperform Whisper on\nSpeechStew, whose training data the models have not had access to.\nWe further compare the multilingual performance of the models on the held-out set from FLEURS.\nAs shown in Table 3, USM-LAS and USM-CTC both outperform Whisper by 66% relative WER,\ndespite using a smaller amount of multilingual supervised data (90k versus Whisper\u2019s 117k, when\nen-US is excluded). USM-LAS consistently outperforms USM-CTC for short-form ASR tasks.\n4.2\nMassively Multilingual Results Beyond 100 Languages\nThe lower part of Table 3 shows our results for in-domain fine-tuning. Our pre-trained model improves\nthe FLEURS benchmark significantly, even when using only 10 hours per language. Compared to the\nprevious SoTA in [67], our model achieves a 30% relative improvement in terms of WER across 102\nlanguages. Our results show that while generic speech models can be powerful, performance is still\nmaximized by in-domain fine-tuning.\n4.3\nMOST Produces Robust Representations that Generalize to New Domains\nMOST training aligns the representations of speech and text by training simultaneously on the two\nmodalities. We investigate whether MOST representations are useful for adapting the model to new\ndomains by freezing the entire learned encoder produced by MOST and adjusting a small amount of\nparameters added to the network by residual adapters. As shown in Table 3, by adding only 2% to\nthe total number of parameters, the MOST representation model (USM-M-adapter) only performs\nslightly worse than the fine-tuning baselines, still showing competitive performance on downstream\nASR and AST tasks. The small number of parameters being trained in this approach makes it feasible\nto extend our system to a large number of new domains and new tasks, even with a limited amount of\ntraining data, such as in FLEURS.\n4.4\nPushing the Quality of ASR on Unseen Languages\nTable 4: Noisy student training for unseen languages. WERs (%) for the teacher adapter models and\nthe student models are presented. The relative improvement (%) of the student models can be found\nin the last column.\nLanguages\nWhisper-v2\n# hrs in YT-NTL\nUSM-LAS-Adapter\nUSM-M + pseudo label\nRel. Imprv.\nHausa (ha)\n88.9\n2175.0\n24.5\n22.8\n7.5\nKazakh (kk)\n37.7\n196.0\n11.8\n10.9\n8.3\nShona (sn)\n121.0\n247.0\n29.1\n22.2\n31.1\nPashto (ps)\n93.7\n254.0\n36.0\n35.4\n1.7\nYoruba (yo)\n94.8\n1292.0\n33.4\n30.6\n9.2\nTail languages often do not have paired transcriptions for supervised learning\u2014we refer to these\nlanguages as unseen languages, as the model has not seen paired data for these lanugages during\ntraining. To create pseudo-labels for these languages, we first build a USM-LAS-Adapter by attaching\nresidual adapters to USM-LAS and training them using FLEURS data. By using the USM-LAS-\nAdapter as a teacher, we can now transcribe the unlabeled data in the unseen languages as part\nof the YT-NTL dataset. As shown in Table 4, we observe consistent wins for all languages on\nthe FLEURS benchmark. For some languages, the improvement is larger than 30%. This further\ndemonstrates the robustness of the USM-LAS model\u2014despite using only 10 hours of out of domain\ndata from FLEURS, the USM-LAS-Adapter is able to transcribe YouTube data to produce meaningful\nrecognition results that lead to these improvements. We find the approach of training adapter models\n13\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\nschedule where training initially is conducted with un-transcribed speech and paired speech-text data,\nand unspoken text is utilized only after 20k steps. The joint training employing all three types of data\nlasts for another 100K steps.\nSupervised Training: We use two separate optimizers for the encoder parameters and the decoder\nparameters of the network [71]. For USM-CTC and USM-LAS, we train the model for 100k steps\nwith 2048 batch size. For in-domain experiments, the checkpoint is selected based on development\nset performance.\nTraining Large Models: We use the GShard [72] framework with the GSPMD backend [73] to\ntrain our large models on TPUs.\n3\nDatasets\n3.1\nAudio Data\nFigure 6: The video category and length distribution of YT-513-U.\nThe following audio datasets are used in this report to train our models:\n\u2022 YouTube SUPervised Plus (YT-SUP+):\n\u2013 YT-SUP: 90k hours of segmented, labeled audio across 75 languages.\n\u2013 YT-Pseudo-Labeled: 100k hours of segmented, pseudo-labeled en-US audio from YT-\nNTL-U. The pseudo-labels are generated by a 600M CTC model trained on YT-SUP\nen-US data.\n\u2022 YouTube Next Thousand Languages Unsupervised (YT-NTL-U): 12.1M hours of seg-\nmented, unlabeled audio, including:\n\u2013 YT-55-U: 12M hours of segmented, unlabeled audio on 55 rich resource languages\nidentified by YouTube production language id models.\n\u2013 YT-513-U: 100k hours of segmented, unlabeled audio across 513 tail languages not\ncovered by YouTube production language id models. These languages are identified by\nvendors.\nLet us expand upon how each dataset has been constructed.\nYT-SUP+: YT-SUP is a dataset with audio from videos that have user-uploaded transcripts from 75\nlanguages. We group consecutive segments into a longer unit similar to [57]. The maximal sequence\nlength for training is 30 seconds. The total amount of training data is 90k hours, ranging from English\n(en-US) (3.5k hours) to Amharic (Am-Et) (150 hours). We also introduce an additional 100k hours of\nen-US audio from YT-NTL-U to YT-SUP. We choose to generate pseudo-labels on this dataset using\na 600M-parameter CTC YT teacher model trained on YT-SUP. Each audio is randomly segmented\nbetween 5 to 15 seconds.\nYT-55-U: YT-55-U is built by first randomly collecting 3 million hours of audio from \"speech-heavy\"\nYouTube videos, filtered by language. The 3 million hours of audio is then further segmented by the\nYT teacher model. Instead of using a teacher model as in [3], the non-speech segments identified\nby a Voice Activity Detection (VAD) model are removed to yield approximately 1 million hours of\n10\nunlabeled audio data. Later, we use a YouTube production language identification model to select 55\nlanguages from that audio.\nYT-513-U: We create an additional dataset called YT-513-U to ensure coverage of lower resource\nlanguages in our pre-training dataset. We reached out to vendors and native speakers to identify YT\nvideos containing speech in specific long tail languages, collecting a dataset of unlabeled speech in\n513 languages. Vendors were tasked with ensuring a variety of domains, voices, and content in the\nvideos that are collected in each language. These videos are segmented into speech segments using a\nVAD model, resulting in a total of 102k hours of speech. Our final YT-513-U dataset contains 88\nlanguages with over 500 hours of speech each, 237 languages with between 100-500 hours, and 188\nlanguages with less than 100 hours of data. The languages chosen for this collection are wide-ranging,\nwith a majority of our data corresponding to languages from South Asia, Southeast Asia, West Africa,\nand East Africa. The distribution of video categories and lengths in our dataset are depicted in\nFigure 6.\nIn addition to YouTube data, we also include public data for MOST training:\n\u2022 Public Unsupervised (Pub-U): Following [20], we use approximately 429k hours of\nunlabeled speech data in 51 languages. It includes: 372k hours of speech data spanning\n23 languages from VoxPopuli [74], read speech data in 25 languages drawn from the v6.1\nrelease of Common Voice [75], 50k hours of read books data in eight European languages\nfrom Multilingual LibriSpeech [76] and 1k hours of telephonic conversation data spanning\n17 African and Asian languages from BABEL [77].\n\u2022 Public Supervised (Pub-S): Similar to [20], our public supervised set includes approxi-\nmately 1.3k hours of speech and transcript data spanning 14 languages from VoxPopuli,\n10 hour training splits for each of the 8 MLS languages, and 1k hours of data spanning 17\nlanguages from the Babel ASR task.\nNote that the public data is only used for in-domain pre-training and is excluded for training the\ngeneric USM-LAS/CTC models. This allows us to treat the public task performance as out-of-domain\nbenchmarks for the USM-LAS/CTC models.\n3.2\nText Data\nWeb-NTL: For pre-training with unlabeled text, we use a web-crawled corpus of monolingual text\ncontaining over 28B sentences [78]. The dataset spans 1140 languages, 205 of which have over 1M\nsentences and 199 of which have between 100k and 1M sentences. We up-sample lower resource\nlanguages using temperature-based sampling [79] with T = 3.0. More details about the dataset and\nthe mining approach have been described in Section 2 of [78].\n3.3\nDownstream Benchmarks\n3.3.1\nSpeech Recognition (ASR)\nWe present our results on two public tasks, SpeechStew [2] and FLEURS [16], and an internal\nbenchmark on YouTube.\nThe SpeechStew [2] dataset is assembled by putting together seven public speech corpora\u2014AMI [80],\nCommon Voice [81], English Broadcast News3, LibriSpeech [82], Switchboard/Fisher4, TED-LIUM\nv3 [83,84] and Wall Street Journal5, which are all standard benchmarks [85\u201387] covering different\ndomains in en-US.\nThe FLEURS [16] dataset is a publicly available, multi-way parallel dataset of 10 hours of read\nspeech in 102 languages spanning 7 geo-groups. We restrict our use of the dataset to its ASR\nbenchmark. Among the 102 languages present in the FLEURS benchmark, we select 62 to serve as a\nsub-group to compare our generic ASR system with Whisper [1], as those languages are covered by\nthe training sets of both models. We also report full results for in-domain fine-tuning and adaptation.\nUnlike [16], we report both WER and CER metrics, as CER is inappropriate as an indicator of\n3Linguistic data consortium (LDC) datasets LDC97S44, LDC97T22, LDC98S71 and LDC98T28.\n4LDC datasets LDC2004T19, LDC2005T19, LDC2004S13, LDC2005S13 and LDC97S62.\n5LDC datasets LDC93S6B and LDC94S13B.\n11\nTable 3: WERs (%) across multiple tasks for multiple settings compared against pre-existing baselines,\nwith the exception of CoVoST 2, for which the BLEU score is presented. For the YouTube long-form\nset, we select the top-25 languages Whisper was trained on and exclude all languages for which\nWhisper produces > 40% WER to reduce the noise introduced by LAS hallucination in the Whisper\nmodel. For FLEURS, we report both the WER and the CER for our models. \u2020Results omitted for the\nWhisper-shortform model on the YouTube long-form dataset as the model has a high deletion problem\non this set. \u2021The Whisper-shortform model uses segmented decoding to reduce its hallucination\nproblem on CORAAL. \u00a7Our adapter setup adds about 2.3% of the total parameters while keeping the\nencoder frozen from pre-training.\nTask\nMultilingual Long-form ASR\nMultidomain en-US\nMultilingual ASR\nAST\nDataset\nYouTube\nCORAAL\nSpeechStew\nFLEURS\nCoVoST 2\nLangauges\nen-US\n18\n73\nen-US\nen-US\n62\n102\n21\nPrior Work (single model)\nWhisper-longform\n17.7\n27.8\n-\n23.9\n12.8\nWhisper-shortform\u2020\n-\n-\n-\n13.2\u2021\n11.5\n36.6\n-\n29.1\nOur Work (single model)\nUSM-LAS\n14.4\n19.0\n29.8\n11.2\n10.5\n12.5\n-\n-\nUSM-CTC\n13.7\n18.7\n26.7\n12.1\n10.8\n15.5\n-\n-\nPrior Work (in-domain fine-tuning)\nBigSSL [3]\n14.8\n-\n-\n-\n7.5\n-\n-\n-\nMaestro [67]\n7.2\n25.2\nMaestro-U [67]\n26.0 (8.7)\nOur Work (in-domain fine-tuning)\nUSM\n13.2\n-\n-\n-\n7.4\n13.5\n19.2 (6.9)\n28.7\nUSM-M\n12.5\n-\n-\n-\n7.0\n11.8\n17.4 (6.5)\n30.7\nOur Work (frozen encoder)\nUSM-M-adapter\u00a7\n-\n-\n-\n-\n7.5\n12.4\n17.6 (6.7)\n29.6\nperformance for some languages. When presenting the error rate metrics, we use CER for Chinese,\nJapanese, Thai, Lao, and Burmese to be consistent with Whisper [1].\nThe test set for the YouTube domain consists of utterances from 73 languages with an average of\n15 hours of audio per language, the audio length for each individual language ranging from 1 to 24\nhours. The audio is transcribed manually from popular YouTube videos, each with a duration of up to\n30 minutes.\n3.3.2\nSpeech Translation (AST)\nFollowing [20], we use CoVoST 2 [18] to benchmark multilingual speech translation. We evaluate\nthe multilingual XX-to-English task that covers translation from 21 source languages into English.\nDepending on the language, the training data ranges in size from 1 - 264 hours.\nBesides speech translation data, we also add text-to-text translation data for training the model as\nin [20]. This dataset includes the text translation data from CoVoST 2 combined with all data from\neither WMT or TED Talks, as available.\n4\nKey Results\n4.1\nRobust Speech Recognition for Massively Multilingual Tasks\nIn this section, we compare the performance of our models against public baselines, including\nWhisper large-v26 [1], which has been trained on 680k hours of weakly supervised data across 100\nlanguages.\nFor the massively multilingual speech recognition test dataset from YouTube, we observe that Whisper\nhallucinates in many languages, resulting in a WER exceeding 100%. For a reasonable comparison,\nwe restrict the language set on which we compare the performance USM against Whisper by first\nselecting the top-25 languages from the training data for Whisper and further excluding languages for\nwhich Whisper produces > 40% WER. We also use segmented decoding for Whisper with 30-second\nsegments to further reduce the effect of hallucinations. As shown in Table 3, our USM-LAS and\n6Whisper large-v2 on Github (https://github.com/openai/whisper.git, revision b4308c4) is used for evaluation.\n12\nUSM-CTC models outperform Whisper by a wide margin on YouTube en-US, despite training on\nsignificantly less supervised data (3.5k hours versus Whisper\u2019s 400k hours [1]). While the USM-LAS\nmodel also requires segmented decoding to reduce long-form degradation as discussed in section\n2.4, it is far more robust, out-performing Whisper by a relative 30% WER on those 18 languages.\nUSM-CTC does not exhibit long-form performance degradation and achieves the best performance\non YouTube.\nOn the out-of-domain long-form CORAAL set, both USM-CTC and USM-LAS outperform Whisper\nby more than 10% relative WER. USM-CTC and USM-LAS similary outperform Whisper on\nSpeechStew, whose training data the models have not had access to.\nWe further compare the multilingual performance of the models on the held-out set from FLEURS.\nAs shown in Table 3, USM-LAS and USM-CTC both outperform Whisper by 66% relative WER,\ndespite using a smaller amount of multilingual supervised data (90k versus Whisper\u2019s 117k, when\nen-US is excluded). USM-LAS consistently outperforms USM-CTC for short-form ASR tasks.\n4.2\nMassively Multilingual Results Beyond 100 Languages\nThe lower part of Table 3 shows our results for in-domain fine-tuning. Our pre-trained model improves\nthe FLEURS benchmark significantly, even when using only 10 hours per language. Compared to the\nprevious SoTA in [67], our model achieves a 30% relative improvement in terms of WER across 102\nlanguages. Our results show that while generic speech models can be powerful, performance is still\nmaximized by in-domain fine-tuning.\n4.3\nMOST Produces Robust Representations that Generalize to New Domains\nMOST training aligns the representations of speech and text by training simultaneously on the two\nmodalities. We investigate whether MOST representations are useful for adapting the model to new\ndomains by freezing the entire learned encoder produced by MOST and adjusting a small amount of\nparameters added to the network by residual adapters. As shown in Table 3, by adding only 2% to\nthe total number of parameters, the MOST representation model (USM-M-adapter) only performs\nslightly worse than the fine-tuning baselines, still showing competitive performance on downstream\nASR and AST tasks. The small number of parameters being trained in this approach makes it feasible\nto extend our system to a large number of new domains and new tasks, even with a limited amount of\ntraining data, such as in FLEURS.\n4.4\nPushing the Quality of ASR on Unseen Languages\nTable 4: Noisy student training for unseen languages. WERs (%) for the teacher adapter models and\nthe student models are presented. The relative improvement (%) of the student models can be found\nin the last column.\nLanguages\nWhisper-v2\n# hrs in YT-NTL\nUSM-LAS-Adapter\nUSM-M + pseudo label\nRel. Imprv.\nHausa (ha)\n88.9\n2175.0\n24.5\n22.8\n7.5\nKazakh (kk)\n37.7\n196.0\n11.8\n10.9\n8.3\nShona (sn)\n121.0\n247.0\n29.1\n22.2\n31.1\nPashto (ps)\n93.7\n254.0\n36.0\n35.4\n1.7\nYoruba (yo)\n94.8\n1292.0\n33.4\n30.6\n9.2\nTail languages often do not have paired transcriptions for supervised learning\u2014we refer to these\nlanguages as unseen languages, as the model has not seen paired data for these lanugages during\ntraining. To create pseudo-labels for these languages, we first build a USM-LAS-Adapter by attaching\nresidual adapters to USM-LAS and training them using FLEURS data. By using the USM-LAS-\nAdapter as a teacher, we can now transcribe the unlabeled data in the unseen languages as part\nof the YT-NTL dataset. As shown in Table 4, we observe consistent wins for all languages on\nthe FLEURS benchmark. For some languages, the improvement is larger than 30%. This further\ndemonstrates the robustness of the USM-LAS model\u2014despite using only 10 hours of out of domain\ndata from FLEURS, the USM-LAS-Adapter is able to transcribe YouTube data to produce meaningful\nrecognition results that lead to these improvements. We find the approach of training adapter models\n13\non small datasets and utilizing them for pseudo-labeling to be a promising route for scaling up the\nnumber of languages that can be transcribed by USMs.\n4.5\nUSMs are Strong AST Models\nThe multi-lingual speech translation performance of fine-tuned USMs are shown in Table 3. We\nfind that we are already comparable to the CoVoST 2 SoTA BLEU score by fine-tuning the speech-\nonly USM. We note that the previous SoTA uses 125k hours of supervised speech translation data\ncompared to the 859 hours of data used by the USM. After MOST training, USM-M can use both\nspeech and text as training input. By introducing text-to-text machine translation (MT) data during\nfine-tuning, USM-M is able to achieve an unprecedented > 30 BLEU on CoVoST (a 1 BLEU increase\nfrom SoTA).\n5\nAnalysis and Ablations\n5.1\nMulti-Softmax Loss for BEST-RQ\nWe observe a consistent > 5% relative improvement in ASR and AST benchmarks by increasing\nthe number of the softmax groups in the multi-softmax loss for BEST-RQ training from 1 to 16, as\nshown in Table 5. We also find that using multiple softmax groups significantly reduces performance\nvariation across different pre-training runs and improves convergence speed.\nTable 5: YT-55 versus YT-NTL across different domains, with and without multi-softmax groups. For\nsimplicity, we report CER for FLEURS. For CoVoST, we report the BLEU score. YT-NTL covers 27\nadditional languages not covered in YT-55.\nModel\npre-train Set\n# Params (B)\n# Softmax\nFLEURS (CER)\nCoVoST (BLEU)\n.\n102 langs\n27 langs\nConformer-0.6B\nYT-55\n0.6\n1\n9.5\n-\n20.9\nConformer-2B\nYT-55\n2.0\n1\n7.9\n9.5\n26.6\nConformer-2B\nYT-NTL-U\n2.0\n1\n7.4\n8.5\n27.5\nConformer-2B\nYT-NTL-U\n2.0\n16\n6.9\n8.1\n28.7\n5.2\nModel and Language Scaling\nWe find that scaling up the model size and increasing the language coverage of the pre-training\ndataset greatly benefits the performance of the USMs, as demonstrated in Table 5. In particular, we\nfind a 10% relative improvement of ASR and AST performance by using YT-NTL vs. YT-55 for\npre-training, despite the fact that each newly added language in YT-NTL contains approximately 500\nhours of speech\u2014a relatively small amount. As could be expected, the relative gains on the newly\ncovered languages are more substantial than those on other languages.\n5.3\nBEST-RQ is a Scalable Self-supervised Learner\nBEST-RQ has been shown to outperform or be comparable to other prominent pre-training methods\nfor speech recognition, including wav2vec 2.0 and W2v-BERT in the original work in which it was\nintroduced [10]. Here we investigate its comparative performance and scaling properties, similar\nto what has been done for wav2vec 2.0 in [3] and W2v-BERT in [20]. We utilize the set-up of\npre-training the model using YT-55 and fine-tuning it on CoVoST 2. As shown in Table 6, our results\nindicate that for the Conformer-0.6B, W2v-BERT and BEST-RQ perform similarly, but BEST-RQ\nobtains greater gains when scaled up. A contributing factor to this can be that W2v-BERT is more\nprone to codebook collapse and training instabilities at the 2B scale, while BEST-RQ by construction\ndoesn\u2019t suffer from codebook collapse.\n5.4\nChunk-wise attention for robust long-form speech recognition\nFig. 7 depicts the long-form performance degradation issue as described in section 2.4. In the figure,\nwe see that for the shallow Conformer model with 17 layers, using a small local self attention context\n14\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\nschedule where training initially is conducted with un-transcribed speech and paired speech-text data,\nand unspoken text is utilized only after 20k steps. The joint training employing all three types of data\nlasts for another 100K steps.\nSupervised Training: We use two separate optimizers for the encoder parameters and the decoder\nparameters of the network [71]. For USM-CTC and USM-LAS, we train the model for 100k steps\nwith 2048 batch size. For in-domain experiments, the checkpoint is selected based on development\nset performance.\nTraining Large Models: We use the GShard [72] framework with the GSPMD backend [73] to\ntrain our large models on TPUs.\n3\nDatasets\n3.1\nAudio Data\nFigure 6: The video category and length distribution of YT-513-U.\nThe following audio datasets are used in this report to train our models:\n\u2022 YouTube SUPervised Plus (YT-SUP+):\n\u2013 YT-SUP: 90k hours of segmented, labeled audio across 75 languages.\n\u2013 YT-Pseudo-Labeled: 100k hours of segmented, pseudo-labeled en-US audio from YT-\nNTL-U. The pseudo-labels are generated by a 600M CTC model trained on YT-SUP\nen-US data.\n\u2022 YouTube Next Thousand Languages Unsupervised (YT-NTL-U): 12.1M hours of seg-\nmented, unlabeled audio, including:\n\u2013 YT-55-U: 12M hours of segmented, unlabeled audio on 55 rich resource languages\nidentified by YouTube production language id models.\n\u2013 YT-513-U: 100k hours of segmented, unlabeled audio across 513 tail languages not\ncovered by YouTube production language id models. These languages are identified by\nvendors.\nLet us expand upon how each dataset has been constructed.\nYT-SUP+: YT-SUP is a dataset with audio from videos that have user-uploaded transcripts from 75\nlanguages. We group consecutive segments into a longer unit similar to [57]. The maximal sequence\nlength for training is 30 seconds. The total amount of training data is 90k hours, ranging from English\n(en-US) (3.5k hours) to Amharic (Am-Et) (150 hours). We also introduce an additional 100k hours of\nen-US audio from YT-NTL-U to YT-SUP. We choose to generate pseudo-labels on this dataset using\na 600M-parameter CTC YT teacher model trained on YT-SUP. Each audio is randomly segmented\nbetween 5 to 15 seconds.\nYT-55-U: YT-55-U is built by first randomly collecting 3 million hours of audio from \"speech-heavy\"\nYouTube videos, filtered by language. The 3 million hours of audio is then further segmented by the\nYT teacher model. Instead of using a teacher model as in [3], the non-speech segments identified\nby a Voice Activity Detection (VAD) model are removed to yield approximately 1 million hours of\n10\nunlabeled audio data. Later, we use a YouTube production language identification model to select 55\nlanguages from that audio.\nYT-513-U: We create an additional dataset called YT-513-U to ensure coverage of lower resource\nlanguages in our pre-training dataset. We reached out to vendors and native speakers to identify YT\nvideos containing speech in specific long tail languages, collecting a dataset of unlabeled speech in\n513 languages. Vendors were tasked with ensuring a variety of domains, voices, and content in the\nvideos that are collected in each language. These videos are segmented into speech segments using a\nVAD model, resulting in a total of 102k hours of speech. Our final YT-513-U dataset contains 88\nlanguages with over 500 hours of speech each, 237 languages with between 100-500 hours, and 188\nlanguages with less than 100 hours of data. The languages chosen for this collection are wide-ranging,\nwith a majority of our data corresponding to languages from South Asia, Southeast Asia, West Africa,\nand East Africa. The distribution of video categories and lengths in our dataset are depicted in\nFigure 6.\nIn addition to YouTube data, we also include public data for MOST training:\n\u2022 Public Unsupervised (Pub-U): Following [20], we use approximately 429k hours of\nunlabeled speech data in 51 languages. It includes: 372k hours of speech data spanning\n23 languages from VoxPopuli [74], read speech data in 25 languages drawn from the v6.1\nrelease of Common Voice [75], 50k hours of read books data in eight European languages\nfrom Multilingual LibriSpeech [76] and 1k hours of telephonic conversation data spanning\n17 African and Asian languages from BABEL [77].\n\u2022 Public Supervised (Pub-S): Similar to [20], our public supervised set includes approxi-\nmately 1.3k hours of speech and transcript data spanning 14 languages from VoxPopuli,\n10 hour training splits for each of the 8 MLS languages, and 1k hours of data spanning 17\nlanguages from the Babel ASR task.\nNote that the public data is only used for in-domain pre-training and is excluded for training the\ngeneric USM-LAS/CTC models. This allows us to treat the public task performance as out-of-domain\nbenchmarks for the USM-LAS/CTC models.\n3.2\nText Data\nWeb-NTL: For pre-training with unlabeled text, we use a web-crawled corpus of monolingual text\ncontaining over 28B sentences [78]. The dataset spans 1140 languages, 205 of which have over 1M\nsentences and 199 of which have between 100k and 1M sentences. We up-sample lower resource\nlanguages using temperature-based sampling [79] with T = 3.0. More details about the dataset and\nthe mining approach have been described in Section 2 of [78].\n3.3\nDownstream Benchmarks\n3.3.1\nSpeech Recognition (ASR)\nWe present our results on two public tasks, SpeechStew [2] and FLEURS [16], and an internal\nbenchmark on YouTube.\nThe SpeechStew [2] dataset is assembled by putting together seven public speech corpora\u2014AMI [80],\nCommon Voice [81], English Broadcast News3, LibriSpeech [82], Switchboard/Fisher4, TED-LIUM\nv3 [83,84] and Wall Street Journal5, which are all standard benchmarks [85\u201387] covering different\ndomains in en-US.\nThe FLEURS [16] dataset is a publicly available, multi-way parallel dataset of 10 hours of read\nspeech in 102 languages spanning 7 geo-groups. We restrict our use of the dataset to its ASR\nbenchmark. Among the 102 languages present in the FLEURS benchmark, we select 62 to serve as a\nsub-group to compare our generic ASR system with Whisper [1], as those languages are covered by\nthe training sets of both models. We also report full results for in-domain fine-tuning and adaptation.\nUnlike [16], we report both WER and CER metrics, as CER is inappropriate as an indicator of\n3Linguistic data consortium (LDC) datasets LDC97S44, LDC97T22, LDC98S71 and LDC98T28.\n4LDC datasets LDC2004T19, LDC2005T19, LDC2004S13, LDC2005S13 and LDC97S62.\n5LDC datasets LDC93S6B and LDC94S13B.\n11\nTable 3: WERs (%) across multiple tasks for multiple settings compared against pre-existing baselines,\nwith the exception of CoVoST 2, for which the BLEU score is presented. For the YouTube long-form\nset, we select the top-25 languages Whisper was trained on and exclude all languages for which\nWhisper produces > 40% WER to reduce the noise introduced by LAS hallucination in the Whisper\nmodel. For FLEURS, we report both the WER and the CER for our models. \u2020Results omitted for the\nWhisper-shortform model on the YouTube long-form dataset as the model has a high deletion problem\non this set. \u2021The Whisper-shortform model uses segmented decoding to reduce its hallucination\nproblem on CORAAL. \u00a7Our adapter setup adds about 2.3% of the total parameters while keeping the\nencoder frozen from pre-training.\nTask\nMultilingual Long-form ASR\nMultidomain en-US\nMultilingual ASR\nAST\nDataset\nYouTube\nCORAAL\nSpeechStew\nFLEURS\nCoVoST 2\nLangauges\nen-US\n18\n73\nen-US\nen-US\n62\n102\n21\nPrior Work (single model)\nWhisper-longform\n17.7\n27.8\n-\n23.9\n12.8\nWhisper-shortform\u2020\n-\n-\n-\n13.2\u2021\n11.5\n36.6\n-\n29.1\nOur Work (single model)\nUSM-LAS\n14.4\n19.0\n29.8\n11.2\n10.5\n12.5\n-\n-\nUSM-CTC\n13.7\n18.7\n26.7\n12.1\n10.8\n15.5\n-\n-\nPrior Work (in-domain fine-tuning)\nBigSSL [3]\n14.8\n-\n-\n-\n7.5\n-\n-\n-\nMaestro [67]\n7.2\n25.2\nMaestro-U [67]\n26.0 (8.7)\nOur Work (in-domain fine-tuning)\nUSM\n13.2\n-\n-\n-\n7.4\n13.5\n19.2 (6.9)\n28.7\nUSM-M\n12.5\n-\n-\n-\n7.0\n11.8\n17.4 (6.5)\n30.7\nOur Work (frozen encoder)\nUSM-M-adapter\u00a7\n-\n-\n-\n-\n7.5\n12.4\n17.6 (6.7)\n29.6\nperformance for some languages. When presenting the error rate metrics, we use CER for Chinese,\nJapanese, Thai, Lao, and Burmese to be consistent with Whisper [1].\nThe test set for the YouTube domain consists of utterances from 73 languages with an average of\n15 hours of audio per language, the audio length for each individual language ranging from 1 to 24\nhours. The audio is transcribed manually from popular YouTube videos, each with a duration of up to\n30 minutes.\n3.3.2\nSpeech Translation (AST)\nFollowing [20], we use CoVoST 2 [18] to benchmark multilingual speech translation. We evaluate\nthe multilingual XX-to-English task that covers translation from 21 source languages into English.\nDepending on the language, the training data ranges in size from 1 - 264 hours.\nBesides speech translation data, we also add text-to-text translation data for training the model as\nin [20]. This dataset includes the text translation data from CoVoST 2 combined with all data from\neither WMT or TED Talks, as available.\n4\nKey Results\n4.1\nRobust Speech Recognition for Massively Multilingual Tasks\nIn this section, we compare the performance of our models against public baselines, including\nWhisper large-v26 [1], which has been trained on 680k hours of weakly supervised data across 100\nlanguages.\nFor the massively multilingual speech recognition test dataset from YouTube, we observe that Whisper\nhallucinates in many languages, resulting in a WER exceeding 100%. For a reasonable comparison,\nwe restrict the language set on which we compare the performance USM against Whisper by first\nselecting the top-25 languages from the training data for Whisper and further excluding languages for\nwhich Whisper produces > 40% WER. We also use segmented decoding for Whisper with 30-second\nsegments to further reduce the effect of hallucinations. As shown in Table 3, our USM-LAS and\n6Whisper large-v2 on Github (https://github.com/openai/whisper.git, revision b4308c4) is used for evaluation.\n12\nUSM-CTC models outperform Whisper by a wide margin on YouTube en-US, despite training on\nsignificantly less supervised data (3.5k hours versus Whisper\u2019s 400k hours [1]). While the USM-LAS\nmodel also requires segmented decoding to reduce long-form degradation as discussed in section\n2.4, it is far more robust, out-performing Whisper by a relative 30% WER on those 18 languages.\nUSM-CTC does not exhibit long-form performance degradation and achieves the best performance\non YouTube.\nOn the out-of-domain long-form CORAAL set, both USM-CTC and USM-LAS outperform Whisper\nby more than 10% relative WER. USM-CTC and USM-LAS similary outperform Whisper on\nSpeechStew, whose training data the models have not had access to.\nWe further compare the multilingual performance of the models on the held-out set from FLEURS.\nAs shown in Table 3, USM-LAS and USM-CTC both outperform Whisper by 66% relative WER,\ndespite using a smaller amount of multilingual supervised data (90k versus Whisper\u2019s 117k, when\nen-US is excluded). USM-LAS consistently outperforms USM-CTC for short-form ASR tasks.\n4.2\nMassively Multilingual Results Beyond 100 Languages\nThe lower part of Table 3 shows our results for in-domain fine-tuning. Our pre-trained model improves\nthe FLEURS benchmark significantly, even when using only 10 hours per language. Compared to the\nprevious SoTA in [67], our model achieves a 30% relative improvement in terms of WER across 102\nlanguages. Our results show that while generic speech models can be powerful, performance is still\nmaximized by in-domain fine-tuning.\n4.3\nMOST Produces Robust Representations that Generalize to New Domains\nMOST training aligns the representations of speech and text by training simultaneously on the two\nmodalities. We investigate whether MOST representations are useful for adapting the model to new\ndomains by freezing the entire learned encoder produced by MOST and adjusting a small amount of\nparameters added to the network by residual adapters. As shown in Table 3, by adding only 2% to\nthe total number of parameters, the MOST representation model (USM-M-adapter) only performs\nslightly worse than the fine-tuning baselines, still showing competitive performance on downstream\nASR and AST tasks. The small number of parameters being trained in this approach makes it feasible\nto extend our system to a large number of new domains and new tasks, even with a limited amount of\ntraining data, such as in FLEURS.\n4.4\nPushing the Quality of ASR on Unseen Languages\nTable 4: Noisy student training for unseen languages. WERs (%) for the teacher adapter models and\nthe student models are presented. The relative improvement (%) of the student models can be found\nin the last column.\nLanguages\nWhisper-v2\n# hrs in YT-NTL\nUSM-LAS-Adapter\nUSM-M + pseudo label\nRel. Imprv.\nHausa (ha)\n88.9\n2175.0\n24.5\n22.8\n7.5\nKazakh (kk)\n37.7\n196.0\n11.8\n10.9\n8.3\nShona (sn)\n121.0\n247.0\n29.1\n22.2\n31.1\nPashto (ps)\n93.7\n254.0\n36.0\n35.4\n1.7\nYoruba (yo)\n94.8\n1292.0\n33.4\n30.6\n9.2\nTail languages often do not have paired transcriptions for supervised learning\u2014we refer to these\nlanguages as unseen languages, as the model has not seen paired data for these lanugages during\ntraining. To create pseudo-labels for these languages, we first build a USM-LAS-Adapter by attaching\nresidual adapters to USM-LAS and training them using FLEURS data. By using the USM-LAS-\nAdapter as a teacher, we can now transcribe the unlabeled data in the unseen languages as part\nof the YT-NTL dataset. As shown in Table 4, we observe consistent wins for all languages on\nthe FLEURS benchmark. For some languages, the improvement is larger than 30%. This further\ndemonstrates the robustness of the USM-LAS model\u2014despite using only 10 hours of out of domain\ndata from FLEURS, the USM-LAS-Adapter is able to transcribe YouTube data to produce meaningful\nrecognition results that lead to these improvements. We find the approach of training adapter models\n13\non small datasets and utilizing them for pseudo-labeling to be a promising route for scaling up the\nnumber of languages that can be transcribed by USMs.\n4.5\nUSMs are Strong AST Models\nThe multi-lingual speech translation performance of fine-tuned USMs are shown in Table 3. We\nfind that we are already comparable to the CoVoST 2 SoTA BLEU score by fine-tuning the speech-\nonly USM. We note that the previous SoTA uses 125k hours of supervised speech translation data\ncompared to the 859 hours of data used by the USM. After MOST training, USM-M can use both\nspeech and text as training input. By introducing text-to-text machine translation (MT) data during\nfine-tuning, USM-M is able to achieve an unprecedented > 30 BLEU on CoVoST (a 1 BLEU increase\nfrom SoTA).\n5\nAnalysis and Ablations\n5.1\nMulti-Softmax Loss for BEST-RQ\nWe observe a consistent > 5% relative improvement in ASR and AST benchmarks by increasing\nthe number of the softmax groups in the multi-softmax loss for BEST-RQ training from 1 to 16, as\nshown in Table 5. We also find that using multiple softmax groups significantly reduces performance\nvariation across different pre-training runs and improves convergence speed.\nTable 5: YT-55 versus YT-NTL across different domains, with and without multi-softmax groups. For\nsimplicity, we report CER for FLEURS. For CoVoST, we report the BLEU score. YT-NTL covers 27\nadditional languages not covered in YT-55.\nModel\npre-train Set\n# Params (B)\n# Softmax\nFLEURS (CER)\nCoVoST (BLEU)\n.\n102 langs\n27 langs\nConformer-0.6B\nYT-55\n0.6\n1\n9.5\n-\n20.9\nConformer-2B\nYT-55\n2.0\n1\n7.9\n9.5\n26.6\nConformer-2B\nYT-NTL-U\n2.0\n1\n7.4\n8.5\n27.5\nConformer-2B\nYT-NTL-U\n2.0\n16\n6.9\n8.1\n28.7\n5.2\nModel and Language Scaling\nWe find that scaling up the model size and increasing the language coverage of the pre-training\ndataset greatly benefits the performance of the USMs, as demonstrated in Table 5. In particular, we\nfind a 10% relative improvement of ASR and AST performance by using YT-NTL vs. YT-55 for\npre-training, despite the fact that each newly added language in YT-NTL contains approximately 500\nhours of speech\u2014a relatively small amount. As could be expected, the relative gains on the newly\ncovered languages are more substantial than those on other languages.\n5.3\nBEST-RQ is a Scalable Self-supervised Learner\nBEST-RQ has been shown to outperform or be comparable to other prominent pre-training methods\nfor speech recognition, including wav2vec 2.0 and W2v-BERT in the original work in which it was\nintroduced [10]. Here we investigate its comparative performance and scaling properties, similar\nto what has been done for wav2vec 2.0 in [3] and W2v-BERT in [20]. We utilize the set-up of\npre-training the model using YT-55 and fine-tuning it on CoVoST 2. As shown in Table 6, our results\nindicate that for the Conformer-0.6B, W2v-BERT and BEST-RQ perform similarly, but BEST-RQ\nobtains greater gains when scaled up. A contributing factor to this can be that W2v-BERT is more\nprone to codebook collapse and training instabilities at the 2B scale, while BEST-RQ by construction\ndoesn\u2019t suffer from codebook collapse.\n5.4\nChunk-wise attention for robust long-form speech recognition\nFig. 7 depicts the long-form performance degradation issue as described in section 2.4. In the figure,\nwe see that for the shallow Conformer model with 17 layers, using a small local self attention context\n14\nTable 6: BLEU scores for the CoVoST 2 X \u2192En task to compare BEST-RQ against W2v-BERT.\nHigher is better.\nX \u2192English\nhigh\nmid\nlow\nall\nPrevious Work\nXLS-R (0.3B) [33]\n30.6\n18.9\n5.1\n13.2\nXLS-R (1B) [33]\n34.3\n25.5\n11.7\n19.3\nXLS-R (2B) [33]\n36.1\n27.7\n15.1\n22.1\nConformer-0.6B\nW2v-BERT\n35.6\n25.3\n13.4\n20.4\nBEST-RQ\n32.5\n25.6\n14.7\n20.7\nConformer-2B\nW2v-BERT\n36.0\n27.8\n15.6\n22.4\nBEST-RQ\n35.8\n31.3\n21.5\n26.6\nFigure 7: The word error rate measured on the YouTube en-US long-form test set for Conformer\nmodels with varying depth.\n(65) length, the word error rate measured on the long-form test set gradually improves as the training\nprogresses. With a deeper model that has 48 layers but roughly the same number of parameters,\nhowever, the larger receptive field mismatch results in higher test WERs as the training step increases.\nTable 7 demonstrates that chunk-wise attention is able to address the long-form degradation issue and\nshow robust performance across four different languages\u2014en-US (English), ru-RU (Russian), ko-KR\n(Korean), and uk-UA (Ukrainian). We compare chunk-wise attention models with an 8-second chunk\nsize (CW-8s in Table 7) against local self attention models which uses 128 context frames in each\nconformer layer (LSA-128). We note that further increasing the context window size of the local self\nattention model results in high deletion error rates on all languages of the YouTube long-form test\nsets. These results show that the chunk-wise attention models do not exhibit long-form performance\ndegradation and are able to improve upon the performance of the local self attention models operating\nat the maximum allowed receptive field length.\nTable 7: Chunk-wise attention. WER (%) is reported on the YouTube long-form set.\nModel\n# Params (B)\n# Layers\nen-US\nru-RU\nko-KR\nuk-UA\nLSA-128\n0.6\n24\n16.2\n16.6\n26.2\n15.5\nCW-8s\n0.6\n24\n12.5\n14.7\n19.5\n15.3\n5.5\nTPU Serving Capacity of USM-CTC Models\nIn section 4, we have demonstrated that USM-CTC models are powerful generic ASR models\nwith reliable long-form transcription performance and excellent generalization properties. Here we\n15\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\nschedule where training initially is conducted with un-transcribed speech and paired speech-text data,\nand unspoken text is utilized only after 20k steps. The joint training employing all three types of data\nlasts for another 100K steps.\nSupervised Training: We use two separate optimizers for the encoder parameters and the decoder\nparameters of the network [71]. For USM-CTC and USM-LAS, we train the model for 100k steps\nwith 2048 batch size. For in-domain experiments, the checkpoint is selected based on development\nset performance.\nTraining Large Models: We use the GShard [72] framework with the GSPMD backend [73] to\ntrain our large models on TPUs.\n3\nDatasets\n3.1\nAudio Data\nFigure 6: The video category and length distribution of YT-513-U.\nThe following audio datasets are used in this report to train our models:\n\u2022 YouTube SUPervised Plus (YT-SUP+):\n\u2013 YT-SUP: 90k hours of segmented, labeled audio across 75 languages.\n\u2013 YT-Pseudo-Labeled: 100k hours of segmented, pseudo-labeled en-US audio from YT-\nNTL-U. The pseudo-labels are generated by a 600M CTC model trained on YT-SUP\nen-US data.\n\u2022 YouTube Next Thousand Languages Unsupervised (YT-NTL-U): 12.1M hours of seg-\nmented, unlabeled audio, including:\n\u2013 YT-55-U: 12M hours of segmented, unlabeled audio on 55 rich resource languages\nidentified by YouTube production language id models.\n\u2013 YT-513-U: 100k hours of segmented, unlabeled audio across 513 tail languages not\ncovered by YouTube production language id models. These languages are identified by\nvendors.\nLet us expand upon how each dataset has been constructed.\nYT-SUP+: YT-SUP is a dataset with audio from videos that have user-uploaded transcripts from 75\nlanguages. We group consecutive segments into a longer unit similar to [57]. The maximal sequence\nlength for training is 30 seconds. The total amount of training data is 90k hours, ranging from English\n(en-US) (3.5k hours) to Amharic (Am-Et) (150 hours). We also introduce an additional 100k hours of\nen-US audio from YT-NTL-U to YT-SUP. We choose to generate pseudo-labels on this dataset using\na 600M-parameter CTC YT teacher model trained on YT-SUP. Each audio is randomly segmented\nbetween 5 to 15 seconds.\nYT-55-U: YT-55-U is built by first randomly collecting 3 million hours of audio from \"speech-heavy\"\nYouTube videos, filtered by language. The 3 million hours of audio is then further segmented by the\nYT teacher model. Instead of using a teacher model as in [3], the non-speech segments identified\nby a Voice Activity Detection (VAD) model are removed to yield approximately 1 million hours of\n10\nunlabeled audio data. Later, we use a YouTube production language identification model to select 55\nlanguages from that audio.\nYT-513-U: We create an additional dataset called YT-513-U to ensure coverage of lower resource\nlanguages in our pre-training dataset. We reached out to vendors and native speakers to identify YT\nvideos containing speech in specific long tail languages, collecting a dataset of unlabeled speech in\n513 languages. Vendors were tasked with ensuring a variety of domains, voices, and content in the\nvideos that are collected in each language. These videos are segmented into speech segments using a\nVAD model, resulting in a total of 102k hours of speech. Our final YT-513-U dataset contains 88\nlanguages with over 500 hours of speech each, 237 languages with between 100-500 hours, and 188\nlanguages with less than 100 hours of data. The languages chosen for this collection are wide-ranging,\nwith a majority of our data corresponding to languages from South Asia, Southeast Asia, West Africa,\nand East Africa. The distribution of video categories and lengths in our dataset are depicted in\nFigure 6.\nIn addition to YouTube data, we also include public data for MOST training:\n\u2022 Public Unsupervised (Pub-U): Following [20], we use approximately 429k hours of\nunlabeled speech data in 51 languages. It includes: 372k hours of speech data spanning\n23 languages from VoxPopuli [74], read speech data in 25 languages drawn from the v6.1\nrelease of Common Voice [75], 50k hours of read books data in eight European languages\nfrom Multilingual LibriSpeech [76] and 1k hours of telephonic conversation data spanning\n17 African and Asian languages from BABEL [77].\n\u2022 Public Supervised (Pub-S): Similar to [20], our public supervised set includes approxi-\nmately 1.3k hours of speech and transcript data spanning 14 languages from VoxPopuli,\n10 hour training splits for each of the 8 MLS languages, and 1k hours of data spanning 17\nlanguages from the Babel ASR task.\nNote that the public data is only used for in-domain pre-training and is excluded for training the\ngeneric USM-LAS/CTC models. This allows us to treat the public task performance as out-of-domain\nbenchmarks for the USM-LAS/CTC models.\n3.2\nText Data\nWeb-NTL: For pre-training with unlabeled text, we use a web-crawled corpus of monolingual text\ncontaining over 28B sentences [78]. The dataset spans 1140 languages, 205 of which have over 1M\nsentences and 199 of which have between 100k and 1M sentences. We up-sample lower resource\nlanguages using temperature-based sampling [79] with T = 3.0. More details about the dataset and\nthe mining approach have been described in Section 2 of [78].\n3.3\nDownstream Benchmarks\n3.3.1\nSpeech Recognition (ASR)\nWe present our results on two public tasks, SpeechStew [2] and FLEURS [16], and an internal\nbenchmark on YouTube.\nThe SpeechStew [2] dataset is assembled by putting together seven public speech corpora\u2014AMI [80],\nCommon Voice [81], English Broadcast News3, LibriSpeech [82], Switchboard/Fisher4, TED-LIUM\nv3 [83,84] and Wall Street Journal5, which are all standard benchmarks [85\u201387] covering different\ndomains in en-US.\nThe FLEURS [16] dataset is a publicly available, multi-way parallel dataset of 10 hours of read\nspeech in 102 languages spanning 7 geo-groups. We restrict our use of the dataset to its ASR\nbenchmark. Among the 102 languages present in the FLEURS benchmark, we select 62 to serve as a\nsub-group to compare our generic ASR system with Whisper [1], as those languages are covered by\nthe training sets of both models. We also report full results for in-domain fine-tuning and adaptation.\nUnlike [16], we report both WER and CER metrics, as CER is inappropriate as an indicator of\n3Linguistic data consortium (LDC) datasets LDC97S44, LDC97T22, LDC98S71 and LDC98T28.\n4LDC datasets LDC2004T19, LDC2005T19, LDC2004S13, LDC2005S13 and LDC97S62.\n5LDC datasets LDC93S6B and LDC94S13B.\n11\nTable 3: WERs (%) across multiple tasks for multiple settings compared against pre-existing baselines,\nwith the exception of CoVoST 2, for which the BLEU score is presented. For the YouTube long-form\nset, we select the top-25 languages Whisper was trained on and exclude all languages for which\nWhisper produces > 40% WER to reduce the noise introduced by LAS hallucination in the Whisper\nmodel. For FLEURS, we report both the WER and the CER for our models. \u2020Results omitted for the\nWhisper-shortform model on the YouTube long-form dataset as the model has a high deletion problem\non this set. \u2021The Whisper-shortform model uses segmented decoding to reduce its hallucination\nproblem on CORAAL. \u00a7Our adapter setup adds about 2.3% of the total parameters while keeping the\nencoder frozen from pre-training.\nTask\nMultilingual Long-form ASR\nMultidomain en-US\nMultilingual ASR\nAST\nDataset\nYouTube\nCORAAL\nSpeechStew\nFLEURS\nCoVoST 2\nLangauges\nen-US\n18\n73\nen-US\nen-US\n62\n102\n21\nPrior Work (single model)\nWhisper-longform\n17.7\n27.8\n-\n23.9\n12.8\nWhisper-shortform\u2020\n-\n-\n-\n13.2\u2021\n11.5\n36.6\n-\n29.1\nOur Work (single model)\nUSM-LAS\n14.4\n19.0\n29.8\n11.2\n10.5\n12.5\n-\n-\nUSM-CTC\n13.7\n18.7\n26.7\n12.1\n10.8\n15.5\n-\n-\nPrior Work (in-domain fine-tuning)\nBigSSL [3]\n14.8\n-\n-\n-\n7.5\n-\n-\n-\nMaestro [67]\n7.2\n25.2\nMaestro-U [67]\n26.0 (8.7)\nOur Work (in-domain fine-tuning)\nUSM\n13.2\n-\n-\n-\n7.4\n13.5\n19.2 (6.9)\n28.7\nUSM-M\n12.5\n-\n-\n-\n7.0\n11.8\n17.4 (6.5)\n30.7\nOur Work (frozen encoder)\nUSM-M-adapter\u00a7\n-\n-\n-\n-\n7.5\n12.4\n17.6 (6.7)\n29.6\nperformance for some languages. When presenting the error rate metrics, we use CER for Chinese,\nJapanese, Thai, Lao, and Burmese to be consistent with Whisper [1].\nThe test set for the YouTube domain consists of utterances from 73 languages with an average of\n15 hours of audio per language, the audio length for each individual language ranging from 1 to 24\nhours. The audio is transcribed manually from popular YouTube videos, each with a duration of up to\n30 minutes.\n3.3.2\nSpeech Translation (AST)\nFollowing [20], we use CoVoST 2 [18] to benchmark multilingual speech translation. We evaluate\nthe multilingual XX-to-English task that covers translation from 21 source languages into English.\nDepending on the language, the training data ranges in size from 1 - 264 hours.\nBesides speech translation data, we also add text-to-text translation data for training the model as\nin [20]. This dataset includes the text translation data from CoVoST 2 combined with all data from\neither WMT or TED Talks, as available.\n4\nKey Results\n4.1\nRobust Speech Recognition for Massively Multilingual Tasks\nIn this section, we compare the performance of our models against public baselines, including\nWhisper large-v26 [1], which has been trained on 680k hours of weakly supervised data across 100\nlanguages.\nFor the massively multilingual speech recognition test dataset from YouTube, we observe that Whisper\nhallucinates in many languages, resulting in a WER exceeding 100%. For a reasonable comparison,\nwe restrict the language set on which we compare the performance USM against Whisper by first\nselecting the top-25 languages from the training data for Whisper and further excluding languages for\nwhich Whisper produces > 40% WER. We also use segmented decoding for Whisper with 30-second\nsegments to further reduce the effect of hallucinations. As shown in Table 3, our USM-LAS and\n6Whisper large-v2 on Github (https://github.com/openai/whisper.git, revision b4308c4) is used for evaluation.\n12\nUSM-CTC models outperform Whisper by a wide margin on YouTube en-US, despite training on\nsignificantly less supervised data (3.5k hours versus Whisper\u2019s 400k hours [1]). While the USM-LAS\nmodel also requires segmented decoding to reduce long-form degradation as discussed in section\n2.4, it is far more robust, out-performing Whisper by a relative 30% WER on those 18 languages.\nUSM-CTC does not exhibit long-form performance degradation and achieves the best performance\non YouTube.\nOn the out-of-domain long-form CORAAL set, both USM-CTC and USM-LAS outperform Whisper\nby more than 10% relative WER. USM-CTC and USM-LAS similary outperform Whisper on\nSpeechStew, whose training data the models have not had access to.\nWe further compare the multilingual performance of the models on the held-out set from FLEURS.\nAs shown in Table 3, USM-LAS and USM-CTC both outperform Whisper by 66% relative WER,\ndespite using a smaller amount of multilingual supervised data (90k versus Whisper\u2019s 117k, when\nen-US is excluded). USM-LAS consistently outperforms USM-CTC for short-form ASR tasks.\n4.2\nMassively Multilingual Results Beyond 100 Languages\nThe lower part of Table 3 shows our results for in-domain fine-tuning. Our pre-trained model improves\nthe FLEURS benchmark significantly, even when using only 10 hours per language. Compared to the\nprevious SoTA in [67], our model achieves a 30% relative improvement in terms of WER across 102\nlanguages. Our results show that while generic speech models can be powerful, performance is still\nmaximized by in-domain fine-tuning.\n4.3\nMOST Produces Robust Representations that Generalize to New Domains\nMOST training aligns the representations of speech and text by training simultaneously on the two\nmodalities. We investigate whether MOST representations are useful for adapting the model to new\ndomains by freezing the entire learned encoder produced by MOST and adjusting a small amount of\nparameters added to the network by residual adapters. As shown in Table 3, by adding only 2% to\nthe total number of parameters, the MOST representation model (USM-M-adapter) only performs\nslightly worse than the fine-tuning baselines, still showing competitive performance on downstream\nASR and AST tasks. The small number of parameters being trained in this approach makes it feasible\nto extend our system to a large number of new domains and new tasks, even with a limited amount of\ntraining data, such as in FLEURS.\n4.4\nPushing the Quality of ASR on Unseen Languages\nTable 4: Noisy student training for unseen languages. WERs (%) for the teacher adapter models and\nthe student models are presented. The relative improvement (%) of the student models can be found\nin the last column.\nLanguages\nWhisper-v2\n# hrs in YT-NTL\nUSM-LAS-Adapter\nUSM-M + pseudo label\nRel. Imprv.\nHausa (ha)\n88.9\n2175.0\n24.5\n22.8\n7.5\nKazakh (kk)\n37.7\n196.0\n11.8\n10.9\n8.3\nShona (sn)\n121.0\n247.0\n29.1\n22.2\n31.1\nPashto (ps)\n93.7\n254.0\n36.0\n35.4\n1.7\nYoruba (yo)\n94.8\n1292.0\n33.4\n30.6\n9.2\nTail languages often do not have paired transcriptions for supervised learning\u2014we refer to these\nlanguages as unseen languages, as the model has not seen paired data for these lanugages during\ntraining. To create pseudo-labels for these languages, we first build a USM-LAS-Adapter by attaching\nresidual adapters to USM-LAS and training them using FLEURS data. By using the USM-LAS-\nAdapter as a teacher, we can now transcribe the unlabeled data in the unseen languages as part\nof the YT-NTL dataset. As shown in Table 4, we observe consistent wins for all languages on\nthe FLEURS benchmark. For some languages, the improvement is larger than 30%. This further\ndemonstrates the robustness of the USM-LAS model\u2014despite using only 10 hours of out of domain\ndata from FLEURS, the USM-LAS-Adapter is able to transcribe YouTube data to produce meaningful\nrecognition results that lead to these improvements. We find the approach of training adapter models\n13\non small datasets and utilizing them for pseudo-labeling to be a promising route for scaling up the\nnumber of languages that can be transcribed by USMs.\n4.5\nUSMs are Strong AST Models\nThe multi-lingual speech translation performance of fine-tuned USMs are shown in Table 3. We\nfind that we are already comparable to the CoVoST 2 SoTA BLEU score by fine-tuning the speech-\nonly USM. We note that the previous SoTA uses 125k hours of supervised speech translation data\ncompared to the 859 hours of data used by the USM. After MOST training, USM-M can use both\nspeech and text as training input. By introducing text-to-text machine translation (MT) data during\nfine-tuning, USM-M is able to achieve an unprecedented > 30 BLEU on CoVoST (a 1 BLEU increase\nfrom SoTA).\n5\nAnalysis and Ablations\n5.1\nMulti-Softmax Loss for BEST-RQ\nWe observe a consistent > 5% relative improvement in ASR and AST benchmarks by increasing\nthe number of the softmax groups in the multi-softmax loss for BEST-RQ training from 1 to 16, as\nshown in Table 5. We also find that using multiple softmax groups significantly reduces performance\nvariation across different pre-training runs and improves convergence speed.\nTable 5: YT-55 versus YT-NTL across different domains, with and without multi-softmax groups. For\nsimplicity, we report CER for FLEURS. For CoVoST, we report the BLEU score. YT-NTL covers 27\nadditional languages not covered in YT-55.\nModel\npre-train Set\n# Params (B)\n# Softmax\nFLEURS (CER)\nCoVoST (BLEU)\n.\n102 langs\n27 langs\nConformer-0.6B\nYT-55\n0.6\n1\n9.5\n-\n20.9\nConformer-2B\nYT-55\n2.0\n1\n7.9\n9.5\n26.6\nConformer-2B\nYT-NTL-U\n2.0\n1\n7.4\n8.5\n27.5\nConformer-2B\nYT-NTL-U\n2.0\n16\n6.9\n8.1\n28.7\n5.2\nModel and Language Scaling\nWe find that scaling up the model size and increasing the language coverage of the pre-training\ndataset greatly benefits the performance of the USMs, as demonstrated in Table 5. In particular, we\nfind a 10% relative improvement of ASR and AST performance by using YT-NTL vs. YT-55 for\npre-training, despite the fact that each newly added language in YT-NTL contains approximately 500\nhours of speech\u2014a relatively small amount. As could be expected, the relative gains on the newly\ncovered languages are more substantial than those on other languages.\n5.3\nBEST-RQ is a Scalable Self-supervised Learner\nBEST-RQ has been shown to outperform or be comparable to other prominent pre-training methods\nfor speech recognition, including wav2vec 2.0 and W2v-BERT in the original work in which it was\nintroduced [10]. Here we investigate its comparative performance and scaling properties, similar\nto what has been done for wav2vec 2.0 in [3] and W2v-BERT in [20]. We utilize the set-up of\npre-training the model using YT-55 and fine-tuning it on CoVoST 2. As shown in Table 6, our results\nindicate that for the Conformer-0.6B, W2v-BERT and BEST-RQ perform similarly, but BEST-RQ\nobtains greater gains when scaled up. A contributing factor to this can be that W2v-BERT is more\nprone to codebook collapse and training instabilities at the 2B scale, while BEST-RQ by construction\ndoesn\u2019t suffer from codebook collapse.\n5.4\nChunk-wise attention for robust long-form speech recognition\nFig. 7 depicts the long-form performance degradation issue as described in section 2.4. In the figure,\nwe see that for the shallow Conformer model with 17 layers, using a small local self attention context\n14\nTable 6: BLEU scores for the CoVoST 2 X \u2192En task to compare BEST-RQ against W2v-BERT.\nHigher is better.\nX \u2192English\nhigh\nmid\nlow\nall\nPrevious Work\nXLS-R (0.3B) [33]\n30.6\n18.9\n5.1\n13.2\nXLS-R (1B) [33]\n34.3\n25.5\n11.7\n19.3\nXLS-R (2B) [33]\n36.1\n27.7\n15.1\n22.1\nConformer-0.6B\nW2v-BERT\n35.6\n25.3\n13.4\n20.4\nBEST-RQ\n32.5\n25.6\n14.7\n20.7\nConformer-2B\nW2v-BERT\n36.0\n27.8\n15.6\n22.4\nBEST-RQ\n35.8\n31.3\n21.5\n26.6\nFigure 7: The word error rate measured on the YouTube en-US long-form test set for Conformer\nmodels with varying depth.\n(65) length, the word error rate measured on the long-form test set gradually improves as the training\nprogresses. With a deeper model that has 48 layers but roughly the same number of parameters,\nhowever, the larger receptive field mismatch results in higher test WERs as the training step increases.\nTable 7 demonstrates that chunk-wise attention is able to address the long-form degradation issue and\nshow robust performance across four different languages\u2014en-US (English), ru-RU (Russian), ko-KR\n(Korean), and uk-UA (Ukrainian). We compare chunk-wise attention models with an 8-second chunk\nsize (CW-8s in Table 7) against local self attention models which uses 128 context frames in each\nconformer layer (LSA-128). We note that further increasing the context window size of the local self\nattention model results in high deletion error rates on all languages of the YouTube long-form test\nsets. These results show that the chunk-wise attention models do not exhibit long-form performance\ndegradation and are able to improve upon the performance of the local self attention models operating\nat the maximum allowed receptive field length.\nTable 7: Chunk-wise attention. WER (%) is reported on the YouTube long-form set.\nModel\n# Params (B)\n# Layers\nen-US\nru-RU\nko-KR\nuk-UA\nLSA-128\n0.6\n24\n16.2\n16.6\n26.2\n15.5\nCW-8s\n0.6\n24\n12.5\n14.7\n19.5\n15.3\n5.5\nTPU Serving Capacity of USM-CTC Models\nIn section 4, we have demonstrated that USM-CTC models are powerful generic ASR models\nwith reliable long-form transcription performance and excellent generalization properties. Here we\n15\nTable 8: RTF for USM-2B.\nModel\nbf-16\nStreaming\n# Params (B)\nTPU [88]\nBatch Size\n1.0/RTF\nConformer-0.1B\nY\nY\n0.1\nTPUv4i\n64\n3047\nConformer-0.6B\nN\nN\n0.6\nTPUv4i\n64\n1920\nConformer-2B\nN\nN\n2.0\nTPUv4i\n32\n827\nmeasure the serving capacity of the USM-CTC model as represented by the real time factor (RTF) in\nan ideal setup where we assume that each batch sent to TPU is fully packed along the time axis. The\nresults of these measurements are presented in Table 8. Surprisingly, we find that the 2B-paramter\nUSM-CTC model is only 3.9\u00d7 slower than the 100M-parameter streaming model [89], primarily\ndue to the fact that our models operate at batch processing mode. This result demonstrates that the\nUSM-CTC can be used as an offline transcriber efficiently on TPUs (or GPUs).\n6\nDiscussion\nIn this report, we put forward a practical and flexible approach for training speech understanding\nmodels capable of scaling speech recognition to hundreds of languages. We conclude the report with\nsummarizing insights gained in the process:\nUnlabeled versus weakly labeled data: We believe diverse unlabeled data is more practical to\nacquire for building usable ASR for tail languages than weakly labeled data. We have demonstrated\nthat collaborating with native speakers to identify unsupervised data in hundreds of tail languages\ncan be an effective route to improving recognition performance on low resource languages.\nIn-domain data is best: We have demonstrated that we can build a robust ASR system across many\ndomains by utilizing a large amount of unsupervised data and a small amount of labeled data. Our\nresults, however, also confirm that the most effective way to optimize the performance for a given\ndomain is to use in-domain data to fine-tune the model.\nCTC vs RNN-T vs LAS: The best transducer depends on the downstream task. A large pre-trained\nmodel with a frozen encoder can allow experimenters to test different transducers quickly and select\nthe optimal transducer for their purpose.\nAcknowledgments\nWe would like to thank Alexis Conneau, Min Ma, Shikhar Bharadwaj, Sid Dalmia, Jiahui Yu, Jian\nCheng, Paul Rubenstein, Ye Jia, Justin Snyder, Vincent Tsang, Yuanzhong Xu, Tao Wang, Anusha\nRamesh, Calum Barnes, Salem Haykal for useful discussions.\nWe appreciate valuable feedback and support from Eli Collins, Jeff Dean, Sissie Hsiao, Zoubin\nGhahramani. Special thanks to Austin Tarango, Lara Tumeh, and Jason Porta for their guidance\naround responsible AI practices.\nReferences\n[1] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition\nvia large-scale weak supervision,\u201d arXiv preprint arXiv:2212.04356, 2022.\n[2] W. Chan, D. Park, C. Lee, Y. Zhang, Q. Le, and M. Norouzi, \u201cSpeechstew: Simply mix all available speech\nrecognition data to train one large neural network,\u201d arXiv preprint arXiv:2104.02133, 2021.\n[3] Y. Zhang, D. S. Park, W. Han, J. Qin, A. Gulati, J. Shor, A. Jansen, Y. Xu, Y. Huang, S. Wang, Z. Zhou, B. Li,\nM. Ma, W. Chan, J. Yu, Y. Wang, L. Cao, K. C. Sim, B. Ramabhadran, T. N. Sainath, F. Beaufays, Z. Chen,\nQ. V. Le, C.-C. Chiu, R. Pang, and Y. Wu, \u201cBigssl: Exploring the frontier of large-scale semi-supervised\nlearning for automatic speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16,\nno. 6, pp. 1519\u20131532, 2022.\n[4] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani, W. R. Huang, and M. Ma, \u201cScaling\nend-to-end models for large-scale multilingual asr,\u201d arXiv preprint arXiv:2104.14830, 2021.\n16\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\nschedule where training initially is conducted with un-transcribed speech and paired speech-text data,\nand unspoken text is utilized only after 20k steps. The joint training employing all three types of data\nlasts for another 100K steps.\nSupervised Training: We use two separate optimizers for the encoder parameters and the decoder\nparameters of the network [71]. For USM-CTC and USM-LAS, we train the model for 100k steps\nwith 2048 batch size. For in-domain experiments, the checkpoint is selected based on development\nset performance.\nTraining Large Models: We use the GShard [72] framework with the GSPMD backend [73] to\ntrain our large models on TPUs.\n3\nDatasets\n3.1\nAudio Data\nFigure 6: The video category and length distribution of YT-513-U.\nThe following audio datasets are used in this report to train our models:\n\u2022 YouTube SUPervised Plus (YT-SUP+):\n\u2013 YT-SUP: 90k hours of segmented, labeled audio across 75 languages.\n\u2013 YT-Pseudo-Labeled: 100k hours of segmented, pseudo-labeled en-US audio from YT-\nNTL-U. The pseudo-labels are generated by a 600M CTC model trained on YT-SUP\nen-US data.\n\u2022 YouTube Next Thousand Languages Unsupervised (YT-NTL-U): 12.1M hours of seg-\nmented, unlabeled audio, including:\n\u2013 YT-55-U: 12M hours of segmented, unlabeled audio on 55 rich resource languages\nidentified by YouTube production language id models.\n\u2013 YT-513-U: 100k hours of segmented, unlabeled audio across 513 tail languages not\ncovered by YouTube production language id models. These languages are identified by\nvendors.\nLet us expand upon how each dataset has been constructed.\nYT-SUP+: YT-SUP is a dataset with audio from videos that have user-uploaded transcripts from 75\nlanguages. We group consecutive segments into a longer unit similar to [57]. The maximal sequence\nlength for training is 30 seconds. The total amount of training data is 90k hours, ranging from English\n(en-US) (3.5k hours) to Amharic (Am-Et) (150 hours). We also introduce an additional 100k hours of\nen-US audio from YT-NTL-U to YT-SUP. We choose to generate pseudo-labels on this dataset using\na 600M-parameter CTC YT teacher model trained on YT-SUP. Each audio is randomly segmented\nbetween 5 to 15 seconds.\nYT-55-U: YT-55-U is built by first randomly collecting 3 million hours of audio from \"speech-heavy\"\nYouTube videos, filtered by language. The 3 million hours of audio is then further segmented by the\nYT teacher model. Instead of using a teacher model as in [3], the non-speech segments identified\nby a Voice Activity Detection (VAD) model are removed to yield approximately 1 million hours of\n10\nunlabeled audio data. Later, we use a YouTube production language identification model to select 55\nlanguages from that audio.\nYT-513-U: We create an additional dataset called YT-513-U to ensure coverage of lower resource\nlanguages in our pre-training dataset. We reached out to vendors and native speakers to identify YT\nvideos containing speech in specific long tail languages, collecting a dataset of unlabeled speech in\n513 languages. Vendors were tasked with ensuring a variety of domains, voices, and content in the\nvideos that are collected in each language. These videos are segmented into speech segments using a\nVAD model, resulting in a total of 102k hours of speech. Our final YT-513-U dataset contains 88\nlanguages with over 500 hours of speech each, 237 languages with between 100-500 hours, and 188\nlanguages with less than 100 hours of data. The languages chosen for this collection are wide-ranging,\nwith a majority of our data corresponding to languages from South Asia, Southeast Asia, West Africa,\nand East Africa. The distribution of video categories and lengths in our dataset are depicted in\nFigure 6.\nIn addition to YouTube data, we also include public data for MOST training:\n\u2022 Public Unsupervised (Pub-U): Following [20], we use approximately 429k hours of\nunlabeled speech data in 51 languages. It includes: 372k hours of speech data spanning\n23 languages from VoxPopuli [74], read speech data in 25 languages drawn from the v6.1\nrelease of Common Voice [75], 50k hours of read books data in eight European languages\nfrom Multilingual LibriSpeech [76] and 1k hours of telephonic conversation data spanning\n17 African and Asian languages from BABEL [77].\n\u2022 Public Supervised (Pub-S): Similar to [20], our public supervised set includes approxi-\nmately 1.3k hours of speech and transcript data spanning 14 languages from VoxPopuli,\n10 hour training splits for each of the 8 MLS languages, and 1k hours of data spanning 17\nlanguages from the Babel ASR task.\nNote that the public data is only used for in-domain pre-training and is excluded for training the\ngeneric USM-LAS/CTC models. This allows us to treat the public task performance as out-of-domain\nbenchmarks for the USM-LAS/CTC models.\n3.2\nText Data\nWeb-NTL: For pre-training with unlabeled text, we use a web-crawled corpus of monolingual text\ncontaining over 28B sentences [78]. The dataset spans 1140 languages, 205 of which have over 1M\nsentences and 199 of which have between 100k and 1M sentences. We up-sample lower resource\nlanguages using temperature-based sampling [79] with T = 3.0. More details about the dataset and\nthe mining approach have been described in Section 2 of [78].\n3.3\nDownstream Benchmarks\n3.3.1\nSpeech Recognition (ASR)\nWe present our results on two public tasks, SpeechStew [2] and FLEURS [16], and an internal\nbenchmark on YouTube.\nThe SpeechStew [2] dataset is assembled by putting together seven public speech corpora\u2014AMI [80],\nCommon Voice [81], English Broadcast News3, LibriSpeech [82], Switchboard/Fisher4, TED-LIUM\nv3 [83,84] and Wall Street Journal5, which are all standard benchmarks [85\u201387] covering different\ndomains in en-US.\nThe FLEURS [16] dataset is a publicly available, multi-way parallel dataset of 10 hours of read\nspeech in 102 languages spanning 7 geo-groups. We restrict our use of the dataset to its ASR\nbenchmark. Among the 102 languages present in the FLEURS benchmark, we select 62 to serve as a\nsub-group to compare our generic ASR system with Whisper [1], as those languages are covered by\nthe training sets of both models. We also report full results for in-domain fine-tuning and adaptation.\nUnlike [16], we report both WER and CER metrics, as CER is inappropriate as an indicator of\n3Linguistic data consortium (LDC) datasets LDC97S44, LDC97T22, LDC98S71 and LDC98T28.\n4LDC datasets LDC2004T19, LDC2005T19, LDC2004S13, LDC2005S13 and LDC97S62.\n5LDC datasets LDC93S6B and LDC94S13B.\n11\nTable 3: WERs (%) across multiple tasks for multiple settings compared against pre-existing baselines,\nwith the exception of CoVoST 2, for which the BLEU score is presented. For the YouTube long-form\nset, we select the top-25 languages Whisper was trained on and exclude all languages for which\nWhisper produces > 40% WER to reduce the noise introduced by LAS hallucination in the Whisper\nmodel. For FLEURS, we report both the WER and the CER for our models. \u2020Results omitted for the\nWhisper-shortform model on the YouTube long-form dataset as the model has a high deletion problem\non this set. \u2021The Whisper-shortform model uses segmented decoding to reduce its hallucination\nproblem on CORAAL. \u00a7Our adapter setup adds about 2.3% of the total parameters while keeping the\nencoder frozen from pre-training.\nTask\nMultilingual Long-form ASR\nMultidomain en-US\nMultilingual ASR\nAST\nDataset\nYouTube\nCORAAL\nSpeechStew\nFLEURS\nCoVoST 2\nLangauges\nen-US\n18\n73\nen-US\nen-US\n62\n102\n21\nPrior Work (single model)\nWhisper-longform\n17.7\n27.8\n-\n23.9\n12.8\nWhisper-shortform\u2020\n-\n-\n-\n13.2\u2021\n11.5\n36.6\n-\n29.1\nOur Work (single model)\nUSM-LAS\n14.4\n19.0\n29.8\n11.2\n10.5\n12.5\n-\n-\nUSM-CTC\n13.7\n18.7\n26.7\n12.1\n10.8\n15.5\n-\n-\nPrior Work (in-domain fine-tuning)\nBigSSL [3]\n14.8\n-\n-\n-\n7.5\n-\n-\n-\nMaestro [67]\n7.2\n25.2\nMaestro-U [67]\n26.0 (8.7)\nOur Work (in-domain fine-tuning)\nUSM\n13.2\n-\n-\n-\n7.4\n13.5\n19.2 (6.9)\n28.7\nUSM-M\n12.5\n-\n-\n-\n7.0\n11.8\n17.4 (6.5)\n30.7\nOur Work (frozen encoder)\nUSM-M-adapter\u00a7\n-\n-\n-\n-\n7.5\n12.4\n17.6 (6.7)\n29.6\nperformance for some languages. When presenting the error rate metrics, we use CER for Chinese,\nJapanese, Thai, Lao, and Burmese to be consistent with Whisper [1].\nThe test set for the YouTube domain consists of utterances from 73 languages with an average of\n15 hours of audio per language, the audio length for each individual language ranging from 1 to 24\nhours. The audio is transcribed manually from popular YouTube videos, each with a duration of up to\n30 minutes.\n3.3.2\nSpeech Translation (AST)\nFollowing [20], we use CoVoST 2 [18] to benchmark multilingual speech translation. We evaluate\nthe multilingual XX-to-English task that covers translation from 21 source languages into English.\nDepending on the language, the training data ranges in size from 1 - 264 hours.\nBesides speech translation data, we also add text-to-text translation data for training the model as\nin [20]. This dataset includes the text translation data from CoVoST 2 combined with all data from\neither WMT or TED Talks, as available.\n4\nKey Results\n4.1\nRobust Speech Recognition for Massively Multilingual Tasks\nIn this section, we compare the performance of our models against public baselines, including\nWhisper large-v26 [1], which has been trained on 680k hours of weakly supervised data across 100\nlanguages.\nFor the massively multilingual speech recognition test dataset from YouTube, we observe that Whisper\nhallucinates in many languages, resulting in a WER exceeding 100%. For a reasonable comparison,\nwe restrict the language set on which we compare the performance USM against Whisper by first\nselecting the top-25 languages from the training data for Whisper and further excluding languages for\nwhich Whisper produces > 40% WER. We also use segmented decoding for Whisper with 30-second\nsegments to further reduce the effect of hallucinations. As shown in Table 3, our USM-LAS and\n6Whisper large-v2 on Github (https://github.com/openai/whisper.git, revision b4308c4) is used for evaluation.\n12\nUSM-CTC models outperform Whisper by a wide margin on YouTube en-US, despite training on\nsignificantly less supervised data (3.5k hours versus Whisper\u2019s 400k hours [1]). While the USM-LAS\nmodel also requires segmented decoding to reduce long-form degradation as discussed in section\n2.4, it is far more robust, out-performing Whisper by a relative 30% WER on those 18 languages.\nUSM-CTC does not exhibit long-form performance degradation and achieves the best performance\non YouTube.\nOn the out-of-domain long-form CORAAL set, both USM-CTC and USM-LAS outperform Whisper\nby more than 10% relative WER. USM-CTC and USM-LAS similary outperform Whisper on\nSpeechStew, whose training data the models have not had access to.\nWe further compare the multilingual performance of the models on the held-out set from FLEURS.\nAs shown in Table 3, USM-LAS and USM-CTC both outperform Whisper by 66% relative WER,\ndespite using a smaller amount of multilingual supervised data (90k versus Whisper\u2019s 117k, when\nen-US is excluded). USM-LAS consistently outperforms USM-CTC for short-form ASR tasks.\n4.2\nMassively Multilingual Results Beyond 100 Languages\nThe lower part of Table 3 shows our results for in-domain fine-tuning. Our pre-trained model improves\nthe FLEURS benchmark significantly, even when using only 10 hours per language. Compared to the\nprevious SoTA in [67], our model achieves a 30% relative improvement in terms of WER across 102\nlanguages. Our results show that while generic speech models can be powerful, performance is still\nmaximized by in-domain fine-tuning.\n4.3\nMOST Produces Robust Representations that Generalize to New Domains\nMOST training aligns the representations of speech and text by training simultaneously on the two\nmodalities. We investigate whether MOST representations are useful for adapting the model to new\ndomains by freezing the entire learned encoder produced by MOST and adjusting a small amount of\nparameters added to the network by residual adapters. As shown in Table 3, by adding only 2% to\nthe total number of parameters, the MOST representation model (USM-M-adapter) only performs\nslightly worse than the fine-tuning baselines, still showing competitive performance on downstream\nASR and AST tasks. The small number of parameters being trained in this approach makes it feasible\nto extend our system to a large number of new domains and new tasks, even with a limited amount of\ntraining data, such as in FLEURS.\n4.4\nPushing the Quality of ASR on Unseen Languages\nTable 4: Noisy student training for unseen languages. WERs (%) for the teacher adapter models and\nthe student models are presented. The relative improvement (%) of the student models can be found\nin the last column.\nLanguages\nWhisper-v2\n# hrs in YT-NTL\nUSM-LAS-Adapter\nUSM-M + pseudo label\nRel. Imprv.\nHausa (ha)\n88.9\n2175.0\n24.5\n22.8\n7.5\nKazakh (kk)\n37.7\n196.0\n11.8\n10.9\n8.3\nShona (sn)\n121.0\n247.0\n29.1\n22.2\n31.1\nPashto (ps)\n93.7\n254.0\n36.0\n35.4\n1.7\nYoruba (yo)\n94.8\n1292.0\n33.4\n30.6\n9.2\nTail languages often do not have paired transcriptions for supervised learning\u2014we refer to these\nlanguages as unseen languages, as the model has not seen paired data for these lanugages during\ntraining. To create pseudo-labels for these languages, we first build a USM-LAS-Adapter by attaching\nresidual adapters to USM-LAS and training them using FLEURS data. By using the USM-LAS-\nAdapter as a teacher, we can now transcribe the unlabeled data in the unseen languages as part\nof the YT-NTL dataset. As shown in Table 4, we observe consistent wins for all languages on\nthe FLEURS benchmark. For some languages, the improvement is larger than 30%. This further\ndemonstrates the robustness of the USM-LAS model\u2014despite using only 10 hours of out of domain\ndata from FLEURS, the USM-LAS-Adapter is able to transcribe YouTube data to produce meaningful\nrecognition results that lead to these improvements. We find the approach of training adapter models\n13\non small datasets and utilizing them for pseudo-labeling to be a promising route for scaling up the\nnumber of languages that can be transcribed by USMs.\n4.5\nUSMs are Strong AST Models\nThe multi-lingual speech translation performance of fine-tuned USMs are shown in Table 3. We\nfind that we are already comparable to the CoVoST 2 SoTA BLEU score by fine-tuning the speech-\nonly USM. We note that the previous SoTA uses 125k hours of supervised speech translation data\ncompared to the 859 hours of data used by the USM. After MOST training, USM-M can use both\nspeech and text as training input. By introducing text-to-text machine translation (MT) data during\nfine-tuning, USM-M is able to achieve an unprecedented > 30 BLEU on CoVoST (a 1 BLEU increase\nfrom SoTA).\n5\nAnalysis and Ablations\n5.1\nMulti-Softmax Loss for BEST-RQ\nWe observe a consistent > 5% relative improvement in ASR and AST benchmarks by increasing\nthe number of the softmax groups in the multi-softmax loss for BEST-RQ training from 1 to 16, as\nshown in Table 5. We also find that using multiple softmax groups significantly reduces performance\nvariation across different pre-training runs and improves convergence speed.\nTable 5: YT-55 versus YT-NTL across different domains, with and without multi-softmax groups. For\nsimplicity, we report CER for FLEURS. For CoVoST, we report the BLEU score. YT-NTL covers 27\nadditional languages not covered in YT-55.\nModel\npre-train Set\n# Params (B)\n# Softmax\nFLEURS (CER)\nCoVoST (BLEU)\n.\n102 langs\n27 langs\nConformer-0.6B\nYT-55\n0.6\n1\n9.5\n-\n20.9\nConformer-2B\nYT-55\n2.0\n1\n7.9\n9.5\n26.6\nConformer-2B\nYT-NTL-U\n2.0\n1\n7.4\n8.5\n27.5\nConformer-2B\nYT-NTL-U\n2.0\n16\n6.9\n8.1\n28.7\n5.2\nModel and Language Scaling\nWe find that scaling up the model size and increasing the language coverage of the pre-training\ndataset greatly benefits the performance of the USMs, as demonstrated in Table 5. In particular, we\nfind a 10% relative improvement of ASR and AST performance by using YT-NTL vs. YT-55 for\npre-training, despite the fact that each newly added language in YT-NTL contains approximately 500\nhours of speech\u2014a relatively small amount. As could be expected, the relative gains on the newly\ncovered languages are more substantial than those on other languages.\n5.3\nBEST-RQ is a Scalable Self-supervised Learner\nBEST-RQ has been shown to outperform or be comparable to other prominent pre-training methods\nfor speech recognition, including wav2vec 2.0 and W2v-BERT in the original work in which it was\nintroduced [10]. Here we investigate its comparative performance and scaling properties, similar\nto what has been done for wav2vec 2.0 in [3] and W2v-BERT in [20]. We utilize the set-up of\npre-training the model using YT-55 and fine-tuning it on CoVoST 2. As shown in Table 6, our results\nindicate that for the Conformer-0.6B, W2v-BERT and BEST-RQ perform similarly, but BEST-RQ\nobtains greater gains when scaled up. A contributing factor to this can be that W2v-BERT is more\nprone to codebook collapse and training instabilities at the 2B scale, while BEST-RQ by construction\ndoesn\u2019t suffer from codebook collapse.\n5.4\nChunk-wise attention for robust long-form speech recognition\nFig. 7 depicts the long-form performance degradation issue as described in section 2.4. In the figure,\nwe see that for the shallow Conformer model with 17 layers, using a small local self attention context\n14\nTable 6: BLEU scores for the CoVoST 2 X \u2192En task to compare BEST-RQ against W2v-BERT.\nHigher is better.\nX \u2192English\nhigh\nmid\nlow\nall\nPrevious Work\nXLS-R (0.3B) [33]\n30.6\n18.9\n5.1\n13.2\nXLS-R (1B) [33]\n34.3\n25.5\n11.7\n19.3\nXLS-R (2B) [33]\n36.1\n27.7\n15.1\n22.1\nConformer-0.6B\nW2v-BERT\n35.6\n25.3\n13.4\n20.4\nBEST-RQ\n32.5\n25.6\n14.7\n20.7\nConformer-2B\nW2v-BERT\n36.0\n27.8\n15.6\n22.4\nBEST-RQ\n35.8\n31.3\n21.5\n26.6\nFigure 7: The word error rate measured on the YouTube en-US long-form test set for Conformer\nmodels with varying depth.\n(65) length, the word error rate measured on the long-form test set gradually improves as the training\nprogresses. With a deeper model that has 48 layers but roughly the same number of parameters,\nhowever, the larger receptive field mismatch results in higher test WERs as the training step increases.\nTable 7 demonstrates that chunk-wise attention is able to address the long-form degradation issue and\nshow robust performance across four different languages\u2014en-US (English), ru-RU (Russian), ko-KR\n(Korean), and uk-UA (Ukrainian). We compare chunk-wise attention models with an 8-second chunk\nsize (CW-8s in Table 7) against local self attention models which uses 128 context frames in each\nconformer layer (LSA-128). We note that further increasing the context window size of the local self\nattention model results in high deletion error rates on all languages of the YouTube long-form test\nsets. These results show that the chunk-wise attention models do not exhibit long-form performance\ndegradation and are able to improve upon the performance of the local self attention models operating\nat the maximum allowed receptive field length.\nTable 7: Chunk-wise attention. WER (%) is reported on the YouTube long-form set.\nModel\n# Params (B)\n# Layers\nen-US\nru-RU\nko-KR\nuk-UA\nLSA-128\n0.6\n24\n16.2\n16.6\n26.2\n15.5\nCW-8s\n0.6\n24\n12.5\n14.7\n19.5\n15.3\n5.5\nTPU Serving Capacity of USM-CTC Models\nIn section 4, we have demonstrated that USM-CTC models are powerful generic ASR models\nwith reliable long-form transcription performance and excellent generalization properties. Here we\n15\nTable 8: RTF for USM-2B.\nModel\nbf-16\nStreaming\n# Params (B)\nTPU [88]\nBatch Size\n1.0/RTF\nConformer-0.1B\nY\nY\n0.1\nTPUv4i\n64\n3047\nConformer-0.6B\nN\nN\n0.6\nTPUv4i\n64\n1920\nConformer-2B\nN\nN\n2.0\nTPUv4i\n32\n827\nmeasure the serving capacity of the USM-CTC model as represented by the real time factor (RTF) in\nan ideal setup where we assume that each batch sent to TPU is fully packed along the time axis. The\nresults of these measurements are presented in Table 8. Surprisingly, we find that the 2B-paramter\nUSM-CTC model is only 3.9\u00d7 slower than the 100M-parameter streaming model [89], primarily\ndue to the fact that our models operate at batch processing mode. This result demonstrates that the\nUSM-CTC can be used as an offline transcriber efficiently on TPUs (or GPUs).\n6\nDiscussion\nIn this report, we put forward a practical and flexible approach for training speech understanding\nmodels capable of scaling speech recognition to hundreds of languages. We conclude the report with\nsummarizing insights gained in the process:\nUnlabeled versus weakly labeled data: We believe diverse unlabeled data is more practical to\nacquire for building usable ASR for tail languages than weakly labeled data. We have demonstrated\nthat collaborating with native speakers to identify unsupervised data in hundreds of tail languages\ncan be an effective route to improving recognition performance on low resource languages.\nIn-domain data is best: We have demonstrated that we can build a robust ASR system across many\ndomains by utilizing a large amount of unsupervised data and a small amount of labeled data. Our\nresults, however, also confirm that the most effective way to optimize the performance for a given\ndomain is to use in-domain data to fine-tune the model.\nCTC vs RNN-T vs LAS: The best transducer depends on the downstream task. A large pre-trained\nmodel with a frozen encoder can allow experimenters to test different transducers quickly and select\nthe optimal transducer for their purpose.\nAcknowledgments\nWe would like to thank Alexis Conneau, Min Ma, Shikhar Bharadwaj, Sid Dalmia, Jiahui Yu, Jian\nCheng, Paul Rubenstein, Ye Jia, Justin Snyder, Vincent Tsang, Yuanzhong Xu, Tao Wang, Anusha\nRamesh, Calum Barnes, Salem Haykal for useful discussions.\nWe appreciate valuable feedback and support from Eli Collins, Jeff Dean, Sissie Hsiao, Zoubin\nGhahramani. Special thanks to Austin Tarango, Lara Tumeh, and Jason Porta for their guidance\naround responsible AI practices.\nReferences\n[1] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition\nvia large-scale weak supervision,\u201d arXiv preprint arXiv:2212.04356, 2022.\n[2] W. Chan, D. Park, C. Lee, Y. Zhang, Q. Le, and M. Norouzi, \u201cSpeechstew: Simply mix all available speech\nrecognition data to train one large neural network,\u201d arXiv preprint arXiv:2104.02133, 2021.\n[3] Y. Zhang, D. S. Park, W. Han, J. Qin, A. Gulati, J. Shor, A. Jansen, Y. Xu, Y. Huang, S. Wang, Z. Zhou, B. Li,\nM. Ma, W. Chan, J. Yu, Y. Wang, L. Cao, K. C. Sim, B. Ramabhadran, T. N. Sainath, F. Beaufays, Z. Chen,\nQ. V. Le, C.-C. Chiu, R. Pang, and Y. Wu, \u201cBigssl: Exploring the frontier of large-scale semi-supervised\nlearning for automatic speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16,\nno. 6, pp. 1519\u20131532, 2022.\n[4] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani, W. R. Huang, and M. Ma, \u201cScaling\nend-to-end models for large-scale multilingual asr,\u201d arXiv preprint arXiv:2104.14830, 2021.\n16\n[5] X. Li, F. Metze, D. R. Mortensen, A. W. Black, and S. Watanabe, \u201cAsr2k: Speech recognition for around\n2000 languages without audio,\u201d arXiv preprint arXiv:2209.02842, 2022.\n[6] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning\nof speech representations,\u201d arXiv preprint arXiv:2006.11477, 2020.\n[7] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, \u201cSelf-training with noisy student improves imagenet\nclassification,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 10 687\u201310 698.\n[8] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. V. Le, \u201cImproved noisy student\ntraining for automatic speech recognition,\u201d arXiv preprint arXiv:2005.09629, 2020.\n[9] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu et al.,\n\u201cConformer: Convolution-augmented transformer for speech recognition,\u201d arXiv preprint arXiv:2005.08100,\n2020.\n[10] C.-C. Chiu, J. Qin, Y. Zhang, J. Yu, and Y. Wu, \u201cSelf-supervised learning with random-projection quantizer\nfor speech recognition,\u201d in Proceedings of the 39th International Conference on Machine Learning,\nser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,\nG. Niu, and S. Sabato, Eds., vol. 162.\nPMLR, 17\u201323 Jul 2022, pp. 3915\u20133924. [Online]. Available:\nhttps://proceedings.mlr.press/v162/chiu22a.html\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers\nfor language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018.\n[12] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, G. Wang, and P. Moreno, \u201cInjecting text in self-\nsupervised speech pretraining,\u201d arXiv preprint arXiv:2108.12226, 2021.\n[13] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. Moreno, A. Bapna, and H. Zen, \u201cMaestro: Matched\nspeech text representations through modality matching,\u201d arXiv preprint arXiv:2204.03409, 2022.\n[14] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling\nunsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international\nconference on Machine learning, 2006, pp. 369\u2013376.\n[15] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary\nconversational speech recognition,\u201d in 2016 IEEE international conference on acoustics, speech and signal\nprocessing (ICASSP).\nIEEE, 2016, pp. 4960\u20134964.\n[16] A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and\nA. Bapna, \u201cFleurs: Few-shot learning evaluation of universal representations of speech,\u201d arXiv preprint\narXiv:2205.12446, 2022.\n[17] T. Kendall and C. Farrington, \u201cThe corpus of regional african american language. version 2021.07. eugene,\nor: The online resources for african american language project,\u201d 2021.\n[18] C. Wang, A. Wu, and J. Pino, \u201cCoVoST 2 and massively multilingual speech-to-text translation,\u201d in\ninterspeech, 2021.\n[19] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, \u201cTowards a unified view of parameter-efficient\ntransfer learning,\u201d in International Conference on Learning Representations, 2022. [Online]. Available:\nhttps://openreview.net/forum?id=0RDcd5Axok\n[20] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, and A. Conneau,\n\u201cmslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374,\n2022.\n[21] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, \u201cW2v-bert: Combining contrastive\nlearning and masked language modeling for self-supervised speech pre-training,\u201d in 2021 IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU).\nIEEE, 2021, pp. 244\u2013250.\n[22] W.-N. Hsu and J. Glass, \u201cExtracting domain invariant features by unsupervised learning for robust automatic\nspeech recognition,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2018, pp. 5614\u20135618.\n[23] Y.-A. Chung and J. Glass, \u201cSpeech2vec: A sequence-to-sequence framework for learning word embeddings\nfrom speech,\u201d arXiv preprint arXiv:1803.08976, 2018.\n[24] A. v. d. Oord, Y. Li, and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv\npreprint arXiv:1807.03748, 2018.\n[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, \u201cAn unsupervised autoregressive model for speech\nrepresentation learning,\u201d arXiv preprint arXiv:1904.03240, 2019.\n[26] J. Chorowski, R. J. Weiss, S. Bengio, and A. van den Oord, \u201cUnsupervised speech representation learning\nusing wavenet autoencoders,\u201d IEEE/ACM transactions on audio, speech, and language processing, vol. 27,\nno. 12, pp. 2041\u20132053, 2019.\n17\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\nschedule where training initially is conducted with un-transcribed speech and paired speech-text data,\nand unspoken text is utilized only after 20k steps. The joint training employing all three types of data\nlasts for another 100K steps.\nSupervised Training: We use two separate optimizers for the encoder parameters and the decoder\nparameters of the network [71]. For USM-CTC and USM-LAS, we train the model for 100k steps\nwith 2048 batch size. For in-domain experiments, the checkpoint is selected based on development\nset performance.\nTraining Large Models: We use the GShard [72] framework with the GSPMD backend [73] to\ntrain our large models on TPUs.\n3\nDatasets\n3.1\nAudio Data\nFigure 6: The video category and length distribution of YT-513-U.\nThe following audio datasets are used in this report to train our models:\n\u2022 YouTube SUPervised Plus (YT-SUP+):\n\u2013 YT-SUP: 90k hours of segmented, labeled audio across 75 languages.\n\u2013 YT-Pseudo-Labeled: 100k hours of segmented, pseudo-labeled en-US audio from YT-\nNTL-U. The pseudo-labels are generated by a 600M CTC model trained on YT-SUP\nen-US data.\n\u2022 YouTube Next Thousand Languages Unsupervised (YT-NTL-U): 12.1M hours of seg-\nmented, unlabeled audio, including:\n\u2013 YT-55-U: 12M hours of segmented, unlabeled audio on 55 rich resource languages\nidentified by YouTube production language id models.\n\u2013 YT-513-U: 100k hours of segmented, unlabeled audio across 513 tail languages not\ncovered by YouTube production language id models. These languages are identified by\nvendors.\nLet us expand upon how each dataset has been constructed.\nYT-SUP+: YT-SUP is a dataset with audio from videos that have user-uploaded transcripts from 75\nlanguages. We group consecutive segments into a longer unit similar to [57]. The maximal sequence\nlength for training is 30 seconds. The total amount of training data is 90k hours, ranging from English\n(en-US) (3.5k hours) to Amharic (Am-Et) (150 hours). We also introduce an additional 100k hours of\nen-US audio from YT-NTL-U to YT-SUP. We choose to generate pseudo-labels on this dataset using\na 600M-parameter CTC YT teacher model trained on YT-SUP. Each audio is randomly segmented\nbetween 5 to 15 seconds.\nYT-55-U: YT-55-U is built by first randomly collecting 3 million hours of audio from \"speech-heavy\"\nYouTube videos, filtered by language. The 3 million hours of audio is then further segmented by the\nYT teacher model. Instead of using a teacher model as in [3], the non-speech segments identified\nby a Voice Activity Detection (VAD) model are removed to yield approximately 1 million hours of\n10\nunlabeled audio data. Later, we use a YouTube production language identification model to select 55\nlanguages from that audio.\nYT-513-U: We create an additional dataset called YT-513-U to ensure coverage of lower resource\nlanguages in our pre-training dataset. We reached out to vendors and native speakers to identify YT\nvideos containing speech in specific long tail languages, collecting a dataset of unlabeled speech in\n513 languages. Vendors were tasked with ensuring a variety of domains, voices, and content in the\nvideos that are collected in each language. These videos are segmented into speech segments using a\nVAD model, resulting in a total of 102k hours of speech. Our final YT-513-U dataset contains 88\nlanguages with over 500 hours of speech each, 237 languages with between 100-500 hours, and 188\nlanguages with less than 100 hours of data. The languages chosen for this collection are wide-ranging,\nwith a majority of our data corresponding to languages from South Asia, Southeast Asia, West Africa,\nand East Africa. The distribution of video categories and lengths in our dataset are depicted in\nFigure 6.\nIn addition to YouTube data, we also include public data for MOST training:\n\u2022 Public Unsupervised (Pub-U): Following [20], we use approximately 429k hours of\nunlabeled speech data in 51 languages. It includes: 372k hours of speech data spanning\n23 languages from VoxPopuli [74], read speech data in 25 languages drawn from the v6.1\nrelease of Common Voice [75], 50k hours of read books data in eight European languages\nfrom Multilingual LibriSpeech [76] and 1k hours of telephonic conversation data spanning\n17 African and Asian languages from BABEL [77].\n\u2022 Public Supervised (Pub-S): Similar to [20], our public supervised set includes approxi-\nmately 1.3k hours of speech and transcript data spanning 14 languages from VoxPopuli,\n10 hour training splits for each of the 8 MLS languages, and 1k hours of data spanning 17\nlanguages from the Babel ASR task.\nNote that the public data is only used for in-domain pre-training and is excluded for training the\ngeneric USM-LAS/CTC models. This allows us to treat the public task performance as out-of-domain\nbenchmarks for the USM-LAS/CTC models.\n3.2\nText Data\nWeb-NTL: For pre-training with unlabeled text, we use a web-crawled corpus of monolingual text\ncontaining over 28B sentences [78]. The dataset spans 1140 languages, 205 of which have over 1M\nsentences and 199 of which have between 100k and 1M sentences. We up-sample lower resource\nlanguages using temperature-based sampling [79] with T = 3.0. More details about the dataset and\nthe mining approach have been described in Section 2 of [78].\n3.3\nDownstream Benchmarks\n3.3.1\nSpeech Recognition (ASR)\nWe present our results on two public tasks, SpeechStew [2] and FLEURS [16], and an internal\nbenchmark on YouTube.\nThe SpeechStew [2] dataset is assembled by putting together seven public speech corpora\u2014AMI [80],\nCommon Voice [81], English Broadcast News3, LibriSpeech [82], Switchboard/Fisher4, TED-LIUM\nv3 [83,84] and Wall Street Journal5, which are all standard benchmarks [85\u201387] covering different\ndomains in en-US.\nThe FLEURS [16] dataset is a publicly available, multi-way parallel dataset of 10 hours of read\nspeech in 102 languages spanning 7 geo-groups. We restrict our use of the dataset to its ASR\nbenchmark. Among the 102 languages present in the FLEURS benchmark, we select 62 to serve as a\nsub-group to compare our generic ASR system with Whisper [1], as those languages are covered by\nthe training sets of both models. We also report full results for in-domain fine-tuning and adaptation.\nUnlike [16], we report both WER and CER metrics, as CER is inappropriate as an indicator of\n3Linguistic data consortium (LDC) datasets LDC97S44, LDC97T22, LDC98S71 and LDC98T28.\n4LDC datasets LDC2004T19, LDC2005T19, LDC2004S13, LDC2005S13 and LDC97S62.\n5LDC datasets LDC93S6B and LDC94S13B.\n11\nTable 3: WERs (%) across multiple tasks for multiple settings compared against pre-existing baselines,\nwith the exception of CoVoST 2, for which the BLEU score is presented. For the YouTube long-form\nset, we select the top-25 languages Whisper was trained on and exclude all languages for which\nWhisper produces > 40% WER to reduce the noise introduced by LAS hallucination in the Whisper\nmodel. For FLEURS, we report both the WER and the CER for our models. \u2020Results omitted for the\nWhisper-shortform model on the YouTube long-form dataset as the model has a high deletion problem\non this set. \u2021The Whisper-shortform model uses segmented decoding to reduce its hallucination\nproblem on CORAAL. \u00a7Our adapter setup adds about 2.3% of the total parameters while keeping the\nencoder frozen from pre-training.\nTask\nMultilingual Long-form ASR\nMultidomain en-US\nMultilingual ASR\nAST\nDataset\nYouTube\nCORAAL\nSpeechStew\nFLEURS\nCoVoST 2\nLangauges\nen-US\n18\n73\nen-US\nen-US\n62\n102\n21\nPrior Work (single model)\nWhisper-longform\n17.7\n27.8\n-\n23.9\n12.8\nWhisper-shortform\u2020\n-\n-\n-\n13.2\u2021\n11.5\n36.6\n-\n29.1\nOur Work (single model)\nUSM-LAS\n14.4\n19.0\n29.8\n11.2\n10.5\n12.5\n-\n-\nUSM-CTC\n13.7\n18.7\n26.7\n12.1\n10.8\n15.5\n-\n-\nPrior Work (in-domain fine-tuning)\nBigSSL [3]\n14.8\n-\n-\n-\n7.5\n-\n-\n-\nMaestro [67]\n7.2\n25.2\nMaestro-U [67]\n26.0 (8.7)\nOur Work (in-domain fine-tuning)\nUSM\n13.2\n-\n-\n-\n7.4\n13.5\n19.2 (6.9)\n28.7\nUSM-M\n12.5\n-\n-\n-\n7.0\n11.8\n17.4 (6.5)\n30.7\nOur Work (frozen encoder)\nUSM-M-adapter\u00a7\n-\n-\n-\n-\n7.5\n12.4\n17.6 (6.7)\n29.6\nperformance for some languages. When presenting the error rate metrics, we use CER for Chinese,\nJapanese, Thai, Lao, and Burmese to be consistent with Whisper [1].\nThe test set for the YouTube domain consists of utterances from 73 languages with an average of\n15 hours of audio per language, the audio length for each individual language ranging from 1 to 24\nhours. The audio is transcribed manually from popular YouTube videos, each with a duration of up to\n30 minutes.\n3.3.2\nSpeech Translation (AST)\nFollowing [20], we use CoVoST 2 [18] to benchmark multilingual speech translation. We evaluate\nthe multilingual XX-to-English task that covers translation from 21 source languages into English.\nDepending on the language, the training data ranges in size from 1 - 264 hours.\nBesides speech translation data, we also add text-to-text translation data for training the model as\nin [20]. This dataset includes the text translation data from CoVoST 2 combined with all data from\neither WMT or TED Talks, as available.\n4\nKey Results\n4.1\nRobust Speech Recognition for Massively Multilingual Tasks\nIn this section, we compare the performance of our models against public baselines, including\nWhisper large-v26 [1], which has been trained on 680k hours of weakly supervised data across 100\nlanguages.\nFor the massively multilingual speech recognition test dataset from YouTube, we observe that Whisper\nhallucinates in many languages, resulting in a WER exceeding 100%. For a reasonable comparison,\nwe restrict the language set on which we compare the performance USM against Whisper by first\nselecting the top-25 languages from the training data for Whisper and further excluding languages for\nwhich Whisper produces > 40% WER. We also use segmented decoding for Whisper with 30-second\nsegments to further reduce the effect of hallucinations. As shown in Table 3, our USM-LAS and\n6Whisper large-v2 on Github (https://github.com/openai/whisper.git, revision b4308c4) is used for evaluation.\n12\nUSM-CTC models outperform Whisper by a wide margin on YouTube en-US, despite training on\nsignificantly less supervised data (3.5k hours versus Whisper\u2019s 400k hours [1]). While the USM-LAS\nmodel also requires segmented decoding to reduce long-form degradation as discussed in section\n2.4, it is far more robust, out-performing Whisper by a relative 30% WER on those 18 languages.\nUSM-CTC does not exhibit long-form performance degradation and achieves the best performance\non YouTube.\nOn the out-of-domain long-form CORAAL set, both USM-CTC and USM-LAS outperform Whisper\nby more than 10% relative WER. USM-CTC and USM-LAS similary outperform Whisper on\nSpeechStew, whose training data the models have not had access to.\nWe further compare the multilingual performance of the models on the held-out set from FLEURS.\nAs shown in Table 3, USM-LAS and USM-CTC both outperform Whisper by 66% relative WER,\ndespite using a smaller amount of multilingual supervised data (90k versus Whisper\u2019s 117k, when\nen-US is excluded). USM-LAS consistently outperforms USM-CTC for short-form ASR tasks.\n4.2\nMassively Multilingual Results Beyond 100 Languages\nThe lower part of Table 3 shows our results for in-domain fine-tuning. Our pre-trained model improves\nthe FLEURS benchmark significantly, even when using only 10 hours per language. Compared to the\nprevious SoTA in [67], our model achieves a 30% relative improvement in terms of WER across 102\nlanguages. Our results show that while generic speech models can be powerful, performance is still\nmaximized by in-domain fine-tuning.\n4.3\nMOST Produces Robust Representations that Generalize to New Domains\nMOST training aligns the representations of speech and text by training simultaneously on the two\nmodalities. We investigate whether MOST representations are useful for adapting the model to new\ndomains by freezing the entire learned encoder produced by MOST and adjusting a small amount of\nparameters added to the network by residual adapters. As shown in Table 3, by adding only 2% to\nthe total number of parameters, the MOST representation model (USM-M-adapter) only performs\nslightly worse than the fine-tuning baselines, still showing competitive performance on downstream\nASR and AST tasks. The small number of parameters being trained in this approach makes it feasible\nto extend our system to a large number of new domains and new tasks, even with a limited amount of\ntraining data, such as in FLEURS.\n4.4\nPushing the Quality of ASR on Unseen Languages\nTable 4: Noisy student training for unseen languages. WERs (%) for the teacher adapter models and\nthe student models are presented. The relative improvement (%) of the student models can be found\nin the last column.\nLanguages\nWhisper-v2\n# hrs in YT-NTL\nUSM-LAS-Adapter\nUSM-M + pseudo label\nRel. Imprv.\nHausa (ha)\n88.9\n2175.0\n24.5\n22.8\n7.5\nKazakh (kk)\n37.7\n196.0\n11.8\n10.9\n8.3\nShona (sn)\n121.0\n247.0\n29.1\n22.2\n31.1\nPashto (ps)\n93.7\n254.0\n36.0\n35.4\n1.7\nYoruba (yo)\n94.8\n1292.0\n33.4\n30.6\n9.2\nTail languages often do not have paired transcriptions for supervised learning\u2014we refer to these\nlanguages as unseen languages, as the model has not seen paired data for these lanugages during\ntraining. To create pseudo-labels for these languages, we first build a USM-LAS-Adapter by attaching\nresidual adapters to USM-LAS and training them using FLEURS data. By using the USM-LAS-\nAdapter as a teacher, we can now transcribe the unlabeled data in the unseen languages as part\nof the YT-NTL dataset. As shown in Table 4, we observe consistent wins for all languages on\nthe FLEURS benchmark. For some languages, the improvement is larger than 30%. This further\ndemonstrates the robustness of the USM-LAS model\u2014despite using only 10 hours of out of domain\ndata from FLEURS, the USM-LAS-Adapter is able to transcribe YouTube data to produce meaningful\nrecognition results that lead to these improvements. We find the approach of training adapter models\n13\non small datasets and utilizing them for pseudo-labeling to be a promising route for scaling up the\nnumber of languages that can be transcribed by USMs.\n4.5\nUSMs are Strong AST Models\nThe multi-lingual speech translation performance of fine-tuned USMs are shown in Table 3. We\nfind that we are already comparable to the CoVoST 2 SoTA BLEU score by fine-tuning the speech-\nonly USM. We note that the previous SoTA uses 125k hours of supervised speech translation data\ncompared to the 859 hours of data used by the USM. After MOST training, USM-M can use both\nspeech and text as training input. By introducing text-to-text machine translation (MT) data during\nfine-tuning, USM-M is able to achieve an unprecedented > 30 BLEU on CoVoST (a 1 BLEU increase\nfrom SoTA).\n5\nAnalysis and Ablations\n5.1\nMulti-Softmax Loss for BEST-RQ\nWe observe a consistent > 5% relative improvement in ASR and AST benchmarks by increasing\nthe number of the softmax groups in the multi-softmax loss for BEST-RQ training from 1 to 16, as\nshown in Table 5. We also find that using multiple softmax groups significantly reduces performance\nvariation across different pre-training runs and improves convergence speed.\nTable 5: YT-55 versus YT-NTL across different domains, with and without multi-softmax groups. For\nsimplicity, we report CER for FLEURS. For CoVoST, we report the BLEU score. YT-NTL covers 27\nadditional languages not covered in YT-55.\nModel\npre-train Set\n# Params (B)\n# Softmax\nFLEURS (CER)\nCoVoST (BLEU)\n.\n102 langs\n27 langs\nConformer-0.6B\nYT-55\n0.6\n1\n9.5\n-\n20.9\nConformer-2B\nYT-55\n2.0\n1\n7.9\n9.5\n26.6\nConformer-2B\nYT-NTL-U\n2.0\n1\n7.4\n8.5\n27.5\nConformer-2B\nYT-NTL-U\n2.0\n16\n6.9\n8.1\n28.7\n5.2\nModel and Language Scaling\nWe find that scaling up the model size and increasing the language coverage of the pre-training\ndataset greatly benefits the performance of the USMs, as demonstrated in Table 5. In particular, we\nfind a 10% relative improvement of ASR and AST performance by using YT-NTL vs. YT-55 for\npre-training, despite the fact that each newly added language in YT-NTL contains approximately 500\nhours of speech\u2014a relatively small amount. As could be expected, the relative gains on the newly\ncovered languages are more substantial than those on other languages.\n5.3\nBEST-RQ is a Scalable Self-supervised Learner\nBEST-RQ has been shown to outperform or be comparable to other prominent pre-training methods\nfor speech recognition, including wav2vec 2.0 and W2v-BERT in the original work in which it was\nintroduced [10]. Here we investigate its comparative performance and scaling properties, similar\nto what has been done for wav2vec 2.0 in [3] and W2v-BERT in [20]. We utilize the set-up of\npre-training the model using YT-55 and fine-tuning it on CoVoST 2. As shown in Table 6, our results\nindicate that for the Conformer-0.6B, W2v-BERT and BEST-RQ perform similarly, but BEST-RQ\nobtains greater gains when scaled up. A contributing factor to this can be that W2v-BERT is more\nprone to codebook collapse and training instabilities at the 2B scale, while BEST-RQ by construction\ndoesn\u2019t suffer from codebook collapse.\n5.4\nChunk-wise attention for robust long-form speech recognition\nFig. 7 depicts the long-form performance degradation issue as described in section 2.4. In the figure,\nwe see that for the shallow Conformer model with 17 layers, using a small local self attention context\n14\nTable 6: BLEU scores for the CoVoST 2 X \u2192En task to compare BEST-RQ against W2v-BERT.\nHigher is better.\nX \u2192English\nhigh\nmid\nlow\nall\nPrevious Work\nXLS-R (0.3B) [33]\n30.6\n18.9\n5.1\n13.2\nXLS-R (1B) [33]\n34.3\n25.5\n11.7\n19.3\nXLS-R (2B) [33]\n36.1\n27.7\n15.1\n22.1\nConformer-0.6B\nW2v-BERT\n35.6\n25.3\n13.4\n20.4\nBEST-RQ\n32.5\n25.6\n14.7\n20.7\nConformer-2B\nW2v-BERT\n36.0\n27.8\n15.6\n22.4\nBEST-RQ\n35.8\n31.3\n21.5\n26.6\nFigure 7: The word error rate measured on the YouTube en-US long-form test set for Conformer\nmodels with varying depth.\n(65) length, the word error rate measured on the long-form test set gradually improves as the training\nprogresses. With a deeper model that has 48 layers but roughly the same number of parameters,\nhowever, the larger receptive field mismatch results in higher test WERs as the training step increases.\nTable 7 demonstrates that chunk-wise attention is able to address the long-form degradation issue and\nshow robust performance across four different languages\u2014en-US (English), ru-RU (Russian), ko-KR\n(Korean), and uk-UA (Ukrainian). We compare chunk-wise attention models with an 8-second chunk\nsize (CW-8s in Table 7) against local self attention models which uses 128 context frames in each\nconformer layer (LSA-128). We note that further increasing the context window size of the local self\nattention model results in high deletion error rates on all languages of the YouTube long-form test\nsets. These results show that the chunk-wise attention models do not exhibit long-form performance\ndegradation and are able to improve upon the performance of the local self attention models operating\nat the maximum allowed receptive field length.\nTable 7: Chunk-wise attention. WER (%) is reported on the YouTube long-form set.\nModel\n# Params (B)\n# Layers\nen-US\nru-RU\nko-KR\nuk-UA\nLSA-128\n0.6\n24\n16.2\n16.6\n26.2\n15.5\nCW-8s\n0.6\n24\n12.5\n14.7\n19.5\n15.3\n5.5\nTPU Serving Capacity of USM-CTC Models\nIn section 4, we have demonstrated that USM-CTC models are powerful generic ASR models\nwith reliable long-form transcription performance and excellent generalization properties. Here we\n15\nTable 8: RTF for USM-2B.\nModel\nbf-16\nStreaming\n# Params (B)\nTPU [88]\nBatch Size\n1.0/RTF\nConformer-0.1B\nY\nY\n0.1\nTPUv4i\n64\n3047\nConformer-0.6B\nN\nN\n0.6\nTPUv4i\n64\n1920\nConformer-2B\nN\nN\n2.0\nTPUv4i\n32\n827\nmeasure the serving capacity of the USM-CTC model as represented by the real time factor (RTF) in\nan ideal setup where we assume that each batch sent to TPU is fully packed along the time axis. The\nresults of these measurements are presented in Table 8. Surprisingly, we find that the 2B-paramter\nUSM-CTC model is only 3.9\u00d7 slower than the 100M-parameter streaming model [89], primarily\ndue to the fact that our models operate at batch processing mode. This result demonstrates that the\nUSM-CTC can be used as an offline transcriber efficiently on TPUs (or GPUs).\n6\nDiscussion\nIn this report, we put forward a practical and flexible approach for training speech understanding\nmodels capable of scaling speech recognition to hundreds of languages. We conclude the report with\nsummarizing insights gained in the process:\nUnlabeled versus weakly labeled data: We believe diverse unlabeled data is more practical to\nacquire for building usable ASR for tail languages than weakly labeled data. We have demonstrated\nthat collaborating with native speakers to identify unsupervised data in hundreds of tail languages\ncan be an effective route to improving recognition performance on low resource languages.\nIn-domain data is best: We have demonstrated that we can build a robust ASR system across many\ndomains by utilizing a large amount of unsupervised data and a small amount of labeled data. Our\nresults, however, also confirm that the most effective way to optimize the performance for a given\ndomain is to use in-domain data to fine-tune the model.\nCTC vs RNN-T vs LAS: The best transducer depends on the downstream task. A large pre-trained\nmodel with a frozen encoder can allow experimenters to test different transducers quickly and select\nthe optimal transducer for their purpose.\nAcknowledgments\nWe would like to thank Alexis Conneau, Min Ma, Shikhar Bharadwaj, Sid Dalmia, Jiahui Yu, Jian\nCheng, Paul Rubenstein, Ye Jia, Justin Snyder, Vincent Tsang, Yuanzhong Xu, Tao Wang, Anusha\nRamesh, Calum Barnes, Salem Haykal for useful discussions.\nWe appreciate valuable feedback and support from Eli Collins, Jeff Dean, Sissie Hsiao, Zoubin\nGhahramani. Special thanks to Austin Tarango, Lara Tumeh, and Jason Porta for their guidance\naround responsible AI practices.\nReferences\n[1] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition\nvia large-scale weak supervision,\u201d arXiv preprint arXiv:2212.04356, 2022.\n[2] W. Chan, D. Park, C. Lee, Y. Zhang, Q. Le, and M. Norouzi, \u201cSpeechstew: Simply mix all available speech\nrecognition data to train one large neural network,\u201d arXiv preprint arXiv:2104.02133, 2021.\n[3] Y. Zhang, D. S. Park, W. Han, J. Qin, A. Gulati, J. Shor, A. Jansen, Y. Xu, Y. Huang, S. Wang, Z. Zhou, B. Li,\nM. Ma, W. Chan, J. Yu, Y. Wang, L. Cao, K. C. Sim, B. Ramabhadran, T. N. Sainath, F. Beaufays, Z. Chen,\nQ. V. Le, C.-C. Chiu, R. Pang, and Y. Wu, \u201cBigssl: Exploring the frontier of large-scale semi-supervised\nlearning for automatic speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16,\nno. 6, pp. 1519\u20131532, 2022.\n[4] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani, W. R. Huang, and M. Ma, \u201cScaling\nend-to-end models for large-scale multilingual asr,\u201d arXiv preprint arXiv:2104.14830, 2021.\n16\n[5] X. Li, F. Metze, D. R. Mortensen, A. W. Black, and S. Watanabe, \u201cAsr2k: Speech recognition for around\n2000 languages without audio,\u201d arXiv preprint arXiv:2209.02842, 2022.\n[6] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning\nof speech representations,\u201d arXiv preprint arXiv:2006.11477, 2020.\n[7] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, \u201cSelf-training with noisy student improves imagenet\nclassification,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 10 687\u201310 698.\n[8] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. V. Le, \u201cImproved noisy student\ntraining for automatic speech recognition,\u201d arXiv preprint arXiv:2005.09629, 2020.\n[9] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu et al.,\n\u201cConformer: Convolution-augmented transformer for speech recognition,\u201d arXiv preprint arXiv:2005.08100,\n2020.\n[10] C.-C. Chiu, J. Qin, Y. Zhang, J. Yu, and Y. Wu, \u201cSelf-supervised learning with random-projection quantizer\nfor speech recognition,\u201d in Proceedings of the 39th International Conference on Machine Learning,\nser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,\nG. Niu, and S. Sabato, Eds., vol. 162.\nPMLR, 17\u201323 Jul 2022, pp. 3915\u20133924. [Online]. Available:\nhttps://proceedings.mlr.press/v162/chiu22a.html\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers\nfor language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018.\n[12] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, G. Wang, and P. Moreno, \u201cInjecting text in self-\nsupervised speech pretraining,\u201d arXiv preprint arXiv:2108.12226, 2021.\n[13] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. Moreno, A. Bapna, and H. Zen, \u201cMaestro: Matched\nspeech text representations through modality matching,\u201d arXiv preprint arXiv:2204.03409, 2022.\n[14] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling\nunsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international\nconference on Machine learning, 2006, pp. 369\u2013376.\n[15] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary\nconversational speech recognition,\u201d in 2016 IEEE international conference on acoustics, speech and signal\nprocessing (ICASSP).\nIEEE, 2016, pp. 4960\u20134964.\n[16] A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and\nA. Bapna, \u201cFleurs: Few-shot learning evaluation of universal representations of speech,\u201d arXiv preprint\narXiv:2205.12446, 2022.\n[17] T. Kendall and C. Farrington, \u201cThe corpus of regional african american language. version 2021.07. eugene,\nor: The online resources for african american language project,\u201d 2021.\n[18] C. Wang, A. Wu, and J. Pino, \u201cCoVoST 2 and massively multilingual speech-to-text translation,\u201d in\ninterspeech, 2021.\n[19] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, \u201cTowards a unified view of parameter-efficient\ntransfer learning,\u201d in International Conference on Learning Representations, 2022. [Online]. Available:\nhttps://openreview.net/forum?id=0RDcd5Axok\n[20] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, and A. Conneau,\n\u201cmslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374,\n2022.\n[21] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, \u201cW2v-bert: Combining contrastive\nlearning and masked language modeling for self-supervised speech pre-training,\u201d in 2021 IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU).\nIEEE, 2021, pp. 244\u2013250.\n[22] W.-N. Hsu and J. Glass, \u201cExtracting domain invariant features by unsupervised learning for robust automatic\nspeech recognition,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2018, pp. 5614\u20135618.\n[23] Y.-A. Chung and J. Glass, \u201cSpeech2vec: A sequence-to-sequence framework for learning word embeddings\nfrom speech,\u201d arXiv preprint arXiv:1803.08976, 2018.\n[24] A. v. d. Oord, Y. Li, and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv\npreprint arXiv:1807.03748, 2018.\n[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, \u201cAn unsupervised autoregressive model for speech\nrepresentation learning,\u201d arXiv preprint arXiv:1904.03240, 2019.\n[26] J. Chorowski, R. J. Weiss, S. Bengio, and A. van den Oord, \u201cUnsupervised speech representation learning\nusing wavenet autoencoders,\u201d IEEE/ACM transactions on audio, speech, and language processing, vol. 27,\nno. 12, pp. 2041\u20132053, 2019.\n17\n[27] S. Schneider, A. Baevski, R. Collobert, and M. Auli, \u201cwav2vec: Unsupervised pre-training for speech\nrecognition,\u201d arXiv preprint arXiv:1904.05862, 2019.\n[28] A. Baevski, S. Schneider, and M. Auli, \u201cvq-wav2vec: Self-supervised learning of discrete speech represen-\ntations,\u201d arXiv preprint arXiv:1910.05453, 2019.\n[29] S. Ling, Y. Liu, J. Salazar, and K. Kirchhoff, \u201cDeep contextualized acoustic representations for semi-\nsupervised speech recognition,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nIEEE, 2020, pp. 6429\u20136433.\n[30] A. Baevski, M. Auli, and A. Mohamed, \u201cEffectiveness of self-supervised pre-training for speech recogni-\ntion,\u201d arXiv preprint arXiv:1911.03912, 2019.\n[31] M. Riviere, A. Joulin, P.-E. Mazar\u00e9, and E. Dupoux, \u201cUnsupervised pretraining transfers well across\nlanguages,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2020, pp. 7414\u20137418.\n[32] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. v. d. Oord, \u201cLearning robust and multilingual speech\nrepresentations,\u201d arXiv preprint arXiv:2001.11128, 2020.\n[33] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino\net al., \u201cXls-r: Self-supervised cross-lingual speech representation learning at scale,\u201d arXiv preprint\narXiv:2111.09296, 2021.\n[34] G. Zavaliagkos and T. Colthurst, \u201cUtilizing untranscribed training data to improve performance,\u201d in DARPA\nBroadcast News Transcription and Understanding Workshop, Landsdowne, 1998, pp. 301\u2013305.\n[35] L. Lamel, J. luc Gauvain, and G. Adda, \u201cLightly supervised acoustic model training,\u201d in Proc. ISCA ITRW\nASR2000, 2000, pp. 150\u2013154.\n[36] S. Novotney and R. Schwartz, \u201cAnalysis of low-resource acoustic model self-training,\u201d in Tenth Annual\nConference of the International Speech Communication Association, 2009.\n[37] S. Thomas, M. L. Seltzer, K. Church, and H. Hermansky, \u201cDeep neural network features and semi-\nsupervised training for low resource speech recognition,\u201d in 2013 IEEE international conference on\nacoustics, speech and signal processing.\nIEEE, 2013, pp. 6704\u20136708.\n[38] B. Li, T. N. Sainath, R. Pang, and Z. Wu, \u201cSemi-supervised training for end-to-end models via weak\ndistillation,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2019, pp. 2837\u20132841.\n[39] J. Kahn, A. Lee, and A. Hannun, \u201cSelf-training for end-to-end speech recognition,\u201d in ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp.\n7084\u20137088.\n[40] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave, V. Pratap, A. Sriram, V. Liptchinsky, and\nR. Collobert, \u201cEnd-to-end asr: from supervised to semi-supervised learning with modern architectures,\u201d in\narXiv, 2019.\n[41] S. H. K. Parthasarathi and N. Strom, \u201cLessons from building acoustic models with a million hours of\nspeech,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2019, pp. 6670\u20136674.\n[42] W.-N. Hsu, A. Lee, G. Synnaeve, and A. Hannun, \u201cSemi-supervised speech recognition via local prior\nmatching,\u201d arXiv preprint arXiv:2002.10336, 2020.\n[43] Q. Xu, T. Likhomanenko, J. Kahn, A. Hannun, G. Synnaeve, and R. Collobert, \u201cIterative pseudo-labeling\nfor speech recognition,\u201d arXiv preprint arXiv:2005.09267, 2020.\n[44] Z. Chen, A. Rosenberg, Y. Zhang, H. Zen, M. Ghodsi, Y. Huang, J. Emond, G. Wang, B. Ramabhadran, and\nP. J. Moreno, \u201cSemi-Supervision in ASR: Sequential MixMatch and Factorized TTS-Based Augmentation,\u201d\nin Proc. Interspeech 2021, 2021, pp. 736\u2013740.\n[45] A. Renduchintala, S. Ding, M. Wiesner, and S. Watanabe, \u201cMulti-modal data augmentation for end-to-end\nasr,\u201d arXiv preprint arXiv:1803.10299, 2018.\n[46] A. Bapna, Y.-a. Chung, N. Wu, A. Gulati, Y. Jia, J. H. Clark, M. Johnson, J. Riesa, A. Conneau, and\nY. Zhang, \u201cSlam: A unified encoder for speech and language modeling via speech-text joint pre-training,\u201d\narXiv preprint arXiv:2110.10329, 2021.\n[47] S. Thomas, B. Kingsbury, G. Saon, and H.-K. J. Kuo, \u201cIntegrating text inputs for training and adapting rnn\ntransducer asr models,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2022, pp. 8127\u20138131.\n[48] Y. Cheng, Y. Zhang, M. Johnson, W. Macherey, and A. Bapna, \u201cMu2slam: Multitask, multilingual speech\nand language models,\u201d arXiv preprint arXiv:2212.09553, 2022.\n18\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\nschedule where training initially is conducted with un-transcribed speech and paired speech-text data,\nand unspoken text is utilized only after 20k steps. The joint training employing all three types of data\nlasts for another 100K steps.\nSupervised Training: We use two separate optimizers for the encoder parameters and the decoder\nparameters of the network [71]. For USM-CTC and USM-LAS, we train the model for 100k steps\nwith 2048 batch size. For in-domain experiments, the checkpoint is selected based on development\nset performance.\nTraining Large Models: We use the GShard [72] framework with the GSPMD backend [73] to\ntrain our large models on TPUs.\n3\nDatasets\n3.1\nAudio Data\nFigure 6: The video category and length distribution of YT-513-U.\nThe following audio datasets are used in this report to train our models:\n\u2022 YouTube SUPervised Plus (YT-SUP+):\n\u2013 YT-SUP: 90k hours of segmented, labeled audio across 75 languages.\n\u2013 YT-Pseudo-Labeled: 100k hours of segmented, pseudo-labeled en-US audio from YT-\nNTL-U. The pseudo-labels are generated by a 600M CTC model trained on YT-SUP\nen-US data.\n\u2022 YouTube Next Thousand Languages Unsupervised (YT-NTL-U): 12.1M hours of seg-\nmented, unlabeled audio, including:\n\u2013 YT-55-U: 12M hours of segmented, unlabeled audio on 55 rich resource languages\nidentified by YouTube production language id models.\n\u2013 YT-513-U: 100k hours of segmented, unlabeled audio across 513 tail languages not\ncovered by YouTube production language id models. These languages are identified by\nvendors.\nLet us expand upon how each dataset has been constructed.\nYT-SUP+: YT-SUP is a dataset with audio from videos that have user-uploaded transcripts from 75\nlanguages. We group consecutive segments into a longer unit similar to [57]. The maximal sequence\nlength for training is 30 seconds. The total amount of training data is 90k hours, ranging from English\n(en-US) (3.5k hours) to Amharic (Am-Et) (150 hours). We also introduce an additional 100k hours of\nen-US audio from YT-NTL-U to YT-SUP. We choose to generate pseudo-labels on this dataset using\na 600M-parameter CTC YT teacher model trained on YT-SUP. Each audio is randomly segmented\nbetween 5 to 15 seconds.\nYT-55-U: YT-55-U is built by first randomly collecting 3 million hours of audio from \"speech-heavy\"\nYouTube videos, filtered by language. The 3 million hours of audio is then further segmented by the\nYT teacher model. Instead of using a teacher model as in [3], the non-speech segments identified\nby a Voice Activity Detection (VAD) model are removed to yield approximately 1 million hours of\n10\nunlabeled audio data. Later, we use a YouTube production language identification model to select 55\nlanguages from that audio.\nYT-513-U: We create an additional dataset called YT-513-U to ensure coverage of lower resource\nlanguages in our pre-training dataset. We reached out to vendors and native speakers to identify YT\nvideos containing speech in specific long tail languages, collecting a dataset of unlabeled speech in\n513 languages. Vendors were tasked with ensuring a variety of domains, voices, and content in the\nvideos that are collected in each language. These videos are segmented into speech segments using a\nVAD model, resulting in a total of 102k hours of speech. Our final YT-513-U dataset contains 88\nlanguages with over 500 hours of speech each, 237 languages with between 100-500 hours, and 188\nlanguages with less than 100 hours of data. The languages chosen for this collection are wide-ranging,\nwith a majority of our data corresponding to languages from South Asia, Southeast Asia, West Africa,\nand East Africa. The distribution of video categories and lengths in our dataset are depicted in\nFigure 6.\nIn addition to YouTube data, we also include public data for MOST training:\n\u2022 Public Unsupervised (Pub-U): Following [20], we use approximately 429k hours of\nunlabeled speech data in 51 languages. It includes: 372k hours of speech data spanning\n23 languages from VoxPopuli [74], read speech data in 25 languages drawn from the v6.1\nrelease of Common Voice [75], 50k hours of read books data in eight European languages\nfrom Multilingual LibriSpeech [76] and 1k hours of telephonic conversation data spanning\n17 African and Asian languages from BABEL [77].\n\u2022 Public Supervised (Pub-S): Similar to [20], our public supervised set includes approxi-\nmately 1.3k hours of speech and transcript data spanning 14 languages from VoxPopuli,\n10 hour training splits for each of the 8 MLS languages, and 1k hours of data spanning 17\nlanguages from the Babel ASR task.\nNote that the public data is only used for in-domain pre-training and is excluded for training the\ngeneric USM-LAS/CTC models. This allows us to treat the public task performance as out-of-domain\nbenchmarks for the USM-LAS/CTC models.\n3.2\nText Data\nWeb-NTL: For pre-training with unlabeled text, we use a web-crawled corpus of monolingual text\ncontaining over 28B sentences [78]. The dataset spans 1140 languages, 205 of which have over 1M\nsentences and 199 of which have between 100k and 1M sentences. We up-sample lower resource\nlanguages using temperature-based sampling [79] with T = 3.0. More details about the dataset and\nthe mining approach have been described in Section 2 of [78].\n3.3\nDownstream Benchmarks\n3.3.1\nSpeech Recognition (ASR)\nWe present our results on two public tasks, SpeechStew [2] and FLEURS [16], and an internal\nbenchmark on YouTube.\nThe SpeechStew [2] dataset is assembled by putting together seven public speech corpora\u2014AMI [80],\nCommon Voice [81], English Broadcast News3, LibriSpeech [82], Switchboard/Fisher4, TED-LIUM\nv3 [83,84] and Wall Street Journal5, which are all standard benchmarks [85\u201387] covering different\ndomains in en-US.\nThe FLEURS [16] dataset is a publicly available, multi-way parallel dataset of 10 hours of read\nspeech in 102 languages spanning 7 geo-groups. We restrict our use of the dataset to its ASR\nbenchmark. Among the 102 languages present in the FLEURS benchmark, we select 62 to serve as a\nsub-group to compare our generic ASR system with Whisper [1], as those languages are covered by\nthe training sets of both models. We also report full results for in-domain fine-tuning and adaptation.\nUnlike [16], we report both WER and CER metrics, as CER is inappropriate as an indicator of\n3Linguistic data consortium (LDC) datasets LDC97S44, LDC97T22, LDC98S71 and LDC98T28.\n4LDC datasets LDC2004T19, LDC2005T19, LDC2004S13, LDC2005S13 and LDC97S62.\n5LDC datasets LDC93S6B and LDC94S13B.\n11\nTable 3: WERs (%) across multiple tasks for multiple settings compared against pre-existing baselines,\nwith the exception of CoVoST 2, for which the BLEU score is presented. For the YouTube long-form\nset, we select the top-25 languages Whisper was trained on and exclude all languages for which\nWhisper produces > 40% WER to reduce the noise introduced by LAS hallucination in the Whisper\nmodel. For FLEURS, we report both the WER and the CER for our models. \u2020Results omitted for the\nWhisper-shortform model on the YouTube long-form dataset as the model has a high deletion problem\non this set. \u2021The Whisper-shortform model uses segmented decoding to reduce its hallucination\nproblem on CORAAL. \u00a7Our adapter setup adds about 2.3% of the total parameters while keeping the\nencoder frozen from pre-training.\nTask\nMultilingual Long-form ASR\nMultidomain en-US\nMultilingual ASR\nAST\nDataset\nYouTube\nCORAAL\nSpeechStew\nFLEURS\nCoVoST 2\nLangauges\nen-US\n18\n73\nen-US\nen-US\n62\n102\n21\nPrior Work (single model)\nWhisper-longform\n17.7\n27.8\n-\n23.9\n12.8\nWhisper-shortform\u2020\n-\n-\n-\n13.2\u2021\n11.5\n36.6\n-\n29.1\nOur Work (single model)\nUSM-LAS\n14.4\n19.0\n29.8\n11.2\n10.5\n12.5\n-\n-\nUSM-CTC\n13.7\n18.7\n26.7\n12.1\n10.8\n15.5\n-\n-\nPrior Work (in-domain fine-tuning)\nBigSSL [3]\n14.8\n-\n-\n-\n7.5\n-\n-\n-\nMaestro [67]\n7.2\n25.2\nMaestro-U [67]\n26.0 (8.7)\nOur Work (in-domain fine-tuning)\nUSM\n13.2\n-\n-\n-\n7.4\n13.5\n19.2 (6.9)\n28.7\nUSM-M\n12.5\n-\n-\n-\n7.0\n11.8\n17.4 (6.5)\n30.7\nOur Work (frozen encoder)\nUSM-M-adapter\u00a7\n-\n-\n-\n-\n7.5\n12.4\n17.6 (6.7)\n29.6\nperformance for some languages. When presenting the error rate metrics, we use CER for Chinese,\nJapanese, Thai, Lao, and Burmese to be consistent with Whisper [1].\nThe test set for the YouTube domain consists of utterances from 73 languages with an average of\n15 hours of audio per language, the audio length for each individual language ranging from 1 to 24\nhours. The audio is transcribed manually from popular YouTube videos, each with a duration of up to\n30 minutes.\n3.3.2\nSpeech Translation (AST)\nFollowing [20], we use CoVoST 2 [18] to benchmark multilingual speech translation. We evaluate\nthe multilingual XX-to-English task that covers translation from 21 source languages into English.\nDepending on the language, the training data ranges in size from 1 - 264 hours.\nBesides speech translation data, we also add text-to-text translation data for training the model as\nin [20]. This dataset includes the text translation data from CoVoST 2 combined with all data from\neither WMT or TED Talks, as available.\n4\nKey Results\n4.1\nRobust Speech Recognition for Massively Multilingual Tasks\nIn this section, we compare the performance of our models against public baselines, including\nWhisper large-v26 [1], which has been trained on 680k hours of weakly supervised data across 100\nlanguages.\nFor the massively multilingual speech recognition test dataset from YouTube, we observe that Whisper\nhallucinates in many languages, resulting in a WER exceeding 100%. For a reasonable comparison,\nwe restrict the language set on which we compare the performance USM against Whisper by first\nselecting the top-25 languages from the training data for Whisper and further excluding languages for\nwhich Whisper produces > 40% WER. We also use segmented decoding for Whisper with 30-second\nsegments to further reduce the effect of hallucinations. As shown in Table 3, our USM-LAS and\n6Whisper large-v2 on Github (https://github.com/openai/whisper.git, revision b4308c4) is used for evaluation.\n12\nUSM-CTC models outperform Whisper by a wide margin on YouTube en-US, despite training on\nsignificantly less supervised data (3.5k hours versus Whisper\u2019s 400k hours [1]). While the USM-LAS\nmodel also requires segmented decoding to reduce long-form degradation as discussed in section\n2.4, it is far more robust, out-performing Whisper by a relative 30% WER on those 18 languages.\nUSM-CTC does not exhibit long-form performance degradation and achieves the best performance\non YouTube.\nOn the out-of-domain long-form CORAAL set, both USM-CTC and USM-LAS outperform Whisper\nby more than 10% relative WER. USM-CTC and USM-LAS similary outperform Whisper on\nSpeechStew, whose training data the models have not had access to.\nWe further compare the multilingual performance of the models on the held-out set from FLEURS.\nAs shown in Table 3, USM-LAS and USM-CTC both outperform Whisper by 66% relative WER,\ndespite using a smaller amount of multilingual supervised data (90k versus Whisper\u2019s 117k, when\nen-US is excluded). USM-LAS consistently outperforms USM-CTC for short-form ASR tasks.\n4.2\nMassively Multilingual Results Beyond 100 Languages\nThe lower part of Table 3 shows our results for in-domain fine-tuning. Our pre-trained model improves\nthe FLEURS benchmark significantly, even when using only 10 hours per language. Compared to the\nprevious SoTA in [67], our model achieves a 30% relative improvement in terms of WER across 102\nlanguages. Our results show that while generic speech models can be powerful, performance is still\nmaximized by in-domain fine-tuning.\n4.3\nMOST Produces Robust Representations that Generalize to New Domains\nMOST training aligns the representations of speech and text by training simultaneously on the two\nmodalities. We investigate whether MOST representations are useful for adapting the model to new\ndomains by freezing the entire learned encoder produced by MOST and adjusting a small amount of\nparameters added to the network by residual adapters. As shown in Table 3, by adding only 2% to\nthe total number of parameters, the MOST representation model (USM-M-adapter) only performs\nslightly worse than the fine-tuning baselines, still showing competitive performance on downstream\nASR and AST tasks. The small number of parameters being trained in this approach makes it feasible\nto extend our system to a large number of new domains and new tasks, even with a limited amount of\ntraining data, such as in FLEURS.\n4.4\nPushing the Quality of ASR on Unseen Languages\nTable 4: Noisy student training for unseen languages. WERs (%) for the teacher adapter models and\nthe student models are presented. The relative improvement (%) of the student models can be found\nin the last column.\nLanguages\nWhisper-v2\n# hrs in YT-NTL\nUSM-LAS-Adapter\nUSM-M + pseudo label\nRel. Imprv.\nHausa (ha)\n88.9\n2175.0\n24.5\n22.8\n7.5\nKazakh (kk)\n37.7\n196.0\n11.8\n10.9\n8.3\nShona (sn)\n121.0\n247.0\n29.1\n22.2\n31.1\nPashto (ps)\n93.7\n254.0\n36.0\n35.4\n1.7\nYoruba (yo)\n94.8\n1292.0\n33.4\n30.6\n9.2\nTail languages often do not have paired transcriptions for supervised learning\u2014we refer to these\nlanguages as unseen languages, as the model has not seen paired data for these lanugages during\ntraining. To create pseudo-labels for these languages, we first build a USM-LAS-Adapter by attaching\nresidual adapters to USM-LAS and training them using FLEURS data. By using the USM-LAS-\nAdapter as a teacher, we can now transcribe the unlabeled data in the unseen languages as part\nof the YT-NTL dataset. As shown in Table 4, we observe consistent wins for all languages on\nthe FLEURS benchmark. For some languages, the improvement is larger than 30%. This further\ndemonstrates the robustness of the USM-LAS model\u2014despite using only 10 hours of out of domain\ndata from FLEURS, the USM-LAS-Adapter is able to transcribe YouTube data to produce meaningful\nrecognition results that lead to these improvements. We find the approach of training adapter models\n13\non small datasets and utilizing them for pseudo-labeling to be a promising route for scaling up the\nnumber of languages that can be transcribed by USMs.\n4.5\nUSMs are Strong AST Models\nThe multi-lingual speech translation performance of fine-tuned USMs are shown in Table 3. We\nfind that we are already comparable to the CoVoST 2 SoTA BLEU score by fine-tuning the speech-\nonly USM. We note that the previous SoTA uses 125k hours of supervised speech translation data\ncompared to the 859 hours of data used by the USM. After MOST training, USM-M can use both\nspeech and text as training input. By introducing text-to-text machine translation (MT) data during\nfine-tuning, USM-M is able to achieve an unprecedented > 30 BLEU on CoVoST (a 1 BLEU increase\nfrom SoTA).\n5\nAnalysis and Ablations\n5.1\nMulti-Softmax Loss for BEST-RQ\nWe observe a consistent > 5% relative improvement in ASR and AST benchmarks by increasing\nthe number of the softmax groups in the multi-softmax loss for BEST-RQ training from 1 to 16, as\nshown in Table 5. We also find that using multiple softmax groups significantly reduces performance\nvariation across different pre-training runs and improves convergence speed.\nTable 5: YT-55 versus YT-NTL across different domains, with and without multi-softmax groups. For\nsimplicity, we report CER for FLEURS. For CoVoST, we report the BLEU score. YT-NTL covers 27\nadditional languages not covered in YT-55.\nModel\npre-train Set\n# Params (B)\n# Softmax\nFLEURS (CER)\nCoVoST (BLEU)\n.\n102 langs\n27 langs\nConformer-0.6B\nYT-55\n0.6\n1\n9.5\n-\n20.9\nConformer-2B\nYT-55\n2.0\n1\n7.9\n9.5\n26.6\nConformer-2B\nYT-NTL-U\n2.0\n1\n7.4\n8.5\n27.5\nConformer-2B\nYT-NTL-U\n2.0\n16\n6.9\n8.1\n28.7\n5.2\nModel and Language Scaling\nWe find that scaling up the model size and increasing the language coverage of the pre-training\ndataset greatly benefits the performance of the USMs, as demonstrated in Table 5. In particular, we\nfind a 10% relative improvement of ASR and AST performance by using YT-NTL vs. YT-55 for\npre-training, despite the fact that each newly added language in YT-NTL contains approximately 500\nhours of speech\u2014a relatively small amount. As could be expected, the relative gains on the newly\ncovered languages are more substantial than those on other languages.\n5.3\nBEST-RQ is a Scalable Self-supervised Learner\nBEST-RQ has been shown to outperform or be comparable to other prominent pre-training methods\nfor speech recognition, including wav2vec 2.0 and W2v-BERT in the original work in which it was\nintroduced [10]. Here we investigate its comparative performance and scaling properties, similar\nto what has been done for wav2vec 2.0 in [3] and W2v-BERT in [20]. We utilize the set-up of\npre-training the model using YT-55 and fine-tuning it on CoVoST 2. As shown in Table 6, our results\nindicate that for the Conformer-0.6B, W2v-BERT and BEST-RQ perform similarly, but BEST-RQ\nobtains greater gains when scaled up. A contributing factor to this can be that W2v-BERT is more\nprone to codebook collapse and training instabilities at the 2B scale, while BEST-RQ by construction\ndoesn\u2019t suffer from codebook collapse.\n5.4\nChunk-wise attention for robust long-form speech recognition\nFig. 7 depicts the long-form performance degradation issue as described in section 2.4. In the figure,\nwe see that for the shallow Conformer model with 17 layers, using a small local self attention context\n14\nTable 6: BLEU scores for the CoVoST 2 X \u2192En task to compare BEST-RQ against W2v-BERT.\nHigher is better.\nX \u2192English\nhigh\nmid\nlow\nall\nPrevious Work\nXLS-R (0.3B) [33]\n30.6\n18.9\n5.1\n13.2\nXLS-R (1B) [33]\n34.3\n25.5\n11.7\n19.3\nXLS-R (2B) [33]\n36.1\n27.7\n15.1\n22.1\nConformer-0.6B\nW2v-BERT\n35.6\n25.3\n13.4\n20.4\nBEST-RQ\n32.5\n25.6\n14.7\n20.7\nConformer-2B\nW2v-BERT\n36.0\n27.8\n15.6\n22.4\nBEST-RQ\n35.8\n31.3\n21.5\n26.6\nFigure 7: The word error rate measured on the YouTube en-US long-form test set for Conformer\nmodels with varying depth.\n(65) length, the word error rate measured on the long-form test set gradually improves as the training\nprogresses. With a deeper model that has 48 layers but roughly the same number of parameters,\nhowever, the larger receptive field mismatch results in higher test WERs as the training step increases.\nTable 7 demonstrates that chunk-wise attention is able to address the long-form degradation issue and\nshow robust performance across four different languages\u2014en-US (English), ru-RU (Russian), ko-KR\n(Korean), and uk-UA (Ukrainian). We compare chunk-wise attention models with an 8-second chunk\nsize (CW-8s in Table 7) against local self attention models which uses 128 context frames in each\nconformer layer (LSA-128). We note that further increasing the context window size of the local self\nattention model results in high deletion error rates on all languages of the YouTube long-form test\nsets. These results show that the chunk-wise attention models do not exhibit long-form performance\ndegradation and are able to improve upon the performance of the local self attention models operating\nat the maximum allowed receptive field length.\nTable 7: Chunk-wise attention. WER (%) is reported on the YouTube long-form set.\nModel\n# Params (B)\n# Layers\nen-US\nru-RU\nko-KR\nuk-UA\nLSA-128\n0.6\n24\n16.2\n16.6\n26.2\n15.5\nCW-8s\n0.6\n24\n12.5\n14.7\n19.5\n15.3\n5.5\nTPU Serving Capacity of USM-CTC Models\nIn section 4, we have demonstrated that USM-CTC models are powerful generic ASR models\nwith reliable long-form transcription performance and excellent generalization properties. Here we\n15\nTable 8: RTF for USM-2B.\nModel\nbf-16\nStreaming\n# Params (B)\nTPU [88]\nBatch Size\n1.0/RTF\nConformer-0.1B\nY\nY\n0.1\nTPUv4i\n64\n3047\nConformer-0.6B\nN\nN\n0.6\nTPUv4i\n64\n1920\nConformer-2B\nN\nN\n2.0\nTPUv4i\n32\n827\nmeasure the serving capacity of the USM-CTC model as represented by the real time factor (RTF) in\nan ideal setup where we assume that each batch sent to TPU is fully packed along the time axis. The\nresults of these measurements are presented in Table 8. Surprisingly, we find that the 2B-paramter\nUSM-CTC model is only 3.9\u00d7 slower than the 100M-parameter streaming model [89], primarily\ndue to the fact that our models operate at batch processing mode. This result demonstrates that the\nUSM-CTC can be used as an offline transcriber efficiently on TPUs (or GPUs).\n6\nDiscussion\nIn this report, we put forward a practical and flexible approach for training speech understanding\nmodels capable of scaling speech recognition to hundreds of languages. We conclude the report with\nsummarizing insights gained in the process:\nUnlabeled versus weakly labeled data: We believe diverse unlabeled data is more practical to\nacquire for building usable ASR for tail languages than weakly labeled data. We have demonstrated\nthat collaborating with native speakers to identify unsupervised data in hundreds of tail languages\ncan be an effective route to improving recognition performance on low resource languages.\nIn-domain data is best: We have demonstrated that we can build a robust ASR system across many\ndomains by utilizing a large amount of unsupervised data and a small amount of labeled data. Our\nresults, however, also confirm that the most effective way to optimize the performance for a given\ndomain is to use in-domain data to fine-tune the model.\nCTC vs RNN-T vs LAS: The best transducer depends on the downstream task. A large pre-trained\nmodel with a frozen encoder can allow experimenters to test different transducers quickly and select\nthe optimal transducer for their purpose.\nAcknowledgments\nWe would like to thank Alexis Conneau, Min Ma, Shikhar Bharadwaj, Sid Dalmia, Jiahui Yu, Jian\nCheng, Paul Rubenstein, Ye Jia, Justin Snyder, Vincent Tsang, Yuanzhong Xu, Tao Wang, Anusha\nRamesh, Calum Barnes, Salem Haykal for useful discussions.\nWe appreciate valuable feedback and support from Eli Collins, Jeff Dean, Sissie Hsiao, Zoubin\nGhahramani. Special thanks to Austin Tarango, Lara Tumeh, and Jason Porta for their guidance\naround responsible AI practices.\nReferences\n[1] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition\nvia large-scale weak supervision,\u201d arXiv preprint arXiv:2212.04356, 2022.\n[2] W. Chan, D. Park, C. Lee, Y. Zhang, Q. Le, and M. Norouzi, \u201cSpeechstew: Simply mix all available speech\nrecognition data to train one large neural network,\u201d arXiv preprint arXiv:2104.02133, 2021.\n[3] Y. Zhang, D. S. Park, W. Han, J. Qin, A. Gulati, J. Shor, A. Jansen, Y. Xu, Y. Huang, S. Wang, Z. Zhou, B. Li,\nM. Ma, W. Chan, J. Yu, Y. Wang, L. Cao, K. C. Sim, B. Ramabhadran, T. N. Sainath, F. Beaufays, Z. Chen,\nQ. V. Le, C.-C. Chiu, R. Pang, and Y. Wu, \u201cBigssl: Exploring the frontier of large-scale semi-supervised\nlearning for automatic speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16,\nno. 6, pp. 1519\u20131532, 2022.\n[4] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani, W. R. Huang, and M. Ma, \u201cScaling\nend-to-end models for large-scale multilingual asr,\u201d arXiv preprint arXiv:2104.14830, 2021.\n16\n[5] X. Li, F. Metze, D. R. Mortensen, A. W. Black, and S. Watanabe, \u201cAsr2k: Speech recognition for around\n2000 languages without audio,\u201d arXiv preprint arXiv:2209.02842, 2022.\n[6] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning\nof speech representations,\u201d arXiv preprint arXiv:2006.11477, 2020.\n[7] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, \u201cSelf-training with noisy student improves imagenet\nclassification,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 10 687\u201310 698.\n[8] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. V. Le, \u201cImproved noisy student\ntraining for automatic speech recognition,\u201d arXiv preprint arXiv:2005.09629, 2020.\n[9] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu et al.,\n\u201cConformer: Convolution-augmented transformer for speech recognition,\u201d arXiv preprint arXiv:2005.08100,\n2020.\n[10] C.-C. Chiu, J. Qin, Y. Zhang, J. Yu, and Y. Wu, \u201cSelf-supervised learning with random-projection quantizer\nfor speech recognition,\u201d in Proceedings of the 39th International Conference on Machine Learning,\nser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,\nG. Niu, and S. Sabato, Eds., vol. 162.\nPMLR, 17\u201323 Jul 2022, pp. 3915\u20133924. [Online]. Available:\nhttps://proceedings.mlr.press/v162/chiu22a.html\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers\nfor language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018.\n[12] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, G. Wang, and P. Moreno, \u201cInjecting text in self-\nsupervised speech pretraining,\u201d arXiv preprint arXiv:2108.12226, 2021.\n[13] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. Moreno, A. Bapna, and H. Zen, \u201cMaestro: Matched\nspeech text representations through modality matching,\u201d arXiv preprint arXiv:2204.03409, 2022.\n[14] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling\nunsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international\nconference on Machine learning, 2006, pp. 369\u2013376.\n[15] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary\nconversational speech recognition,\u201d in 2016 IEEE international conference on acoustics, speech and signal\nprocessing (ICASSP).\nIEEE, 2016, pp. 4960\u20134964.\n[16] A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and\nA. Bapna, \u201cFleurs: Few-shot learning evaluation of universal representations of speech,\u201d arXiv preprint\narXiv:2205.12446, 2022.\n[17] T. Kendall and C. Farrington, \u201cThe corpus of regional african american language. version 2021.07. eugene,\nor: The online resources for african american language project,\u201d 2021.\n[18] C. Wang, A. Wu, and J. Pino, \u201cCoVoST 2 and massively multilingual speech-to-text translation,\u201d in\ninterspeech, 2021.\n[19] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, \u201cTowards a unified view of parameter-efficient\ntransfer learning,\u201d in International Conference on Learning Representations, 2022. [Online]. Available:\nhttps://openreview.net/forum?id=0RDcd5Axok\n[20] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, and A. Conneau,\n\u201cmslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374,\n2022.\n[21] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, \u201cW2v-bert: Combining contrastive\nlearning and masked language modeling for self-supervised speech pre-training,\u201d in 2021 IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU).\nIEEE, 2021, pp. 244\u2013250.\n[22] W.-N. Hsu and J. Glass, \u201cExtracting domain invariant features by unsupervised learning for robust automatic\nspeech recognition,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2018, pp. 5614\u20135618.\n[23] Y.-A. Chung and J. Glass, \u201cSpeech2vec: A sequence-to-sequence framework for learning word embeddings\nfrom speech,\u201d arXiv preprint arXiv:1803.08976, 2018.\n[24] A. v. d. Oord, Y. Li, and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv\npreprint arXiv:1807.03748, 2018.\n[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, \u201cAn unsupervised autoregressive model for speech\nrepresentation learning,\u201d arXiv preprint arXiv:1904.03240, 2019.\n[26] J. Chorowski, R. J. Weiss, S. Bengio, and A. van den Oord, \u201cUnsupervised speech representation learning\nusing wavenet autoencoders,\u201d IEEE/ACM transactions on audio, speech, and language processing, vol. 27,\nno. 12, pp. 2041\u20132053, 2019.\n17\n[27] S. Schneider, A. Baevski, R. Collobert, and M. Auli, \u201cwav2vec: Unsupervised pre-training for speech\nrecognition,\u201d arXiv preprint arXiv:1904.05862, 2019.\n[28] A. Baevski, S. Schneider, and M. Auli, \u201cvq-wav2vec: Self-supervised learning of discrete speech represen-\ntations,\u201d arXiv preprint arXiv:1910.05453, 2019.\n[29] S. Ling, Y. Liu, J. Salazar, and K. Kirchhoff, \u201cDeep contextualized acoustic representations for semi-\nsupervised speech recognition,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nIEEE, 2020, pp. 6429\u20136433.\n[30] A. Baevski, M. Auli, and A. Mohamed, \u201cEffectiveness of self-supervised pre-training for speech recogni-\ntion,\u201d arXiv preprint arXiv:1911.03912, 2019.\n[31] M. Riviere, A. Joulin, P.-E. Mazar\u00e9, and E. Dupoux, \u201cUnsupervised pretraining transfers well across\nlanguages,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2020, pp. 7414\u20137418.\n[32] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. v. d. Oord, \u201cLearning robust and multilingual speech\nrepresentations,\u201d arXiv preprint arXiv:2001.11128, 2020.\n[33] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino\net al., \u201cXls-r: Self-supervised cross-lingual speech representation learning at scale,\u201d arXiv preprint\narXiv:2111.09296, 2021.\n[34] G. Zavaliagkos and T. Colthurst, \u201cUtilizing untranscribed training data to improve performance,\u201d in DARPA\nBroadcast News Transcription and Understanding Workshop, Landsdowne, 1998, pp. 301\u2013305.\n[35] L. Lamel, J. luc Gauvain, and G. Adda, \u201cLightly supervised acoustic model training,\u201d in Proc. ISCA ITRW\nASR2000, 2000, pp. 150\u2013154.\n[36] S. Novotney and R. Schwartz, \u201cAnalysis of low-resource acoustic model self-training,\u201d in Tenth Annual\nConference of the International Speech Communication Association, 2009.\n[37] S. Thomas, M. L. Seltzer, K. Church, and H. Hermansky, \u201cDeep neural network features and semi-\nsupervised training for low resource speech recognition,\u201d in 2013 IEEE international conference on\nacoustics, speech and signal processing.\nIEEE, 2013, pp. 6704\u20136708.\n[38] B. Li, T. N. Sainath, R. Pang, and Z. Wu, \u201cSemi-supervised training for end-to-end models via weak\ndistillation,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2019, pp. 2837\u20132841.\n[39] J. Kahn, A. Lee, and A. Hannun, \u201cSelf-training for end-to-end speech recognition,\u201d in ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp.\n7084\u20137088.\n[40] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave, V. Pratap, A. Sriram, V. Liptchinsky, and\nR. Collobert, \u201cEnd-to-end asr: from supervised to semi-supervised learning with modern architectures,\u201d in\narXiv, 2019.\n[41] S. H. K. Parthasarathi and N. Strom, \u201cLessons from building acoustic models with a million hours of\nspeech,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2019, pp. 6670\u20136674.\n[42] W.-N. Hsu, A. Lee, G. Synnaeve, and A. Hannun, \u201cSemi-supervised speech recognition via local prior\nmatching,\u201d arXiv preprint arXiv:2002.10336, 2020.\n[43] Q. Xu, T. Likhomanenko, J. Kahn, A. Hannun, G. Synnaeve, and R. Collobert, \u201cIterative pseudo-labeling\nfor speech recognition,\u201d arXiv preprint arXiv:2005.09267, 2020.\n[44] Z. Chen, A. Rosenberg, Y. Zhang, H. Zen, M. Ghodsi, Y. Huang, J. Emond, G. Wang, B. Ramabhadran, and\nP. J. Moreno, \u201cSemi-Supervision in ASR: Sequential MixMatch and Factorized TTS-Based Augmentation,\u201d\nin Proc. Interspeech 2021, 2021, pp. 736\u2013740.\n[45] A. Renduchintala, S. Ding, M. Wiesner, and S. Watanabe, \u201cMulti-modal data augmentation for end-to-end\nasr,\u201d arXiv preprint arXiv:1803.10299, 2018.\n[46] A. Bapna, Y.-a. Chung, N. Wu, A. Gulati, Y. Jia, J. H. Clark, M. Johnson, J. Riesa, A. Conneau, and\nY. Zhang, \u201cSlam: A unified encoder for speech and language modeling via speech-text joint pre-training,\u201d\narXiv preprint arXiv:2110.10329, 2021.\n[47] S. Thomas, B. Kingsbury, G. Saon, and H.-K. J. Kuo, \u201cIntegrating text inputs for training and adapting rnn\ntransducer asr models,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2022, pp. 8127\u20138131.\n[48] Y. Cheng, Y. Zhang, M. Johnson, W. Macherey, and A. Bapna, \u201cMu2slam: Multitask, multilingual speech\nand language models,\u201d arXiv preprint arXiv:2212.09553, 2022.\n18\n[49] Z.-H. Zhang, L. Zhou, J. Ao, S. Liu, L. Dai, J. Li, and F. Wei, \u201cSpeechut: Bridging speech and text with\nhidden-unit for encoder-decoder based speech-text pre-training,\u201d in Conference on Empirical Methods in\nNatural Language Processing, 2022.\n[50] Z.-H. Zhang, S. Chen, L. Zhou, Y. Wu, S. Ren, S. Liu, Z. Yao, X. Gong, L. Dai, J. Li, and F. Wei,\n\u201cSpeechlm: Enhanced speech pre-training with unpaired textual data,\u201d ArXiv, vol. abs/2209.15329, 2022.\n[51] S. Khurana, A. Laurent, and J. R. Glass, \u201cSamu-xlsr: Semantically-aligned multimodal utterance-level\ncross-lingual speech representation,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, pp.\n1493\u20131504, 2022.\n[52] X. Zhou, J. Wang, Z. Cui, S. Zhang, Z. Yan, J. Zhou, and C. Zhou, \u201cMmspeech: Multi-modal multi-task\nencoder-decoder pre-training for speech recognition,\u201d ArXiv, vol. abs/2212.00500, 2022.\n[53] T. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen, B. Li, W. Wang, and T. Strohman,\n\u201cJoist: A joint speech and text streaming model for asr,\u201d in 2022 IEEE Spoken Language Technology\nWorkshop (SLT).\nIEEE, 2023, pp. 52\u201359.\n[54] Z. Meng, W. Wang, R. Prabhavalkar, T. N. Sainath, T. Chen, E. Variani, Y. Zhang, B. Li, A. Rosenberg,\nand B. Ramabhadran, \u201cJeit: Joint end-to-end model and internal language model training for speech\nrecognition,\u201d in ICASSP, 2023, 2023.\n[55] Z. Meng, T. Chen, R. Prabhavalkar, Y. Zhang, G. Wang, K. Audhkhasi, J. Emond, T. Strohman, B. Ramab-\nhadran, W. R. Huang et al., \u201cModular hybrid autoregressive transducer,\u201d in 2022 IEEE Spoken Language\nTechnology Workshop (SLT).\nIEEE, 2023, pp. 197\u2013204.\n[56] C.-C. Chiu, W. Han, Y. Zhang, R. Pang, S. Kishchenko, P. Nguyen, A. Narayanan, H. Liao, S. Zhang,\nA. Kannan et al., \u201cA comparison of end-to-end models for long-form speech recognition,\u201d in 2019 IEEE\nautomatic speech recognition and understanding workshop (ASRU).\nIEEE, 2019, pp. 889\u2013896.\n[57] Z. Lu, Y. Pan, T. Doutre, P. Haghani, L. Cao, R. Prabhavalkar, C. Zhang, and T. Strohman, \u201cInput length\nmatters: Improving rnn-t and mwer training for long-form telephony speech recognition,\u201d arXiv preprint\narXiv:2110.03841, 2021.\n[58] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, \u201cTransformer-xl: Attentive language\nmodels beyond a fixed-length context,\u201d arXiv preprint arXiv:1901.02860, 2019.\n[59] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012.\n[60] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring\nthe limits of transfer learning with a unified text-to-text transformer,\u201d The Journal of Machine Learning\nResearch, vol. 21, no. 1, pp. 5485\u20135551, 2020.\n[61] B. Ramabhadran, K. Audhkhasi, P. J. M. Mengibar, and T. Chen, \u201cMixture model attention: Flexible\nstreaming and non-streaming automatic speech recognition,\u201d in Proceedings of Interspeech, 2021, 2021.\n[62] L. Lu, C. Liu, J. Li, and Y. Gong, \u201cExploring transformers for large-scale speech recognition,\u201d arXiv\npreprint arXiv:2005.09684, 2020.\n[63] X. Chen, Y. Wu, Z. Wang, S. Liu, and J. Li, \u201cDeveloping real-time streaming transformer transducer for\nspeech recognition on large-scale dataset,\u201d in International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2021, pp. 5904\u20135908.\n[64] C. Wu, Y. Wang, Y. Shi, C.-F. Yeh, and F. Zhang, \u201cStreaming transformer-based acoustic models using\nself-attention with augmented memory,\u201d arXiv preprint arXiv:2005.08042, 2020.\n[65] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and M. Seltzer, \u201cEmformer: Efficient\nmemory transformer based acoustic model for low latency streaming speech recognition,\u201d in International\nConference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp. 6783\u20136787.\n[66] E. Tsunoo, Y. Kashiwagi, T. Kumakura, and S. Watanabe, \u201cTransformer asr with contextual block process-\ning,\u201d in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).\nIEEE, 2019,\npp. 427\u2013433.\n[67] Z. Chen, A. Bapna, A. Rosenberg, Y. Zhang, B. Ramabhadran, P. Moreno, and N. Chen, \u201cMaestro-\nu: Leveraging joint speech-text representation learning for zero supervised speech asr,\u201d arXiv preprint\narXiv:2210.10027, 2022.\n[68] F. Biadsy, Y. Chen, X. Zhang, O. Rybakov, A. Rosenberg, and P. J. Moreno, \u201cA scalable model specialization\nframework for training and inference using submodels and its application to speech model personalization,\u201d\nin Proc. Interspeech 2022.\nISCA, 2022, pp. 5125\u20135129.\n[69] K. Tomanek, V. Zayats, D. Padfield, K. Vaillancourt, and F. Biadsy, \u201cResidual adapters for parameter-\nefficient asr adaptation to atypical and accented speech,\u201d in Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, 2021, pp. 6751\u20136760.\n[70] M. Schuster and K. Nakajima, \u201cJapanese and korean voice search,\u201d in 2012 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2012, pp. 5149\u20135152.\n19\n"
    },
    {
        "pdf_file": "paper4.pdf",
        "text": "Google USM:\nScaling Automatic Speech Recognition\nBeyond 100 Languages\nYu Zhang\nWei Han\nJames Qin\nYongqiang Wang\nAnkur Bapna\nZhehuai Chen\nNanxin Chen\nBo Li\nVera Axelrod\nGary Wang\nZhong Meng\nKe Hu\nAndrew Rosenberg\nRohit Prabhavalkar\nDaniel S. Park\nParisa Haghani\nJason Riesa\nGinger Perng\nHagen Soltau\nTrevor Strohman\nBhuvana Ramabhadran\nTara Sainath\nPedro Moreno\nChung-Cheng Chiu\nJohan Schalkwyk\nFran\u00e7oise Beaufays\nYonghui Wu \u2217\u2020\nAbstract\nWe introduce the Universal Speech Model (USM), a single large model that per-\nforms automatic speech recognition (ASR) across 100+ languages. This is achieved\nby pre-training the encoder of the model on a large unlabeled multilingual dataset\nof 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller\nlabeled dataset. We use multilingual pre-training with random-projection quantiza-\ntion and speech-text modality matching to achieve state-of-the-art performance on\ndownstream multilingual ASR and speech-to-text translation tasks. We also demon-\nstrate that despite using a labeled training set 1/7-th the size of that used for the\nWhisper model [1], our model exhibits comparable or better performance on both\nin-domain and out-of-domain speech recognition tasks across many languages.\n1\nIntroduction\nRecent advances in self-supervised learning have ushered in a new era for speech recognition.\nWhereas previous works focused mostly on improving the quality of monolingual models for main-\nstream languages, recent studies have increasingly turned to \u201cuniversal\u201d models [1\u20134]. These may\ntake the form of a single model that performs well on multiple tasks [1,2], or one that covers multiple\ndomains [2,3], or one that supports multiple languages [1,5]. In this work, we explore the frontiers of\nlanguage expansion. Our long-term goal is to train a universal ASR model that covers all the spoken\nlanguages in the world.\nA fundamental challenge in scaling speech technologies to many languages is obtaining enough data\nto train high-quality models. With conventional supervised training approaches, audio data needs\nto be manually transcribed, which is lengthy and expensive, or collected from existing transcribed\nsources which are hard to find for tail languages. While transcribed speech may be scarce in many\n\u2217All authors are affiliated with Google Inc.\n\u2020Contact author at ngyuzh@google.com.\nPreprint.\narXiv:2303.01037v3  [cs.CL]  25 Sep 2023\nlanguages, untranscribed speech and text data are practically unlimited. Recent developments in semi-\nsupervised algorithms for speech recognition makes it possible to leverage such data for pre-training\nand produce high-quality speech models with a limited amount of transcribed data [3,6].\nMoreover, recent studies have shown that a single large model can utilize large data sets more\neffectively than smaller models [1,4]. This all points to a promising direction where large amounts of\nunpaired multilingual speech and text data and smaller amounts of transcribed data can contribute to\ntraining a single large universal ASR model.\n1.1\nOur approach\nWe produce large \u201cUniversal Speech Models\" (USMs) through a training pipeline that utilizes three\ntypes of datasets:\n\u2022 Unpaired Audio:\n\u2013 YT-NTL-U: A large unlabeled multilingual dataset consisting of 12M hours of\nYouTube-based audio covering over 300 languages.\n\u2013 Pub-U: 429k hours of unlabeled speech in 51 languages based on public datasets.\n\u2022 Unpaired Text:\n\u2013 Web-NTL: A large multilingual text-only corpus with 28B sentences spanning over\n1140 languages.\n\u2022 Paired ASR Data: We utilize two corpora of paired audio-text data with O(10k) hours of\naudio for supervised training.\n\u2013 YT-SUP+: 90k hours of labeled multilingual data covering 73 language and 100k hours\nof en-US pseudo-labeled data generated by noisy student training (NST) [7,8] from\nYT-NTL-U.\n\u2013 Pub-S: 10k hours of labeled multi-domain en-US public data and 10k labeled multilin-\ngual public data covering 102 languages.\n2B-parameter Conformer [9] models are built using these datasets through the following steps:\n1. Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-\nprojection Quantizer) [10] is used to pre-train the encoder of the model with YT-NTL-U.\n2. MOST (Multi-Objective Supervised pre-Training): The model can optionally be further\nprepared by a multi-objective supervised pre-training pipeline that utilizes all three kinds\nof datasets: YT-NTL-U, Pub-U, Web-NTL and Pub-S. Here, a weighted sum of the BEST-\nRQ masked language model loss [11], along with the text-injection losses (including the\nsupervised ASR loss and modality matching losses) [12,13] is optimized during training.\n3. Supervised ASR Training: We produce generic ASR models trained with connectionist\ntemporal classification (CTC) [14] and Listen, Attend, and Spell (LAS) [15] tranducers for\ndownstream tasks.\nTwo types of models are produced through this pipeline\u2014pre-trained models that can be fine-tuned\non downstream tasks, and generic ASR models for which we assume no downstream fine-tuning\noccurs. The generic ASR models are trained with chunk-wise attention, which we introduce later in\nthis report.\nTable 1: USM models prepared in this work. The generic ASR models are trained on a large\n\"upstream\" ASR corpus and not finetuned further, while the pre-trained models are fine-tuned on\ndownstream tasks.\nModel\nBEST-RQ\nMOST\nModel-Type\nDecoder\nUpstream\nChunk-wise\nASR Dataset\nAttention\nUSM\nYT-NTL-U\nN\nPre-trained\nDownstream Dependent\n-\nN\nUSM-M\nY\nPre-trained\nDownstream Dependent\n-\nN\nUSM-LAS\nN\nGeneric ASR\nLAS\nYT-SUP+\nY\nUSM-CTC\nN\nGeneric ASR\nCTC\nYT-SUP+\nY\n2\nWe denote the pre-trained models USM and USM-M, where the appendix -M indicates that MOST\nhas been utilized for the preparation of the model. The USM and USM-M models can be further\nfine-tuned on the downstream task of choice with an appropriate transducer unit, which can be a CTC,\nLAS or RNN transducer (RNN-T) unit. We evaluate our USM models on two types of benchmarks:\n\u2022 Automatic Speech Recognition (ASR): We use YouTube data to train USMs for YouTube\n(e.g., closed captions). We evaluate the USMs on two public benchmarks, SpeechStew [2]\nand FLEURS [16]. We also report results on the long-form test set CORAAL [17] for which\nonly the evaluation set is available.\n\u2022 Automatic Speech Translation (AST): We test AST performance on CoVoST 2 [18].\nAs indicated in Table 1, the generic ASR models are trained with YT-SUP+ and not fine-tuned on\ndomain-specific datasets for downstream ASR tasks. We, however, explore the possibility of attaching\nadditional \u201cadapter\" units [19] to both generic and pre-trained ASR models and training adapter\nweights while keeping the rest of the model frozen.\nBEST-RQ + \nConformer Encoder\nUnsupervised Pre-training\nUnsupervised Audio from \nhundreds of languages, ~10M hrs\n80% of compute\nMulti-Objective Supervised Pre-Training\nText-injection + \nBEST-RQ + \nSupervised ASR loss\nHigh/Mid resource \nsupervised data\n100h to 10k per \nlanguage\nUnsupervised \nText data 28B \nsentences in over \n1000 languages\n15% of compute\nIn-domain Fine-tuning with task specific \ntransducer\nRNN-T - Low Resource\nCTC - Long-form\nLAS - Short-form and Speech Translation\nTask specific paired data\n5% of compute\nFigure 1: An overview of our approach. Training is split into three stages. (i) The first stage trains\na conformer backbone on a large unlabeled speech dataset, optimizing for the BEST-RQ objective.\n(ii) We continue training this speech representation learning model while optimizing for multiple\nobjectives, the BEST-RQ objective on unlabeled speech, the modality matching, supervised ASR and\nduration modeling losses on paired speech and transcript data and the text reconstruction objective\nwith an RNN-T decoder on unlabeled text. (iii) The third stage fine-tunes this pre-trained encoder on\nthe ASR or AST tasks.\nThe overall training pipeline of our models is summarized in Fig. 1. In our design, once a large\namount of compute is expended in the pre-training stages, the downstream application can be\nconveniently fine-tuned from a model trained from stage-1 or stage-2 with a task-specific transducer.\nOur experimental results demonstrate that this pipelined training framework enables us to build both\ngeneric multilingual ASR systems and domain specific models with state-of-the-art performance.\nWe next present the key findings of our research, provide an overall view of the report, and review\nrelated work.\n1.2\nKey Findings\nSoTA results for downstream multilingual speech tasks: Our USM models achieve state-of-the-art\nperformance for multilingual ASR and AST for multiple datasets in multiple domains. This includes\nSpeechStew (mono-lingual ASR) [2], CORAAL (African American Vernacular English (AAVE)\nASR) [17], FLEURS (multi-lingual ASR) [16], YT (multilingual long-form ASR), and CoVoST\n(AST from English to multiple languages). We depict our model\u2019s performance in the first panel of\nFig. 2. We also build an ASR model for YouTube captioning \u2013 i.e., the transcription of speech in\nYouTube videos, that achieves < 30% WER on 73 languages. With only 90k hours of supervised\ndata, this model performs better than Whisper [1], a strong general ASR system trained on more than\n3\nFigure 2: (Left)\u2020 WERs (%) Our language expansion effort to support more languages on YouTube\n(73 languages) and extending to 100+ languages on the public dataset (FLEURS). Lower is better.\nTo the best of our knowledge, no published model can successfully decode all 73 languages from\nour YouTube set, thus we only list our results. (Middle)\u2020 Our results on ASR benchmarks, with or\nwithout in-domain data. Lower is better. (Right) SoTA results on public speech translation tasks.\nResults presented are presented as high/middle/low resources languages defined in [20]. Higher is\nbetter.\n400k hours of transcribed data (we select 18 languages that Whisper can successfully decode with\nlower than 40% WER). The second panel of Fig. 2 demonstrates that our YouTube captions model\ngeneralizes well to unseen domains.\nBEST-RQ is a scalable speech representation learner: We find that BEST-RQ pre-training can\neffectively scale to the very large data regime with a 2B parameter Conformer-based backbone,\ncomparing favorably against Wav2Vec 2.0 [6] and W2v-BERT [21] in this setting.\nMOST (BEST-RQ + text-injection) is a scalable speech and text representation learner: We\ndemonstrate that MOST is an effective method for utilizing large scale text data for improving\nquality on downstream speech tasks, as demonstrated by quality gains exhibited for the FLEURS\nand CoVoST 2 tasks. Fig. 2 depicts USM\u2019s performance, establishing a new state-of-the-art on the\nFLEURS benchmark across 102 languages for ASR and on CoVoST 2 across 21 languages on AST.\nRepresentations from MOST (BEST-RQ + text-injection) can quickly adapt to new domains:\nWe find that it is possible to obtain powerful downstream ASR/AST models by attaching and training\nlight-weight residual adapter modules, which only add 2% of additional parameters, while keeping\nthe rest of the model frozen.\nChunk-wise attention for robust long-form speech recognition:\nWe introduce chunk-wise\nattention, an effective, scalable method for extending the performance of ASR models trained on\nshorter utterances to very long speech inputs. We find that the USM-CTC/LAS models trained\nwith chunk-wise attention is able to produce high-quality transcripts for very long utterances in the\nYouTube evaluation sets.\n1.3\nOutline\nThe outline of this report is as follows:\nMethods: We review the architecture and the methods used in the paper. We provide brief summaries\nof the Conformer [9], BEST-RQ [10], text-injection [12, 13] used for MOST, and Noisy Student\nTraining (NST) [7,8]. We also introduce chunk-wise attention for scalable training on long utterances.\nData: We describe the four types of datasets used to train our models: the unlabeled multilingual\nspeech dataset YT-NTL-U, the multilingual text corpus Web-NTL, labeled datasets, and pseudo-\nlabeled datasets.\nKey Results: We present the performance of our USM models on downstream ASR and AST\ntasks. We demonstrate that USM establishes new states-of-the-art on several speech understanding\nbenchmarks.\n4\nAnalysis and Ablations: We present analysis of the effects of the key components of our work and\ncompare their performance against existing methods.\n1.4\nRelated Work\nThere is extensive literature on pre-training [6,12,22\u201333] and self-training [8,34\u201344] for ASR. Large\nspeech models trained on large datasets have been studied previously in both monolingual [3] and\nmultilingual contexts [1,4]. Large multi-modal speech models have been explored in [13,20,45\u201354].\nVarious unsupervised pre-training methods for speech models have been proposed and applied in\n[6,10,21].\nOur work is an extension of a host of recent research efforts [3, 10, 13, 53, 55] that have studied\nsemi-supervised learning for ASR in the context of deep-learning. Large speech models (> 1B)\nwere first studied in [3]; we expand upon this approach to train multilingual speech models in this\nwork. We improve the methods used in [3] by employing a more scalable self-supervised learning\nalgorithm (BEST-RQ) and additionally applying multi-modal pre-training (text-injection) to prepare\nthe models. We introduce an improvement to BEST-RQ [10] by utilizing a multi-softmax loss. We\nalso incorporate Multi-Objective Supervised Training (BEST-RQ with text-injection) to improve\nthe quality of speech representations learnt during pre-training, by utilizing transcribed data and\nunlabeled text. Long-form ASR has been studied in [1,56,57]; we propose chunk-wise attention as\nan alternative solution to chunk-based decoding.\nIn this paper, we propose a scalable self-supervised training framework for multilingual ASR which\nextends to hundreds of languages. In particular:\n\u2022 We demonstrate that USMs pre-trained on 300 languages can successfully adapt to both\nASR and AST tasks in new languages with a small amount of supervised data.\n\u2022 We build a generic ASR model on 73 languages by fine-tuning pre-trained models on 90k\nhours of supervised data. We show that the generic ASR models can carry out inference\nefficiently on TPUs and can reliably transcribe hours-long audio on YouTube Caption ASR\nbenchmarks.\n\u2022 We conduct a systematic study on the effects of pre-training, noisy student training, text\ninjection, and model size for multilingual ASR.\n2\nMethods\n2.1\nModel Architecture: Conformer\nWe use the convolution-augmented transformer [9], or Conformer, with relative attention [58] as an\nencoder model. For downstream speech tasks such as ASR or AST, the features produced by the\nConformer are either used as an input to a connectionist temporal classification (CTC) [14], RNN\ntransducer (RNN-T) [59] or a Listen, Attend, and Spell (LAS) [15] unit after additional projection.\nAs will be discussed further, BEST-RQ pre-training is exclusively applied to the encoder, while other\nforms of training (e.g., T5 [60]) train the entire task network as a whole.\nFor our experiments, we consider two models with 600M and 2B parameters respectively. While\nthe main results presented have been obtained using the 2B model, the 600M model is utilized for\nablation studies and observing model scaling behavior. Some features of the models are listed in\nTable 2.\nTable 2: Conformer model parameters.\nModel\n# Params (B)\n# Layers\nDimension\nAtt. Heads\nConv. Kernel Size\nConformer-0.6\n0.6\n24\n1024\n8\n5\nConformer-2B\n2.0\n32\n1536\n16\n5\n2.2\nPre-training: BEST-RQ\nWe select BEST-RQ [10] as the method to pre-train our networks with speech audio. BEST-RQ\nprovides a simple framework with a small number of hyperparameters for unsupervised training on\n5\nFigure 3: BEST-RQ based pre-training with conformer encoder.\nlarge-scale unlabeled audio data. We discuss the comparative advantage of BEST-RQ against other\npre-training methods in section 5.3.\nBEST-RQ employs a BERT-style training task for the audio input that attempts to predict masked\nspeech features. To make the task compatible with BERT-style training, the original speech features\ncorresponding to the masked frames are quantized, and the task requires predicting the quantized\nlabel of these features. For a given number of quantization targets c, random \u201ccodebook\" vectors\nv0, \u00b7 \u00b7 \u00b7 , vc\u22121 are chosen in an embedding space. The discrete label of the speech feature is obtained\nby first projecting the feature into the embedding space by a randomly initialized, frozen projection\nmatrix and then finding the closest codebook vector. The index of this codebook vector is identified\nas the label of the speech feature. Cosine similarity is used as the distance measure for determining\nthe code.\nWe note that while w2v-BERT [21] pre-training has proven to be an effective method for unsupervised\npre-training, it requires an additional quantization module which introduces more complexity. As we\nincrease the model size and language coverage, the learnt codebook module proves costly to tune and\ncan impede progress of model development. Meanwhile, the BEST-RQ algorithm does not require\nsuch a module, making it a more scalable method for pre-training.\n2.2.1\nMulti-softmax\nInstead of utilizing a single codebook [10], we use multiple codebooks to improve BEST-RQ training\nin this study. More precisely, we use N softmax layers to produce N probability predictions from\nthe output of the encoder to compare against N independent quantization targets obtained from the\nmasked speech features. We train the network with equal weights for each softmax layer. The use of\nmultiple codebooks improves the stability and convergence of the model.\n2.3\nSelf-training: Noisy Student Training\nWe utilize noisy student training (NST) [7,8] to generate pseudo-labeled data to augment supervised\ntraining. This is done by first training a teacher model with augmentation on a supervised set, then\nusing that teacher to generate transcripts for unlabeled audio data. A heuristic filtering method based\non the ratio between the number of words and audio length is used to filter the pseudo-labeled data.\nThe pseudo-labeled data is mixed with supervised data to train the student model.\n2.4\nChunk-wise Attention for Long-form ASR\nIn many real-world applications, ASR systems are required to transcribe minutes- or hours-long\naudio. This poses significant challenges to many end-to-end ASR systems, as these ASR systems\n6\nare usually trained on much shorter segments, typically less than 30 seconds. For systems that use\nattention-based encoders, it is impractical to use global attention to attend to the entire audio. Local\nself attention, which only attends to the fixed length of left and right context, is thus widely used.\nFor example, in BEST-RQ pre-training, only 128 left and 128 right context frames are used for local\nself attention. However, stacking many local self attention layers creates a significant receptive field\nmismatch between training and inference. The left figure in Fig. 4 illustrates this issue with a network\nconsisting of 4 local self attention layers, each using only 1 left and 1 right context frames. Since the\ncontext is leaked in every layer, the receptive field width grows linearly with respect to the number of\nlayers; for a big encoder like that of the Conformer-2B, this means that the receptive field width for\nthe encoder output is longer than 327 seconds. During training, the model is trained with at most 30\nseconds speech segments, while at inference time, when minutes or hours long audio is fed to the\nmodel, the encoder needs to process over 300 seconds of audio to produce one encoder output\u2014a\npattern it has never trained on. Our empirical observations demonstrate that, under this train-test\nmismatch, these models with deep architectures and high capacity suffer from high deletion errors.\nWe henceforth refer to this problem as the \u201clong-form (performance) degradation\" problem.\nReceptive field for y3\nReceptive field for y4\nReceptive field for y0,..y3 \nReceptive field for y4,..y8 \nChunk-wise Self-Attention\nLocal Self-Attention\nFigure 4: Comparing receptive fields of two networks with 4 layers of local self attention and chunk-\nwise attention.\nTo solve this problem, we propose a simple modification to the attention mechanism; the attention is\nrestricted to audio chunks. This is illustrated on the right side of Fig. 4, in which 8 frames are divided\ninto 2 chunks, and the attention is performed within each chunk. In this case, there is no context\nleaking in the attention layer, and thus the receptive field width is independent of the number of layers.\nIn our experiments an 8-second chunk resulted in the best recognition quality vs. computational cost\ntrade-off.\nIt is worthwhile to note there are a few other works in the literature which also modify the attention\npattern to deal with the long-form audio in ASR, e.g., [61\u201366]. Though conceptually similar to block\nprocessing (e.g. [65,66]), chunk-wise attention is more flexible. Block processing is performed at\nthe input feature level, which limits the encoder layers to the context frame at the current chunk. On\nthe other hand, chunk-wise attention allows other layers in the encoder (e.g., convolution layers) to\nprocess contextual frames beyond the current chunk. Compared with Whisper [1], which segments\nthe audio into 30 second chunks and uses a heuristic process to carry the decoder states over, we only\nchunk the attention state, and allow the decoder to access the entire encoder output. We also use\neither a CTC or RNN-T decoder to decode on long-form audio, neither of which have been observed\nto hallucinate compared to attention-based sequence-to-sequence decoders. We observe our systems\nare robust on long-form ASR tasks with a simpler decoding process on long-form speech signals.\n7\n\u2026\nOn Speech input\nOn paired input\nOn text input\nFigure 5: Overview of MOST text injection. The left-most panel depicts MOST training on unlabeled\nspeech input; the center panel depicts training on paired speech and text input; the right-most panel\ndepicts training on unlabeled text data.\n2.5\nMulti-Objective Supervised Pre-training: BEST-RQ + text-injection\nIn addition to pre-training with unlabeled speech, we add an additional stage of Multi-Objective\nSupervised pre-Training (MOST) as shown in Fig. 5, where we train the model jointly on unlabeled\nspeech, unlabeled text and paired speech and text data. The training loss for this procedure is\nbased on the text-injection loss including duration modeling and consistency regularization as in\n[13], to which we add a weighted BEST-RQ loss for the encoder of the model. MOST yields two\nbenefits: (i) Training with paired speech and text data with alignment losses results in learning\nspeech representations that are better aligned with text, improving quality on tasks like ASR and\nAST that require mapping the acoustics of the speech signal to text. (ii) Training simultaneously on\nunlabeled text in a model that learns speech and text representations jointly improves the robustness\nof learned representations, especially on low resource languages and domains, also generalizing to\nnew languages with no paired data seen during training [67].\nThe key architectural components for constructing the text-injection loss as utilized in our approach\ninclude: (i) A speech-only encoder that utilizes a convolutional sub-sampling feature encoder and a\nsingle conformer layer. For continued pre-training the feature encoder is initialized from the BEST-\nRQ pre-trained checkpoint while the conformer layer is initialized randomly. (ii) A text-only encoder\nthat consists of an embedding layer, an upsampler, and a conformer layer block. The upsampler used\nin this work is a learned duration based upsampling model [13], though a fixed or random repetition\nupsampler can also be used for text-injection [47,53]. All components are initialized randomly. (iii)\nA shared conformer encoder initialized from the pre-trained BEST-RQ speech encoder. (iv) The\nBEST-RQ speech softmax layers initialized from the BEST-RQ checkpoint. (v) The decoder unit\nwhich is initialized randomly.\nThe main idea of text-injection (e.g. [13,53,54]) is to produce joint, co-aligned embeddings of speech\nand text as sequences in the same embedding space. Given this embedding space, text data with no\nassociated audio can contribute to improving the speech task. The speech and text encoders presented\nabove are intended to produce these embeddings, which need to be matched in the embedding space\nand are also required to be co-aligned in the time dimension. The embeddings enable the text data to\ncontribute to preparing the model for downstream tasks.\nTo achieve these objectives, the architecture as presented above is trained using three types of data,\neach contributing to different types of losses:\n1. The unlabeled speech passes through the shared encoder and the BEST-RQ softmax layers\nto contribute to the BEST-RQ loss.\n8\n2. The paired speech-text data serves multiple functions.\n\u2022 The labeled speech flows through the speech encoder, the shared encoder and the\ndecoder unit and contributes to the standard ASR loss computed against the paired text.\nHere, the speech-text alignments of the paired data are extracted from the decoder unit\nand used to train the duration upsampler within the text encoder.\n\u2022 The text of the paired data also passes through the text encoder. The encoded text\nsequence is used to compute a consistency loss against the encoded speech sequence.\nThis loss is used to train solely the text encoder\u2014the speech encoder weights are frozen\nfor this particular forward-propagation.\n3. The unlabeled text data contributes to a reconstruction loss. This loss is constructed by\npassing the text through the text encoder, then masking chunks of the feature sequence\nproduced. These masked text features live in the same embedding space as masked speech\nfeatures, and thus can be passed through the shared encoder and the decoder unit to compute\nthe ASR loss against the original text. This is the reconstruction loss used to train the model.\nFor training stability, MOST proceeds in two stages\u2014we first train solely on paired data to learn\nstable decoder alignments for 20k steps. We then train the duration upsampler and activate the losses\nfor unlabeled text. We refer the reader to [13] for further details.\nWhen fine-tuning for ASR, we initialize the feature encoder of the ASR model with the speech feature\nencoder, initialize the conformer block with the shared conformer encoder, and add a randomly\ninitialized task-specific transducer.\nIn the MOST set-up, the speech and text representations live in a shared representation space, thereby\nallowing us to utilize text machine translation (MT) data during the fine-tuning stage of AST tasks.\nWe follow the same approach described in [13,20] and report the AST results with joint fine-tuning\nfor models prepared with MOST.\n2.6\nResidual Adaptation with a Frozen Encoder\nIdeally, the fine-tuning process of the model should be scalable with the number of downstream\ntasks while in reality, fine-tuning the pre-trained USM individually for various domains and tasks\nbecomes prohibitively expensive. In order to mitigate this issue, we explore a lightweight alternative\n[19] to training the full network where residual adapters with a small number of parameters are\nadded for each individual language while the pre-trained USM is entirely frozen during fine-tuning.\nWe experiment with adding two parallel adapters to each Conformer block, whose parameter count\namounts to 2% of the original pre-trained USM, and fine-tune the adapters on downstream language\ntasks. When serving the model, the adapter is dynamically loaded according to the language of the\ninput batch [68,69]. This enables one to conduct inference on 100+ languages while keeping the\ntotal number of parameters manageable by re-using the same parameters and computation process for\nthe majority of the time. We also find that training the adapter versus fine-tuning the entire model can\nreduce over-fitting especially when the training data is limited.\n2.7\nTraining Details\nData Processing: The audio is uniformly sampled to 16 kHz quality\u2014any audio with a different\nnative sampling rate is either up-sampled or down-sampled. The audio is then featurized into 128-\ndimensional log-mel filterbank coefficients. Graphemes are used to tokenize the text for FLEURS\nin-domain fine-tuning, while word-piece models (WPMs) [70] are used for tokenization for all other\ntasks.\nBEST-RQ: We follow default masking and quantization parameters of BEST-RQ as in [10]. We use\na 16 codebook multi-softmax loss to stabilize training and improve performance as described in 5.1.\nWe do not use EMA for pre-training.\nMOST: We follow the text encoder and decoder architecture described in [13] but use 4k sentence-\npiece models (SPMs). We use a single 1536-dimensional Conformer layer as the speech encoder and\nConformer-2B encoder as the shared encoder. We mix un-transcribed speech, unspoken text, and\ntranscribed speech in each batch with fixed batch sizes of, respectively, 4096, 8192, and 1024. The\nmodel is initialized with the BEST-RQ pre-trained encoder. MOST employs a curriculum learning\n9\nschedule where training initially is conducted with un-transcribed speech and paired speech-text data,\nand unspoken text is utilized only after 20k steps. The joint training employing all three types of data\nlasts for another 100K steps.\nSupervised Training: We use two separate optimizers for the encoder parameters and the decoder\nparameters of the network [71]. For USM-CTC and USM-LAS, we train the model for 100k steps\nwith 2048 batch size. For in-domain experiments, the checkpoint is selected based on development\nset performance.\nTraining Large Models: We use the GShard [72] framework with the GSPMD backend [73] to\ntrain our large models on TPUs.\n3\nDatasets\n3.1\nAudio Data\nFigure 6: The video category and length distribution of YT-513-U.\nThe following audio datasets are used in this report to train our models:\n\u2022 YouTube SUPervised Plus (YT-SUP+):\n\u2013 YT-SUP: 90k hours of segmented, labeled audio across 75 languages.\n\u2013 YT-Pseudo-Labeled: 100k hours of segmented, pseudo-labeled en-US audio from YT-\nNTL-U. The pseudo-labels are generated by a 600M CTC model trained on YT-SUP\nen-US data.\n\u2022 YouTube Next Thousand Languages Unsupervised (YT-NTL-U): 12.1M hours of seg-\nmented, unlabeled audio, including:\n\u2013 YT-55-U: 12M hours of segmented, unlabeled audio on 55 rich resource languages\nidentified by YouTube production language id models.\n\u2013 YT-513-U: 100k hours of segmented, unlabeled audio across 513 tail languages not\ncovered by YouTube production language id models. These languages are identified by\nvendors.\nLet us expand upon how each dataset has been constructed.\nYT-SUP+: YT-SUP is a dataset with audio from videos that have user-uploaded transcripts from 75\nlanguages. We group consecutive segments into a longer unit similar to [57]. The maximal sequence\nlength for training is 30 seconds. The total amount of training data is 90k hours, ranging from English\n(en-US) (3.5k hours) to Amharic (Am-Et) (150 hours). We also introduce an additional 100k hours of\nen-US audio from YT-NTL-U to YT-SUP. We choose to generate pseudo-labels on this dataset using\na 600M-parameter CTC YT teacher model trained on YT-SUP. Each audio is randomly segmented\nbetween 5 to 15 seconds.\nYT-55-U: YT-55-U is built by first randomly collecting 3 million hours of audio from \"speech-heavy\"\nYouTube videos, filtered by language. The 3 million hours of audio is then further segmented by the\nYT teacher model. Instead of using a teacher model as in [3], the non-speech segments identified\nby a Voice Activity Detection (VAD) model are removed to yield approximately 1 million hours of\n10\nunlabeled audio data. Later, we use a YouTube production language identification model to select 55\nlanguages from that audio.\nYT-513-U: We create an additional dataset called YT-513-U to ensure coverage of lower resource\nlanguages in our pre-training dataset. We reached out to vendors and native speakers to identify YT\nvideos containing speech in specific long tail languages, collecting a dataset of unlabeled speech in\n513 languages. Vendors were tasked with ensuring a variety of domains, voices, and content in the\nvideos that are collected in each language. These videos are segmented into speech segments using a\nVAD model, resulting in a total of 102k hours of speech. Our final YT-513-U dataset contains 88\nlanguages with over 500 hours of speech each, 237 languages with between 100-500 hours, and 188\nlanguages with less than 100 hours of data. The languages chosen for this collection are wide-ranging,\nwith a majority of our data corresponding to languages from South Asia, Southeast Asia, West Africa,\nand East Africa. The distribution of video categories and lengths in our dataset are depicted in\nFigure 6.\nIn addition to YouTube data, we also include public data for MOST training:\n\u2022 Public Unsupervised (Pub-U): Following [20], we use approximately 429k hours of\nunlabeled speech data in 51 languages. It includes: 372k hours of speech data spanning\n23 languages from VoxPopuli [74], read speech data in 25 languages drawn from the v6.1\nrelease of Common Voice [75], 50k hours of read books data in eight European languages\nfrom Multilingual LibriSpeech [76] and 1k hours of telephonic conversation data spanning\n17 African and Asian languages from BABEL [77].\n\u2022 Public Supervised (Pub-S): Similar to [20], our public supervised set includes approxi-\nmately 1.3k hours of speech and transcript data spanning 14 languages from VoxPopuli,\n10 hour training splits for each of the 8 MLS languages, and 1k hours of data spanning 17\nlanguages from the Babel ASR task.\nNote that the public data is only used for in-domain pre-training and is excluded for training the\ngeneric USM-LAS/CTC models. This allows us to treat the public task performance as out-of-domain\nbenchmarks for the USM-LAS/CTC models.\n3.2\nText Data\nWeb-NTL: For pre-training with unlabeled text, we use a web-crawled corpus of monolingual text\ncontaining over 28B sentences [78]. The dataset spans 1140 languages, 205 of which have over 1M\nsentences and 199 of which have between 100k and 1M sentences. We up-sample lower resource\nlanguages using temperature-based sampling [79] with T = 3.0. More details about the dataset and\nthe mining approach have been described in Section 2 of [78].\n3.3\nDownstream Benchmarks\n3.3.1\nSpeech Recognition (ASR)\nWe present our results on two public tasks, SpeechStew [2] and FLEURS [16], and an internal\nbenchmark on YouTube.\nThe SpeechStew [2] dataset is assembled by putting together seven public speech corpora\u2014AMI [80],\nCommon Voice [81], English Broadcast News3, LibriSpeech [82], Switchboard/Fisher4, TED-LIUM\nv3 [83,84] and Wall Street Journal5, which are all standard benchmarks [85\u201387] covering different\ndomains in en-US.\nThe FLEURS [16] dataset is a publicly available, multi-way parallel dataset of 10 hours of read\nspeech in 102 languages spanning 7 geo-groups. We restrict our use of the dataset to its ASR\nbenchmark. Among the 102 languages present in the FLEURS benchmark, we select 62 to serve as a\nsub-group to compare our generic ASR system with Whisper [1], as those languages are covered by\nthe training sets of both models. We also report full results for in-domain fine-tuning and adaptation.\nUnlike [16], we report both WER and CER metrics, as CER is inappropriate as an indicator of\n3Linguistic data consortium (LDC) datasets LDC97S44, LDC97T22, LDC98S71 and LDC98T28.\n4LDC datasets LDC2004T19, LDC2005T19, LDC2004S13, LDC2005S13 and LDC97S62.\n5LDC datasets LDC93S6B and LDC94S13B.\n11\nTable 3: WERs (%) across multiple tasks for multiple settings compared against pre-existing baselines,\nwith the exception of CoVoST 2, for which the BLEU score is presented. For the YouTube long-form\nset, we select the top-25 languages Whisper was trained on and exclude all languages for which\nWhisper produces > 40% WER to reduce the noise introduced by LAS hallucination in the Whisper\nmodel. For FLEURS, we report both the WER and the CER for our models. \u2020Results omitted for the\nWhisper-shortform model on the YouTube long-form dataset as the model has a high deletion problem\non this set. \u2021The Whisper-shortform model uses segmented decoding to reduce its hallucination\nproblem on CORAAL. \u00a7Our adapter setup adds about 2.3% of the total parameters while keeping the\nencoder frozen from pre-training.\nTask\nMultilingual Long-form ASR\nMultidomain en-US\nMultilingual ASR\nAST\nDataset\nYouTube\nCORAAL\nSpeechStew\nFLEURS\nCoVoST 2\nLangauges\nen-US\n18\n73\nen-US\nen-US\n62\n102\n21\nPrior Work (single model)\nWhisper-longform\n17.7\n27.8\n-\n23.9\n12.8\nWhisper-shortform\u2020\n-\n-\n-\n13.2\u2021\n11.5\n36.6\n-\n29.1\nOur Work (single model)\nUSM-LAS\n14.4\n19.0\n29.8\n11.2\n10.5\n12.5\n-\n-\nUSM-CTC\n13.7\n18.7\n26.7\n12.1\n10.8\n15.5\n-\n-\nPrior Work (in-domain fine-tuning)\nBigSSL [3]\n14.8\n-\n-\n-\n7.5\n-\n-\n-\nMaestro [67]\n7.2\n25.2\nMaestro-U [67]\n26.0 (8.7)\nOur Work (in-domain fine-tuning)\nUSM\n13.2\n-\n-\n-\n7.4\n13.5\n19.2 (6.9)\n28.7\nUSM-M\n12.5\n-\n-\n-\n7.0\n11.8\n17.4 (6.5)\n30.7\nOur Work (frozen encoder)\nUSM-M-adapter\u00a7\n-\n-\n-\n-\n7.5\n12.4\n17.6 (6.7)\n29.6\nperformance for some languages. When presenting the error rate metrics, we use CER for Chinese,\nJapanese, Thai, Lao, and Burmese to be consistent with Whisper [1].\nThe test set for the YouTube domain consists of utterances from 73 languages with an average of\n15 hours of audio per language, the audio length for each individual language ranging from 1 to 24\nhours. The audio is transcribed manually from popular YouTube videos, each with a duration of up to\n30 minutes.\n3.3.2\nSpeech Translation (AST)\nFollowing [20], we use CoVoST 2 [18] to benchmark multilingual speech translation. We evaluate\nthe multilingual XX-to-English task that covers translation from 21 source languages into English.\nDepending on the language, the training data ranges in size from 1 - 264 hours.\nBesides speech translation data, we also add text-to-text translation data for training the model as\nin [20]. This dataset includes the text translation data from CoVoST 2 combined with all data from\neither WMT or TED Talks, as available.\n4\nKey Results\n4.1\nRobust Speech Recognition for Massively Multilingual Tasks\nIn this section, we compare the performance of our models against public baselines, including\nWhisper large-v26 [1], which has been trained on 680k hours of weakly supervised data across 100\nlanguages.\nFor the massively multilingual speech recognition test dataset from YouTube, we observe that Whisper\nhallucinates in many languages, resulting in a WER exceeding 100%. For a reasonable comparison,\nwe restrict the language set on which we compare the performance USM against Whisper by first\nselecting the top-25 languages from the training data for Whisper and further excluding languages for\nwhich Whisper produces > 40% WER. We also use segmented decoding for Whisper with 30-second\nsegments to further reduce the effect of hallucinations. As shown in Table 3, our USM-LAS and\n6Whisper large-v2 on Github (https://github.com/openai/whisper.git, revision b4308c4) is used for evaluation.\n12\nUSM-CTC models outperform Whisper by a wide margin on YouTube en-US, despite training on\nsignificantly less supervised data (3.5k hours versus Whisper\u2019s 400k hours [1]). While the USM-LAS\nmodel also requires segmented decoding to reduce long-form degradation as discussed in section\n2.4, it is far more robust, out-performing Whisper by a relative 30% WER on those 18 languages.\nUSM-CTC does not exhibit long-form performance degradation and achieves the best performance\non YouTube.\nOn the out-of-domain long-form CORAAL set, both USM-CTC and USM-LAS outperform Whisper\nby more than 10% relative WER. USM-CTC and USM-LAS similary outperform Whisper on\nSpeechStew, whose training data the models have not had access to.\nWe further compare the multilingual performance of the models on the held-out set from FLEURS.\nAs shown in Table 3, USM-LAS and USM-CTC both outperform Whisper by 66% relative WER,\ndespite using a smaller amount of multilingual supervised data (90k versus Whisper\u2019s 117k, when\nen-US is excluded). USM-LAS consistently outperforms USM-CTC for short-form ASR tasks.\n4.2\nMassively Multilingual Results Beyond 100 Languages\nThe lower part of Table 3 shows our results for in-domain fine-tuning. Our pre-trained model improves\nthe FLEURS benchmark significantly, even when using only 10 hours per language. Compared to the\nprevious SoTA in [67], our model achieves a 30% relative improvement in terms of WER across 102\nlanguages. Our results show that while generic speech models can be powerful, performance is still\nmaximized by in-domain fine-tuning.\n4.3\nMOST Produces Robust Representations that Generalize to New Domains\nMOST training aligns the representations of speech and text by training simultaneously on the two\nmodalities. We investigate whether MOST representations are useful for adapting the model to new\ndomains by freezing the entire learned encoder produced by MOST and adjusting a small amount of\nparameters added to the network by residual adapters. As shown in Table 3, by adding only 2% to\nthe total number of parameters, the MOST representation model (USM-M-adapter) only performs\nslightly worse than the fine-tuning baselines, still showing competitive performance on downstream\nASR and AST tasks. The small number of parameters being trained in this approach makes it feasible\nto extend our system to a large number of new domains and new tasks, even with a limited amount of\ntraining data, such as in FLEURS.\n4.4\nPushing the Quality of ASR on Unseen Languages\nTable 4: Noisy student training for unseen languages. WERs (%) for the teacher adapter models and\nthe student models are presented. The relative improvement (%) of the student models can be found\nin the last column.\nLanguages\nWhisper-v2\n# hrs in YT-NTL\nUSM-LAS-Adapter\nUSM-M + pseudo label\nRel. Imprv.\nHausa (ha)\n88.9\n2175.0\n24.5\n22.8\n7.5\nKazakh (kk)\n37.7\n196.0\n11.8\n10.9\n8.3\nShona (sn)\n121.0\n247.0\n29.1\n22.2\n31.1\nPashto (ps)\n93.7\n254.0\n36.0\n35.4\n1.7\nYoruba (yo)\n94.8\n1292.0\n33.4\n30.6\n9.2\nTail languages often do not have paired transcriptions for supervised learning\u2014we refer to these\nlanguages as unseen languages, as the model has not seen paired data for these lanugages during\ntraining. To create pseudo-labels for these languages, we first build a USM-LAS-Adapter by attaching\nresidual adapters to USM-LAS and training them using FLEURS data. By using the USM-LAS-\nAdapter as a teacher, we can now transcribe the unlabeled data in the unseen languages as part\nof the YT-NTL dataset. As shown in Table 4, we observe consistent wins for all languages on\nthe FLEURS benchmark. For some languages, the improvement is larger than 30%. This further\ndemonstrates the robustness of the USM-LAS model\u2014despite using only 10 hours of out of domain\ndata from FLEURS, the USM-LAS-Adapter is able to transcribe YouTube data to produce meaningful\nrecognition results that lead to these improvements. We find the approach of training adapter models\n13\non small datasets and utilizing them for pseudo-labeling to be a promising route for scaling up the\nnumber of languages that can be transcribed by USMs.\n4.5\nUSMs are Strong AST Models\nThe multi-lingual speech translation performance of fine-tuned USMs are shown in Table 3. We\nfind that we are already comparable to the CoVoST 2 SoTA BLEU score by fine-tuning the speech-\nonly USM. We note that the previous SoTA uses 125k hours of supervised speech translation data\ncompared to the 859 hours of data used by the USM. After MOST training, USM-M can use both\nspeech and text as training input. By introducing text-to-text machine translation (MT) data during\nfine-tuning, USM-M is able to achieve an unprecedented > 30 BLEU on CoVoST (a 1 BLEU increase\nfrom SoTA).\n5\nAnalysis and Ablations\n5.1\nMulti-Softmax Loss for BEST-RQ\nWe observe a consistent > 5% relative improvement in ASR and AST benchmarks by increasing\nthe number of the softmax groups in the multi-softmax loss for BEST-RQ training from 1 to 16, as\nshown in Table 5. We also find that using multiple softmax groups significantly reduces performance\nvariation across different pre-training runs and improves convergence speed.\nTable 5: YT-55 versus YT-NTL across different domains, with and without multi-softmax groups. For\nsimplicity, we report CER for FLEURS. For CoVoST, we report the BLEU score. YT-NTL covers 27\nadditional languages not covered in YT-55.\nModel\npre-train Set\n# Params (B)\n# Softmax\nFLEURS (CER)\nCoVoST (BLEU)\n.\n102 langs\n27 langs\nConformer-0.6B\nYT-55\n0.6\n1\n9.5\n-\n20.9\nConformer-2B\nYT-55\n2.0\n1\n7.9\n9.5\n26.6\nConformer-2B\nYT-NTL-U\n2.0\n1\n7.4\n8.5\n27.5\nConformer-2B\nYT-NTL-U\n2.0\n16\n6.9\n8.1\n28.7\n5.2\nModel and Language Scaling\nWe find that scaling up the model size and increasing the language coverage of the pre-training\ndataset greatly benefits the performance of the USMs, as demonstrated in Table 5. In particular, we\nfind a 10% relative improvement of ASR and AST performance by using YT-NTL vs. YT-55 for\npre-training, despite the fact that each newly added language in YT-NTL contains approximately 500\nhours of speech\u2014a relatively small amount. As could be expected, the relative gains on the newly\ncovered languages are more substantial than those on other languages.\n5.3\nBEST-RQ is a Scalable Self-supervised Learner\nBEST-RQ has been shown to outperform or be comparable to other prominent pre-training methods\nfor speech recognition, including wav2vec 2.0 and W2v-BERT in the original work in which it was\nintroduced [10]. Here we investigate its comparative performance and scaling properties, similar\nto what has been done for wav2vec 2.0 in [3] and W2v-BERT in [20]. We utilize the set-up of\npre-training the model using YT-55 and fine-tuning it on CoVoST 2. As shown in Table 6, our results\nindicate that for the Conformer-0.6B, W2v-BERT and BEST-RQ perform similarly, but BEST-RQ\nobtains greater gains when scaled up. A contributing factor to this can be that W2v-BERT is more\nprone to codebook collapse and training instabilities at the 2B scale, while BEST-RQ by construction\ndoesn\u2019t suffer from codebook collapse.\n5.4\nChunk-wise attention for robust long-form speech recognition\nFig. 7 depicts the long-form performance degradation issue as described in section 2.4. In the figure,\nwe see that for the shallow Conformer model with 17 layers, using a small local self attention context\n14\nTable 6: BLEU scores for the CoVoST 2 X \u2192En task to compare BEST-RQ against W2v-BERT.\nHigher is better.\nX \u2192English\nhigh\nmid\nlow\nall\nPrevious Work\nXLS-R (0.3B) [33]\n30.6\n18.9\n5.1\n13.2\nXLS-R (1B) [33]\n34.3\n25.5\n11.7\n19.3\nXLS-R (2B) [33]\n36.1\n27.7\n15.1\n22.1\nConformer-0.6B\nW2v-BERT\n35.6\n25.3\n13.4\n20.4\nBEST-RQ\n32.5\n25.6\n14.7\n20.7\nConformer-2B\nW2v-BERT\n36.0\n27.8\n15.6\n22.4\nBEST-RQ\n35.8\n31.3\n21.5\n26.6\nFigure 7: The word error rate measured on the YouTube en-US long-form test set for Conformer\nmodels with varying depth.\n(65) length, the word error rate measured on the long-form test set gradually improves as the training\nprogresses. With a deeper model that has 48 layers but roughly the same number of parameters,\nhowever, the larger receptive field mismatch results in higher test WERs as the training step increases.\nTable 7 demonstrates that chunk-wise attention is able to address the long-form degradation issue and\nshow robust performance across four different languages\u2014en-US (English), ru-RU (Russian), ko-KR\n(Korean), and uk-UA (Ukrainian). We compare chunk-wise attention models with an 8-second chunk\nsize (CW-8s in Table 7) against local self attention models which uses 128 context frames in each\nconformer layer (LSA-128). We note that further increasing the context window size of the local self\nattention model results in high deletion error rates on all languages of the YouTube long-form test\nsets. These results show that the chunk-wise attention models do not exhibit long-form performance\ndegradation and are able to improve upon the performance of the local self attention models operating\nat the maximum allowed receptive field length.\nTable 7: Chunk-wise attention. WER (%) is reported on the YouTube long-form set.\nModel\n# Params (B)\n# Layers\nen-US\nru-RU\nko-KR\nuk-UA\nLSA-128\n0.6\n24\n16.2\n16.6\n26.2\n15.5\nCW-8s\n0.6\n24\n12.5\n14.7\n19.5\n15.3\n5.5\nTPU Serving Capacity of USM-CTC Models\nIn section 4, we have demonstrated that USM-CTC models are powerful generic ASR models\nwith reliable long-form transcription performance and excellent generalization properties. Here we\n15\nTable 8: RTF for USM-2B.\nModel\nbf-16\nStreaming\n# Params (B)\nTPU [88]\nBatch Size\n1.0/RTF\nConformer-0.1B\nY\nY\n0.1\nTPUv4i\n64\n3047\nConformer-0.6B\nN\nN\n0.6\nTPUv4i\n64\n1920\nConformer-2B\nN\nN\n2.0\nTPUv4i\n32\n827\nmeasure the serving capacity of the USM-CTC model as represented by the real time factor (RTF) in\nan ideal setup where we assume that each batch sent to TPU is fully packed along the time axis. The\nresults of these measurements are presented in Table 8. Surprisingly, we find that the 2B-paramter\nUSM-CTC model is only 3.9\u00d7 slower than the 100M-parameter streaming model [89], primarily\ndue to the fact that our models operate at batch processing mode. This result demonstrates that the\nUSM-CTC can be used as an offline transcriber efficiently on TPUs (or GPUs).\n6\nDiscussion\nIn this report, we put forward a practical and flexible approach for training speech understanding\nmodels capable of scaling speech recognition to hundreds of languages. We conclude the report with\nsummarizing insights gained in the process:\nUnlabeled versus weakly labeled data: We believe diverse unlabeled data is more practical to\nacquire for building usable ASR for tail languages than weakly labeled data. We have demonstrated\nthat collaborating with native speakers to identify unsupervised data in hundreds of tail languages\ncan be an effective route to improving recognition performance on low resource languages.\nIn-domain data is best: We have demonstrated that we can build a robust ASR system across many\ndomains by utilizing a large amount of unsupervised data and a small amount of labeled data. Our\nresults, however, also confirm that the most effective way to optimize the performance for a given\ndomain is to use in-domain data to fine-tune the model.\nCTC vs RNN-T vs LAS: The best transducer depends on the downstream task. A large pre-trained\nmodel with a frozen encoder can allow experimenters to test different transducers quickly and select\nthe optimal transducer for their purpose.\nAcknowledgments\nWe would like to thank Alexis Conneau, Min Ma, Shikhar Bharadwaj, Sid Dalmia, Jiahui Yu, Jian\nCheng, Paul Rubenstein, Ye Jia, Justin Snyder, Vincent Tsang, Yuanzhong Xu, Tao Wang, Anusha\nRamesh, Calum Barnes, Salem Haykal for useful discussions.\nWe appreciate valuable feedback and support from Eli Collins, Jeff Dean, Sissie Hsiao, Zoubin\nGhahramani. Special thanks to Austin Tarango, Lara Tumeh, and Jason Porta for their guidance\naround responsible AI practices.\nReferences\n[1] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition\nvia large-scale weak supervision,\u201d arXiv preprint arXiv:2212.04356, 2022.\n[2] W. Chan, D. Park, C. Lee, Y. Zhang, Q. Le, and M. Norouzi, \u201cSpeechstew: Simply mix all available speech\nrecognition data to train one large neural network,\u201d arXiv preprint arXiv:2104.02133, 2021.\n[3] Y. Zhang, D. S. Park, W. Han, J. Qin, A. Gulati, J. Shor, A. Jansen, Y. Xu, Y. Huang, S. Wang, Z. Zhou, B. Li,\nM. Ma, W. Chan, J. Yu, Y. Wang, L. Cao, K. C. Sim, B. Ramabhadran, T. N. Sainath, F. Beaufays, Z. Chen,\nQ. V. Le, C.-C. Chiu, R. Pang, and Y. Wu, \u201cBigssl: Exploring the frontier of large-scale semi-supervised\nlearning for automatic speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16,\nno. 6, pp. 1519\u20131532, 2022.\n[4] B. Li, R. Pang, T. N. Sainath, A. Gulati, Y. Zhang, J. Qin, P. Haghani, W. R. Huang, and M. Ma, \u201cScaling\nend-to-end models for large-scale multilingual asr,\u201d arXiv preprint arXiv:2104.14830, 2021.\n16\n[5] X. Li, F. Metze, D. R. Mortensen, A. W. Black, and S. Watanabe, \u201cAsr2k: Speech recognition for around\n2000 languages without audio,\u201d arXiv preprint arXiv:2209.02842, 2022.\n[6] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning\nof speech representations,\u201d arXiv preprint arXiv:2006.11477, 2020.\n[7] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, \u201cSelf-training with noisy student improves imagenet\nclassification,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 10 687\u201310 698.\n[8] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. V. Le, \u201cImproved noisy student\ntraining for automatic speech recognition,\u201d arXiv preprint arXiv:2005.09629, 2020.\n[9] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu et al.,\n\u201cConformer: Convolution-augmented transformer for speech recognition,\u201d arXiv preprint arXiv:2005.08100,\n2020.\n[10] C.-C. Chiu, J. Qin, Y. Zhang, J. Yu, and Y. Wu, \u201cSelf-supervised learning with random-projection quantizer\nfor speech recognition,\u201d in Proceedings of the 39th International Conference on Machine Learning,\nser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,\nG. Niu, and S. Sabato, Eds., vol. 162.\nPMLR, 17\u201323 Jul 2022, pp. 3915\u20133924. [Online]. Available:\nhttps://proceedings.mlr.press/v162/chiu22a.html\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers\nfor language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018.\n[12] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, G. Wang, and P. Moreno, \u201cInjecting text in self-\nsupervised speech pretraining,\u201d arXiv preprint arXiv:2108.12226, 2021.\n[13] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. Moreno, A. Bapna, and H. Zen, \u201cMaestro: Matched\nspeech text representations through modality matching,\u201d arXiv preprint arXiv:2204.03409, 2022.\n[14] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling\nunsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international\nconference on Machine learning, 2006, pp. 369\u2013376.\n[15] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary\nconversational speech recognition,\u201d in 2016 IEEE international conference on acoustics, speech and signal\nprocessing (ICASSP).\nIEEE, 2016, pp. 4960\u20134964.\n[16] A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and\nA. Bapna, \u201cFleurs: Few-shot learning evaluation of universal representations of speech,\u201d arXiv preprint\narXiv:2205.12446, 2022.\n[17] T. Kendall and C. Farrington, \u201cThe corpus of regional african american language. version 2021.07. eugene,\nor: The online resources for african american language project,\u201d 2021.\n[18] C. Wang, A. Wu, and J. Pino, \u201cCoVoST 2 and massively multilingual speech-to-text translation,\u201d in\ninterspeech, 2021.\n[19] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, \u201cTowards a unified view of parameter-efficient\ntransfer learning,\u201d in International Conference on Learning Representations, 2022. [Online]. Available:\nhttps://openreview.net/forum?id=0RDcd5Axok\n[20] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, and A. Conneau,\n\u201cmslam: Massively multilingual joint pre-training for speech and text,\u201d arXiv preprint arXiv:2202.01374,\n2022.\n[21] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, \u201cW2v-bert: Combining contrastive\nlearning and masked language modeling for self-supervised speech pre-training,\u201d in 2021 IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU).\nIEEE, 2021, pp. 244\u2013250.\n[22] W.-N. Hsu and J. Glass, \u201cExtracting domain invariant features by unsupervised learning for robust automatic\nspeech recognition,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2018, pp. 5614\u20135618.\n[23] Y.-A. Chung and J. Glass, \u201cSpeech2vec: A sequence-to-sequence framework for learning word embeddings\nfrom speech,\u201d arXiv preprint arXiv:1803.08976, 2018.\n[24] A. v. d. Oord, Y. Li, and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d arXiv\npreprint arXiv:1807.03748, 2018.\n[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, \u201cAn unsupervised autoregressive model for speech\nrepresentation learning,\u201d arXiv preprint arXiv:1904.03240, 2019.\n[26] J. Chorowski, R. J. Weiss, S. Bengio, and A. van den Oord, \u201cUnsupervised speech representation learning\nusing wavenet autoencoders,\u201d IEEE/ACM transactions on audio, speech, and language processing, vol. 27,\nno. 12, pp. 2041\u20132053, 2019.\n17\n[27] S. Schneider, A. Baevski, R. Collobert, and M. Auli, \u201cwav2vec: Unsupervised pre-training for speech\nrecognition,\u201d arXiv preprint arXiv:1904.05862, 2019.\n[28] A. Baevski, S. Schneider, and M. Auli, \u201cvq-wav2vec: Self-supervised learning of discrete speech represen-\ntations,\u201d arXiv preprint arXiv:1910.05453, 2019.\n[29] S. Ling, Y. Liu, J. Salazar, and K. Kirchhoff, \u201cDeep contextualized acoustic representations for semi-\nsupervised speech recognition,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nIEEE, 2020, pp. 6429\u20136433.\n[30] A. Baevski, M. Auli, and A. Mohamed, \u201cEffectiveness of self-supervised pre-training for speech recogni-\ntion,\u201d arXiv preprint arXiv:1911.03912, 2019.\n[31] M. Riviere, A. Joulin, P.-E. Mazar\u00e9, and E. Dupoux, \u201cUnsupervised pretraining transfers well across\nlanguages,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2020, pp. 7414\u20137418.\n[32] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. v. d. Oord, \u201cLearning robust and multilingual speech\nrepresentations,\u201d arXiv preprint arXiv:2001.11128, 2020.\n[33] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino\net al., \u201cXls-r: Self-supervised cross-lingual speech representation learning at scale,\u201d arXiv preprint\narXiv:2111.09296, 2021.\n[34] G. Zavaliagkos and T. Colthurst, \u201cUtilizing untranscribed training data to improve performance,\u201d in DARPA\nBroadcast News Transcription and Understanding Workshop, Landsdowne, 1998, pp. 301\u2013305.\n[35] L. Lamel, J. luc Gauvain, and G. Adda, \u201cLightly supervised acoustic model training,\u201d in Proc. ISCA ITRW\nASR2000, 2000, pp. 150\u2013154.\n[36] S. Novotney and R. Schwartz, \u201cAnalysis of low-resource acoustic model self-training,\u201d in Tenth Annual\nConference of the International Speech Communication Association, 2009.\n[37] S. Thomas, M. L. Seltzer, K. Church, and H. Hermansky, \u201cDeep neural network features and semi-\nsupervised training for low resource speech recognition,\u201d in 2013 IEEE international conference on\nacoustics, speech and signal processing.\nIEEE, 2013, pp. 6704\u20136708.\n[38] B. Li, T. N. Sainath, R. Pang, and Z. Wu, \u201cSemi-supervised training for end-to-end models via weak\ndistillation,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2019, pp. 2837\u20132841.\n[39] J. Kahn, A. Lee, and A. Hannun, \u201cSelf-training for end-to-end speech recognition,\u201d in ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp.\n7084\u20137088.\n[40] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave, V. Pratap, A. Sriram, V. Liptchinsky, and\nR. Collobert, \u201cEnd-to-end asr: from supervised to semi-supervised learning with modern architectures,\u201d in\narXiv, 2019.\n[41] S. H. K. Parthasarathi and N. Strom, \u201cLessons from building acoustic models with a million hours of\nspeech,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2019, pp. 6670\u20136674.\n[42] W.-N. Hsu, A. Lee, G. Synnaeve, and A. Hannun, \u201cSemi-supervised speech recognition via local prior\nmatching,\u201d arXiv preprint arXiv:2002.10336, 2020.\n[43] Q. Xu, T. Likhomanenko, J. Kahn, A. Hannun, G. Synnaeve, and R. Collobert, \u201cIterative pseudo-labeling\nfor speech recognition,\u201d arXiv preprint arXiv:2005.09267, 2020.\n[44] Z. Chen, A. Rosenberg, Y. Zhang, H. Zen, M. Ghodsi, Y. Huang, J. Emond, G. Wang, B. Ramabhadran, and\nP. J. Moreno, \u201cSemi-Supervision in ASR: Sequential MixMatch and Factorized TTS-Based Augmentation,\u201d\nin Proc. Interspeech 2021, 2021, pp. 736\u2013740.\n[45] A. Renduchintala, S. Ding, M. Wiesner, and S. Watanabe, \u201cMulti-modal data augmentation for end-to-end\nasr,\u201d arXiv preprint arXiv:1803.10299, 2018.\n[46] A. Bapna, Y.-a. Chung, N. Wu, A. Gulati, Y. Jia, J. H. Clark, M. Johnson, J. Riesa, A. Conneau, and\nY. Zhang, \u201cSlam: A unified encoder for speech and language modeling via speech-text joint pre-training,\u201d\narXiv preprint arXiv:2110.10329, 2021.\n[47] S. Thomas, B. Kingsbury, G. Saon, and H.-K. J. Kuo, \u201cIntegrating text inputs for training and adapting rnn\ntransducer asr models,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2022, pp. 8127\u20138131.\n[48] Y. Cheng, Y. Zhang, M. Johnson, W. Macherey, and A. Bapna, \u201cMu2slam: Multitask, multilingual speech\nand language models,\u201d arXiv preprint arXiv:2212.09553, 2022.\n18\n[49] Z.-H. Zhang, L. Zhou, J. Ao, S. Liu, L. Dai, J. Li, and F. Wei, \u201cSpeechut: Bridging speech and text with\nhidden-unit for encoder-decoder based speech-text pre-training,\u201d in Conference on Empirical Methods in\nNatural Language Processing, 2022.\n[50] Z.-H. Zhang, S. Chen, L. Zhou, Y. Wu, S. Ren, S. Liu, Z. Yao, X. Gong, L. Dai, J. Li, and F. Wei,\n\u201cSpeechlm: Enhanced speech pre-training with unpaired textual data,\u201d ArXiv, vol. abs/2209.15329, 2022.\n[51] S. Khurana, A. Laurent, and J. R. Glass, \u201cSamu-xlsr: Semantically-aligned multimodal utterance-level\ncross-lingual speech representation,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, pp.\n1493\u20131504, 2022.\n[52] X. Zhou, J. Wang, Z. Cui, S. Zhang, Z. Yan, J. Zhou, and C. Zhou, \u201cMmspeech: Multi-modal multi-task\nencoder-decoder pre-training for speech recognition,\u201d ArXiv, vol. abs/2212.00500, 2022.\n[53] T. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen, B. Li, W. Wang, and T. Strohman,\n\u201cJoist: A joint speech and text streaming model for asr,\u201d in 2022 IEEE Spoken Language Technology\nWorkshop (SLT).\nIEEE, 2023, pp. 52\u201359.\n[54] Z. Meng, W. Wang, R. Prabhavalkar, T. N. Sainath, T. Chen, E. Variani, Y. Zhang, B. Li, A. Rosenberg,\nand B. Ramabhadran, \u201cJeit: Joint end-to-end model and internal language model training for speech\nrecognition,\u201d in ICASSP, 2023, 2023.\n[55] Z. Meng, T. Chen, R. Prabhavalkar, Y. Zhang, G. Wang, K. Audhkhasi, J. Emond, T. Strohman, B. Ramab-\nhadran, W. R. Huang et al., \u201cModular hybrid autoregressive transducer,\u201d in 2022 IEEE Spoken Language\nTechnology Workshop (SLT).\nIEEE, 2023, pp. 197\u2013204.\n[56] C.-C. Chiu, W. Han, Y. Zhang, R. Pang, S. Kishchenko, P. Nguyen, A. Narayanan, H. Liao, S. Zhang,\nA. Kannan et al., \u201cA comparison of end-to-end models for long-form speech recognition,\u201d in 2019 IEEE\nautomatic speech recognition and understanding workshop (ASRU).\nIEEE, 2019, pp. 889\u2013896.\n[57] Z. Lu, Y. Pan, T. Doutre, P. Haghani, L. Cao, R. Prabhavalkar, C. Zhang, and T. Strohman, \u201cInput length\nmatters: Improving rnn-t and mwer training for long-form telephony speech recognition,\u201d arXiv preprint\narXiv:2110.03841, 2021.\n[58] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, \u201cTransformer-xl: Attentive language\nmodels beyond a fixed-length context,\u201d arXiv preprint arXiv:1901.02860, 2019.\n[59] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012.\n[60] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring\nthe limits of transfer learning with a unified text-to-text transformer,\u201d The Journal of Machine Learning\nResearch, vol. 21, no. 1, pp. 5485\u20135551, 2020.\n[61] B. Ramabhadran, K. Audhkhasi, P. J. M. Mengibar, and T. Chen, \u201cMixture model attention: Flexible\nstreaming and non-streaming automatic speech recognition,\u201d in Proceedings of Interspeech, 2021, 2021.\n[62] L. Lu, C. Liu, J. Li, and Y. Gong, \u201cExploring transformers for large-scale speech recognition,\u201d arXiv\npreprint arXiv:2005.09684, 2020.\n[63] X. Chen, Y. Wu, Z. Wang, S. Liu, and J. Li, \u201cDeveloping real-time streaming transformer transducer for\nspeech recognition on large-scale dataset,\u201d in International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2021, pp. 5904\u20135908.\n[64] C. Wu, Y. Wang, Y. Shi, C.-F. Yeh, and F. Zhang, \u201cStreaming transformer-based acoustic models using\nself-attention with augmented memory,\u201d arXiv preprint arXiv:2005.08042, 2020.\n[65] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and M. Seltzer, \u201cEmformer: Efficient\nmemory transformer based acoustic model for low latency streaming speech recognition,\u201d in International\nConference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp. 6783\u20136787.\n[66] E. Tsunoo, Y. Kashiwagi, T. Kumakura, and S. Watanabe, \u201cTransformer asr with contextual block process-\ning,\u201d in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU).\nIEEE, 2019,\npp. 427\u2013433.\n[67] Z. Chen, A. Bapna, A. Rosenberg, Y. Zhang, B. Ramabhadran, P. Moreno, and N. Chen, \u201cMaestro-\nu: Leveraging joint speech-text representation learning for zero supervised speech asr,\u201d arXiv preprint\narXiv:2210.10027, 2022.\n[68] F. Biadsy, Y. Chen, X. Zhang, O. Rybakov, A. Rosenberg, and P. J. Moreno, \u201cA scalable model specialization\nframework for training and inference using submodels and its application to speech model personalization,\u201d\nin Proc. Interspeech 2022.\nISCA, 2022, pp. 5125\u20135129.\n[69] K. Tomanek, V. Zayats, D. Padfield, K. Vaillancourt, and F. Biadsy, \u201cResidual adapters for parameter-\nefficient asr adaptation to atypical and accented speech,\u201d in Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, 2021, pp. 6751\u20136760.\n[70] M. Schuster and K. Nakajima, \u201cJapanese and korean voice search,\u201d in 2012 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2012, pp. 5149\u20135152.\n19\n[71] Y. Zhang, J. Qin, D. S. Park, W. Han, C.-C. Chiu, R. Pang, Q. V. Le, and Y. Wu, \u201cPushing the limits of\nsemi-supervised learning for automatic speech recognition,\u201d arXiv preprint arXiv:2010.10504, 2020.\n[72] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, \u201cGShard:\nScaling giant models with conditional computation and automatic sharding,\u201d CoRR, vol. abs/2006.16668,\n2020. [Online]. Available: https://arxiv.org/abs/2006.16668\n[73] Y. Xu, H. Lee, D. Chen, B. A. Hechtman, Y. Huang, R. Joshi, M. Krikun, D. Lepikhin, A. Ly,\nM. Maggioni, R. Pang, N. Shazeer, S. Wang, T. Wang, Y. Wu, and Z. Chen, \u201cGSPMD: general and scalable\nparallelization for ML computation graphs,\u201d CoRR, vol. abs/2105.04663, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2105.04663\n[74] C. Wang, M. Rivi\u00e8re, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux,\n\u201cVoxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning\nand interpretation,\u201d arXiv preprint arXiv:2101.00390, 2021.\n[75] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and\nG. Weber, \u201cCommon voice: A massively-multilingual speech corpus,\u201d arXiv preprint arXiv:1912.06670,\n2019.\n[76] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, \u201cMls: A large-scale multilingual dataset for\nspeech research,\u201d arXiv preprint arXiv:2012.03411, 2020.\n[77] M. J. F. Gales, K. Knill, A. Ragni, and S. P. Rath, \u201cSpeech recognition and keyword spotting for low-\nresource languages: Babel project research at cued,\u201d in SLTU, 2014.\n[78] A. Bapna, I. Caswell, J. Kreutzer, O. Firat, D. van Esch, A. Siddhant, M. Niu, P. Baljekar, X. Garcia,\nW. Macherey et al., \u201cBuilding machine translation systems for the next thousand languages,\u201d arXiv preprint\narXiv:2205.03983, 2022.\n[79] N. Arivazhagan, A. Bapna, O. Firat, D. Lepikhin, M. Johnson, M. Krikun, M. X. Chen, Y. Cao, G. Foster,\nC. Cherry et al., \u201cMassively multilingual neural machine translation in the wild: Findings and challenges,\u201d\narXiv preprint arXiv:1907.05019, 2019.\n[80] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,\nM. Kronenthal et al., \u201cThe ami meeting corpus: A pre-announcement,\u201d in International workshop on\nmachine learning for multimodal interaction.\nSpringer, 2005, pp. 28\u201339.\n[81] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and\nG. Weber, \u201cCommon voice: A massively-multilingual speech corpus,\u201d arXiv preprint arXiv:1912.06670,\n2019.\n[82] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public\ndomain audio books,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2015, pp. 5206\u20135210.\n[83] A. Rousseau, P. Del\u00e9glise, and Y. Esteve, \u201cTed-lium: an automatic speech recognition dedicated corpus.\u201d\nin LREC, 2012, pp. 125\u2013129.\n[84] F. Hernandez, V. Nguyen, S. Ghannay, N. Tomashenko, and Y. Esteve, \u201cTed-lium 3: twice as much data\nand corpus repartition for experiments on speaker adaptation,\u201d in International conference on speech and\ncomputer.\nSpringer, 2018, pp. 198\u2013208.\n[85] J.-L. Gauvain, L. F. Lamel, G. Adda, and M. Adda-Decker, \u201cThe limsi continuous speech dictation\nsystem: evaluation on the arpa wall street journal task,\u201d in Proceedings of ICASSP\u201994. IEEE International\nConference on Acoustics, Speech and Signal Processing, vol. 1.\nIEEE, 1994, pp. I\u2013557.\n[86] F. Kubala, J. Davenport, H. Jin, D. Liu, T. Leek, S. Matsoukas, D. Miller, L. Nguyen, F. Richardson,\nR. Schwartz et al., \u201cThe 1997 bbn byblos system applied to broadcast news transcription,\u201d in Proc. DARPA\nBroadcast News Transcription and Understanding Workshop.\nMorgan Kaufmann, 1998, pp. 35\u201340.\n[87] S. Chen, M. Gales, P. Gopalakrishnan, R. Gopinath, H. Printz, D. Kanevsky, P. Olsen, and L. Polymenakos,\n\u201cIbm\u2019s lvcsr system for transcription of broadcast news used in the 1997 hub4 english evaluation,\u201d in\nProceedings of the Speech Recognition Workshop.\nCiteseer, 1998.\n[88] N. P. Jouppi, D. H. Yoon, M. Ashcraft, M. Gottscho, T. B. Jablin, G. Kurian, J. Laudon, S. Li, P. Ma, X. Ma\net al., \u201cTen lessons from three generations shaped google\u2019s tpuv4i: Industrial product,\u201d in 2021 ACM/IEEE\n48th Annual International Symposium on Computer Architecture (ISCA).\nIEEE, 2021, pp. 1\u201314.\n[89] B. Li, A. Gulati, J. Yu, T. N. Sainath, C.-C. Chiu, A. Narayanan, S.-Y. Chang, R. Pang, Y. He, J. Qin\net al., \u201cA better and faster end-to-end model for streaming asr,\u201d in ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp. 5634\u20135638.\n20\n"
    }
]