SYNTHESIZED SECTION
===================

abstract black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. the way forward is to design models that are inherently interpretable. this perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision. access through your institution buy or subscribe this is a preview of subscription content, access via your institution access options access through your institution access nature and 54 other nature portfolio journals get nature+, our best-value online-access subscription 27,99 € / 30 days cancel any time learn more subscribe to this journal receive 12 digital issues and online access to articles 111,21 € per year only 9,27 € per issue learn more buy this article • purchase on springerlink • instant access to the full article pdf. 39,95 € prices may be subject to local taxes which are calculated during checkout additional access options: • log in • learn about institutional subscriptions • read our faqs • contact customer support fig. 1: a fictional depiction of the accuracy–interpretability trade-off. 2: saliency does not explain anything except where the network is looking. 48, indicating that parts of the test image on the left are similar to prototypical parts of training examples. [image] similar content being viewed by others [image] achieving interpretable machine learning by functional decomposition of black-box models into explainable predictor effects article open access 03 november 2025 [image] thermodynamics-inspired explanations of artificial intelligence article open access 09 september 2024 [image] black box problem and african views of trust article open access 14 october 2023 references 1.