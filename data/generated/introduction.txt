introduction
machine learning algorithms have penetrated every aspect of our lives. algorithms make movie
recommendations, suggest products to buy, and who to date. they are increasingly used in high-stakes
scenarios such as loans [113] and hiring decisions [19, 39]. there are clear benefits to algorithmic
decision-making; unlike people, machines do not become tired or bored [45, 119], and can take into
account orders of magnitude more factors than people can. however, like people, algorithms are
vulnerable to biases that render their decisions “unfair” [6, 121]. in the context of decision-making,
fairness is the absence of any prejudice or favoritism toward an individual or group based on
their inherent or acquired characteristics. thus, an unfair algorithm is one whose decisions are
skewed toward a particular group of people. a canonical example comes from a tool used by courts
in the united states to make pretrial detention and release decisions. the software, correctional
offender management profiling for alternative sanctions (compas), measures the risk of a person
to recommit another crime. judges use compas to decide whether to release an offender, or to keep
him or her in prison. an investigation into the software found a bias against african-americans:1
1https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
authors’ address: usc, information sciences institute 4676 admiralty way, suite 1001 marina del rey, ca 90292
this material is based upon work supported by the defense advanced research projects agency (darpa) under agreement
no. hr0011890019.
arxiv:1908.09635v3  [cs.lg]  25 jan 2022
2
mehrabi et al.
compas is more likely to have higher false positive rates for african-american offenders than
caucasian offenders in falsely predicting them to be at a higher risk of recommitting a crime or
recidivism. similar findings have been made in other areas, such as an ai system that judges beauty
pageant winners but was biased against darker-skinned contestants,2 or facial recognition software in
digital cameras that overpredicts asians as blinking.3 these biased predictions stem from the hidden
or neglected biases in data or algorithms.
in this survey we identify two potential sources of unfairness in machine learning outcomes—
those that arise from biases in the data and those that arise from the algorithms. we review research
investigating how biases in data skew what is learned by machine learning algorithms, and nuances
in the way the algorithms themselves work to prevent them from making fair decisions—even when
the data is unbiased. furthermore, we observe that biased algorithmic outcomes might impact user
experience, thus generating a feedback loop between data, algorithms and users that can perpetuate
and even amplify existing sources of bias.
we begin the review with several highly visible real-world cases of where unfair machine learning
algorithms have led to suboptimal and discriminatory outcomes in section 2. in section 3, we
describe the different types and sources of biases that occur within the data-algorithms-users loop
mentioned above. next, in section 4, we present the different ways that the concept of fairness has
been operationalized and studied in the literature. we discuss the ways in which these two concepts
are coupled. last, we will focus on different families of machine learning approaches, how fairness
manifests differently in each one, and the current state-of-the-art for tackling them in section 5,
followed by potential areas of future work in each of the domains in section 6.
2
real-world examples of algorithmic unfairness
with the popularity of ai and machine learning over the past decades, and their prolific spread in
different applications, safety and fairness constraints have become a significant issue for researchers
and engineers. machine learning is used in courts to assess the probability that a defendant recommits
a crime. it is used in different medical fields, in childhood welfare systems [35], and autonomous
vehicles. all of these applications have a direct effect in our lives and can harm our society if not
designed and engineered correctly, that is with considerations to fairness. [123] has a list of the
applications and the ways these ai systems affect our daily lives with their inherent biases, such as
the existence of bias in ai chatbots, employment matching, flight routing, and automated legal aid for
immigration algorithms, and search and advertising placement algorithms. [67] discusses examples
of how bias in the real world can creep into ai and robotic systems, such as bias in face recognition
applications, voice recognition, and search engines. therefore, it is important for researchers and
engineers to be concerned about the downstream applications and their potential harmful effects
when modeling an algorithm or a system.
2.1
systems that demonstrate discrimination
compas is an exemplar of a discriminatory system. in addition to this, discriminatory behavior was
also evident in an algorithm that would deliver advertisements promoting jobs in science, technology,
engineering, and math (stem) fields [88]. this advertisement was designed to deliver advertise-
ments in a gender-neutral way. however, less women compared to men saw the advertisement due to
gender-imbalance which would result in younger women being considered as a valuable subgroup
and more expensive to show advertisements to. this optimization algorithm would deliver ads in
a discriminatory way although its original and pure intention was to be gender-neutral. bias in
2https://www.theguardian.com/technology/2016/sep/08/artificial-intelligence-beauty-contest-doesnt-like-black-people
3http://content.time.com/time/business/article/0,8599,1954643,00.html
a survey on bias and fairness in machine learning
3
facial recognition systems [128] and recommender systems [140] have also been largely studied and
evaluated and in many cases shown to be discriminative towards certain populations and subgroups.
in order to be able to address the bias issue in these applications, it is important for us to know where
these biases are coming from and what we can do to prevent them.
we have enumerated the bias in compas, which is a widely used commercial risk assessment
software. in addition to its bias, it also contains performance issues when compared to humans. when
compared to non-expert human judgment in a study, it was discovered to be not any better than a
normal human [46]. it is also interesting to note that although compas uses 137 features, only 7 of
those were presented to the people in the study. [46] further argues that compas is not any better
than a simple logistic regression model when making decisions. we should think responsibly, and
recognize that the application of these tools, and their subsequent decisions affect peoples’ lives;
therefore, considering fairness constraints is a crucial task while designing and engineering these
types of sensitive tools. in another similar study, while investigating sources of group unfairness
(unfairness across different groups is defined later), the authors in [145] compared savry, a tool
used in risk assessment frameworks that includes human intervention in its process, with automatic
machine learning methods in order to see which one is more accurate and more fair. conducting
these types of studies should be done more frequently, but prior to releasing the tools in order to
avoid doing harm.
2.2
assessment tools
an interesting direction that researchers have taken is introducing tools that can assess the amount
of fairness in a tool or system. for example, aequitas [136] is a toolkit that lets users to test
models with regards to several bias and fairness metrics for different population subgroups. aequitas
produces reports from the obtained data that helps data scientists, machine learning researchers, and
policymakers to make conscious decisions and avoid harm and damage toward certain populations.
ai fairness 360 (aif360) is another toolkit developed by ibm in order to help moving fairness
research algorithms into an industrial setting and to create a benchmark for fairness algorithms to
get evaluated and an environment for fairness researchers to share their ideas [11]. these types of
toolkits can be helpful for learners, researchers, and people working in the industry to move towards
developing fair machine learning application away from discriminatory behavior.
3
bias in data, algorithms, and user experiences
most ai systems and algorithms are data driven and require data upon which to be trained. thus,
data is tightly coupled to the functionality of these algorithms and systems. in the cases where the
underlying training data contains biases, the algorithms trained on them will learn these biases and
reflect them into their predictions. as a result, existing biases in data can affect the algorithms using
the data, producing biased outcomes. algorithms can even amplify and perpetuate existing biases
in the data. in addition, algorithms themselves can display biased behavior due to certain design
choices, even if the data itself is not biased. the outcomes of these biased algorithms can then be fed
into real-world systems and affect users’ decisions, which will result in more biased data for training
future algorithms. for example, imagine a web search engine that puts specific