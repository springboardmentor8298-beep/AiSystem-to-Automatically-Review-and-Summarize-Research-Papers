conclusion
in this survey we introduced problems that can adversely affect ai systems in terms of bias and
unfairness. the issues were viewed primarily from two dimensions: data and algorithms. we illus-
trated problems that demonstrate why fairness is an important issue. we further showed examples of
26
mehrabi et al.
the potential real-world harm that unfairness can have on society—such as applications in judicial
systems, face recognition, and promoting algorithms. we then went over the definitions of fairness
and bias that have been proposed by researchers. to further stimulate the interest of readers, we
provided some of the work done in different areas in terms of addressing the biases that may affect ai
systems and different methods and domains in ai, such as general machine learning, deep learning
and natural language processing. we then further subdivided the fields into a more fine-grained
analysis of each subdomain and the work being done to address fairness constraints in each. the
hope is to expand the horizons of the readers to think deeply while working on a system or a method
to ensure that it has a low likelihood of causing potential harm or bias toward a particular group.
with the expansion of ai use in our world, it is important that researchers take this issue seriously
and expand their knowledge in this field. in this survey we categorized and created a taxonomy of
what has been done so far to address different issues in different domains regarding the fairness issue.
other possible future work and directions can be taken to address the existing problems and biases in
ai that we discussed in the previous sections.
8
acknowledgments
this material is based upon work supported by the defense advanced research projects agency
(darpa) under agreement no. hr0011890019. we would like to thank the organizers, speakers
and the attendees at the ivado-mila 2019 summer school on bias and discrimination in ai. we
would like to also thank brian hu zhang and shreya shankar.
9
appendix
9.1
datasets for fairness research
aside from the existence of bias in datasets, there are datasets that are specifically used to address
bias and fairness issues in machine learning. there are also some datasets that are introduced to
target the issues and biases previously observed in older existing datasets. below we list some of the
widely known datasets that have the characteristics discussed in this survey.
9.1.1
uci adult dataset. uci adult dataset, also known as "census income" dataset, contains
information, extracted from the 1994 census data about people with attributes such as age, occupation,
education, race, sex, marital-status, native-country, hours-per-week etc., indicating whether the
income of a person exceeds $50k/yr or not. it can be used in fairness-related studies that want to
compare gender or race inequalities based on people’s annual incomes, or various other studies [7].
9.1.2
german credit dataset. the german credit dataset contains 1000 credit records contain-
ing attributes such as personal status and sex, credit score, credit amount, housing status etc. it can
be used in studies about gender inequalities on credit-related issues [47].
9.1.3
winobias. the winobias dataset follows the winograd format and has 40 occupations in
sentences that are referenced to human pronouns. there are two types of challenge sentences in the
dataset requiring linkage of gendered pronouns to either male or female stereotypical occupations. it
was used in the coreference resolution study to certify if a system has gender bias or not—in this
case, towards stereotypical occupations [168].
9.1.4
communities and crime dataset. the communities and crime dataset gathers infor-
mation from different communities in the united states related to several factors that can highly
influence some common crimes such as robberies, murders or rapes. the data includes crime data
obtained from the 1990 us lemas survey and the 1995 fbi unified crime report. it also contains
socio-economic data from the 1990 us census.
a survey on bias and fairness in machine learning
27
9.1.5
compas dataset. the compas dataset contains records for defendants from broward
county indicating their jail and prison times, demographics, criminal histories, and compas risk
scores from 2013 to 2014 [89].
9.1.6
recidivism in juvenile justice dataset. the recidivism in juvenile justice dataset
contains all juvenile offenders between ages 12-17 who committed a crime between years 2002 and
2010 and completed a prison sentence in 2010 in catalonia’s juvenile justice system [145].
9.1.7
pilot parliaments benchmark dataset. the pilot parliaments benchmark dataset, also
known as ppb, contains images of 1270 individuals in the national parliaments from three european
(iceland, finland, sweden) and three african (rwanda, senegal, south africa) countries. this
benchmark was released to have more gender and race balance, diversity, and representativeness
[24].
9.1.8
diversity in faces dataset. the diversity in faces (dif) is an image dataset collected for
fairness research in face recognition. dif is a large dataset containing one million annotations for
face images. it is also a diverse dataset with diverse facial features, such as different craniofacial
distances, skin color, facial symmetry and contrast, age, pose, gender, resolution, along with diverse
areas and ratios [107].
dataset name
reference size
area
uci adult dataset
[7]
48,842 income records
social
german credit dataset
[47]
1,000 credit records
financial
pilot parliaments benchmark dataset
[24]
1,270 images
facial images
winobias
[168]
3,160 sentences
coreference resolution
communities and crime dataset
[129]
1,994 crime records
social
compas dataset
[89]
18,610 crime records
social
recidivism in juvenile justice dataset
[28]
4,753 crime records
social
diversity in faces dataset
[107]
1 million images
facial images
table 5. most widely used datasets in the fairness domain with additional information about each of
the datasets including their size and area of concentration.
references
[1] alekh agarwal, miroslav dudik, and zhiwei steven wu. 2019. fair regression: quantitative definitions and reduction-
based algorithms. in international conference on machine learning. 120–129.
[2] sina aghaei, mohammad javad azizi, and phebe vayanos. 2019. learning optimal and fair decision trees for non-
discriminative decision-making. in proceedings of the aaai conference on artificial intelligence, vol. 33. 1418–1426.
[3] nazanin alipourfard, peter g fennell, and kristina lerman. 2018. can you trust the trend?: discovering simpson’s
paradoxes in social data. in proceedings of the eleventh acm international conference on web search and data
mining. acm, 19–27.
[4] nazanin alipourfard, peter g fennell, and kristina lerman. 2018. using simpson’s paradox to discover interesting
patterns in behavioral data. in twelfth international aaai conference on web and social media.
[5] alexander amini, ava soleimany, wilko schwarting, sangeeta bhatia, and daniela rus. 2019. uncovering and
mitigating algorithmic bias through learned latent structure. (2019).
[6] julia angwin, jeff larson, surya mattu, and lauren kirchner. 2016. machine bias: there’s software used across the
country to predict future criminals. and it’s biased against blacks. propublica 2016.
[7] a. asuncion and d.j. newman. 2007. uci machine learning repository.
http://www.ics.uci.edu/$\sim$mlearn/
{mlr}epository.html
[8] arturs backurs, piotr indyk, krzysztof onak, baruch schieber, ali vakilian, and tal wagner. 2019. scalable fair
clustering. in proceedings of the 36th international conference on machine learning (proceedings of machine
learning research, vol. 97), kamalika chaudhuri and ruslan salakhutdinov (eds.). pmlr, long beach, california,
usa, 405–413. http://proceedings.mlr.press/v97/backurs19a.html
28
mehrabi et al.
[9] ricardo baeza-yates. 2018. bias on the web. commun. acm 61, 6 (may 2018), 54–61.
https://doi.org/10.1145/
3209581
[10] samuel barbosa, dan cosley, amit sharma, and roberto m. cesar-jr. 2016. averaging gone wrong: using time-
aware analyses to better understand behavior. (april 2016), 829–841.
[11] rachel ke bellamy, kuntal dey, michael hind, samuel c hoffman, stephanie houde, kalapriya kannan, pranay
lohia, jacquelyn martino, sameep mehta, aleksandra mojsilovic, et al. 2018. ai fairness 360: an extensible toolkit
for detecting, understanding, and mitigating unwanted algorithmic bias. arxiv preprint arxiv:1810.01943 (2018).
[12] emily m. bender and batya friedman. 2018. data statements for natural language processing: toward mitigating
system bias and enabling better science. transactions of the association for computational linguistics 6 (2018),
587–604. https://doi.org/10.1162/tacl_a_00041
[13] misha benjamin, paul gagnon, negar rostamzadeh, chris pal, yoshua bengio, and alex shee. [n.d.]. towards
standardization of data licenses: the montreal data license. ([n. d.]).
[14] richard berk, hoda heidari, shahin jabbari, matthew joseph, michael kearns, jamie morgenstern, seth neel, and
aaron roth. 2017. a convex framework for fair regression. arxiv:1706.02409 [cs.lg]
[15] richard berk, hoda heidari, shahin jabbari, michael kearns, and aaron roth. [n.d.]. fairness in criminal justice risk
assessments: the state of the art. sociological methods & research ([n. d.]), 0049124118782533.
[16] peter j bickel, eugene a hammel, and j william o’connell. 1975. sex bias in graduate admissions: data from
berkeley. science 187, 4175 (1975), 398–404.
[17] rdp binns. 2018. fairness in machine learning: lessons from political philosophy. journal of machine learning
research (2018).
[18] colin r blyth. 1972. on simpson’s paradox and the sure-thing principle. j. amer. statist. assoc. 67, 338 (1972),
364–366.
[19] miranda bogen and aaron rieke. 2018. help wanted: an examination of hiring algorithms, equity. technical report.
and bias. technical report, upturn.
[20] tolga bolukbasi, kai-wei chang, james y zou, venkatesh saligrama, and adam t kalai. 2016. man is to computer
programmer as woman is to homemaker? debiasing word embeddings. in advances in neural information processing
systems. 4349–4357.
[21] shikha bordia and samuel bowman. 2019. identifying and reducing gender bias in word-level language models. in
proceedings of the 2019 conference of the north american chapter of the association for computational linguistics:
student research workshop. 7–15.
[22] avishek bose and william hamilton. 2019. compositional fairness constraints for graph embeddings. in international
conference on machine learning. 715–724.
[23] marc-etienne brunet, colleen alkalay-houlihan, ashton anderson, and richard zemel. 2019. understanding the
origins of bias in word embeddings. in proceedings of the 36th international conference on machine learning
(proceedings of machine learning research, vol. 97), kamalika chaudhuri and ruslan salakhutdinov (eds.). pmlr,
long beach, california, usa, 803–811. http://proceedings.mlr.press/v97/brunet19a.html
[24] joy buolamwini and timnit gebru. 2018. gender shades: intersectional accuracy disparities in commercial gender
classification. in proceedings of the 1st conference on fairness, accountability and transparency (proceedings of
machine learning research, vol. 81), sorelle a. friedler and christo wilson (eds.). pmlr, new york, ny, usa,
77–91. http://proceedings.mlr.press/v81/buolamwini18a.html
[25] toon calders and sicco verwer. 2010. three naive bayes approaches for discrimination-free classification. data
mining and knowledge discovery 21, 2 (2010), 277–292.
[26] aylin caliskan, joanna j bryson, and arvind narayanan. 2017. semantics derived automatically from language corpora
contain human-like biases. science 356, 6334 (2017), 183–186.
[27] flavio calmon, dennis wei, bhanukiran vinzamuri, karthikeyan natesan ramamurthy, and kush r varshney. 2017.
optimized pre-processing for discrimination prevention. in advances in neural information processing systems 30,
i. guyon, u. v. luxburg, s. bengio, h. wallach, r. fergus, s. vishwanathan, and r. garnett (eds.). curran associates,
inc., 3992–4001. http://papers.nips.cc/paper/6988-optimized-pre-processing-for-discrimination-prevention.pdf
[28] manel capdevila, marta ferrer, and eulália luque. 2005. la reincidencia en el delito en la justicia de menores. centro
de estudios jurídicos y formación especializada, generalitat de catalunya. documento no publicado (2005).
[29] allison jb chaney, brandon m stewart, and barbara e engelhardt. 2018. how algorithmic confounding in recom-
mendation systems increases homogeneity and decreases utility. in proceedings of the 12th acm conference on
recommender systems. acm, 224–232.
[30] jiahao chen, nathan kallus, xiaojie mao, geoffry svacha, and madeleine udell. 2019. fairness under unawareness:
assessing disparity when protected class is unobserved. in proceedings of the conference on fairness, accountability,
and transparency. acm, 339–348.
a survey on bias and fairness in machine learning
29
[31] xingyu chen, brandon fain, liang lyu, and kamesh munagala. 2019. proportionally fair clustering. in international
conference on machine learning. 1032–1041.
[32] s. chiappa. 2019. path-specific counterfactual fairness. in thirty-third aaai conference on artificial intelligence.
7801–7808.
[33] s. chiappa and w. s. isaac. 2019. a causal bayesian networks viewpoint on fairness. in e. kosta, j. pierson,
d. slamanig, s. fischer-hübner, s. krenn (eds) privacy and identity management. fairness, accountability, and
transparency in the age of big data. privacy and identity 2018. ifip advances in information and communication
technology, vol. 547. springer, cham.
[34] alexandra chouldechova. 2017. fair prediction with disparate impact: a study of bias in recidivism prediction
instruments. big data 5, 2 (2017), 153–163.
[35] alexandra chouldechova, diana benavides-prado, oleksandr fialko, and rhema vaithianathan. 2018. a case study
of algorithm-assisted decision making in child maltreatment hotline screening decisions. in proceedings of the 1st
conference on fairness, accountability and transparency (proceedings of machine learning research, vol. 81),
sorelle a. friedler and christo wilson (eds.). pmlr, new york, ny, usa, 134–148. http://proceedings.mlr.press/
v81/chouldechova18a.html
[36] alexandra chouldechova and aaron roth. 2018. the frontiers of fairness in machine learning. arxiv preprint
arxiv:1810.08810 (2018).
[37] john s chuang, olivier rivoire, and stanislas leibler. 2009. simpson’s paradox in a synthetic microbial system.
science 323, 5911 (2009), 272–275.
[38] kevin a clarke. 2005. the phantom menace: omitted variable bias in econometric research. conflict management and
peace science 22, 4 (2005), 341–352.
[39] lee cohen, zachary c. lipton, and yishay mansour. 2019. efficient candidate screening under multiple tests and
implications for fairness. arxiv:1905.11361 [cs.lg]
[40] united states. equal employment opportunity commission. [n.d.]. eeoc compliance manual. [washington, d.c.] :
u.s. equal employment opportunity commission, [1992].
[41] sam corbett-davies, emma pierson, avi feller, sharad goel, and aziz huq. 2017. algorithmic decision making and
the cost of fairness. in proceedings of the 23rd acm sigkdd international conference on knowledge discovery and
data mining. acm, 797–806.
[42] elliot creager, david madras, joern-henrik jacobsen, marissa weis, kevin swersky, toniann pitassi, and richard
zemel. 2019. flexibly fair representation learning by disentanglement. in international conference on machine
learning. 1436–1445.
[43] brian d’alessandro, cathy o’neil, and tom lagatta. 2017. conscientious classification: a data scientist’s guide to
discrimination-aware classification. big data 5, 2 (2017), 120–134.
[44] david danks and alex john london. 2017. algorithmic bias in autonomous systems.. in ijcai. 4691–4697.
[45] shai danziger, jonathan levav, and liora avnaim-pesso. 2011. extraneous factors in judicial decisions. proceedings
of the national academy of sciences 108, 17 (2011), 6889–6892.
[46] julia dressel and hany farid. 2018. the accuracy, fairness, and limits of predicting recidivism. science advances 4, 1
(2018). https://doi.org/10.1126/sciadv.aao5580 arxiv:https://advances.sciencemag.org/content/4/1/eaao5580.full.pdf
[47] dheeru dua and casey graff. 2017. uci machine learning repository. http://archive.ics.uci.edu/ml
[48] cynthia dwork, moritz hardt, toniann pitassi, omer reingold, and richard zemel. 2012. fairness through awareness.
in proceedings of the 3rd innovations in theoretical computer science conference (cambridge, massachusetts) (itcs
’12). acm, new york, ny, usa, 214–226. https://doi.org/10.1145/2090236.2090255
[49] cynthia dwork, nicole immorlica, adam tauman kalai, and max leiserson. 2018. decoupled classifiers for
group-fair and efficient machine learning. in proceedings of the 1st conference on fairness, accountability and
transparency (proceedings of machine learning research, vol. 81), sorelle a. friedler and christo wilson (eds.).
pmlr, new york, ny, usa, 119–133. http://proceedings.mlr.press/v81/dwork18a.html
[50] golnoosh farnadi, behrouz babaki, and lise getoor. 2018. fairness in relational domains. in proceedings of the
2018 aaai/acm conference on ai, ethics, and society (new orleans, la, usa) (aies ’18). acm, new york, ny,
usa, 108–114. https://doi.org/10.1145/3278721.3278733
[51] michael feldman, sorelle a. friedler, john moeller, carlos scheidegger, and suresh venkatasubramanian. 2015.
certifying and removing disparate impact. in proceedings of the 21th acm sigkdd international conference on
knowledge discovery and data mining (sydney, nsw, australia) (kdd ’15). association for computing machinery,
new york, ny, usa, 259–268. https://doi.org/10.1145/2783258.2783311
[52] joel escudé font and marta r costa-jussà. 2019. equalizing gender biases in neural machine translation with word
embeddings techniques. arxiv preprint arxiv:1901.03116 (2019).
[53] batya friedman and helen nissenbaum. 1996. bias in computer systems. acm trans. inf. syst. 14, 3 (july 1996),
330–347. https://doi.org/10.1145/230538.230561
30
mehrabi et al.
[54] anna fry, thomas j littlejohns, cathie sudlow, nicola doherty, ligia adamska, tim sprosen, rory collins, and
naomi e allen. 2017. comparison of sociodemographic and health-related characteristics of uk biobank participants
with those of the general population. american journal of epidemiology 186, 9 (06 2017), 1026–1034.
https:
//doi.org/10.1093/aje/kwx246 arxiv:http://oup.prod.sis.lan/aje/article-pdf/186/9/1026/24330720/kwx246.pdf
[55] timnit gebru, jamie morgenstern, briana vecchione, jennifer wortman vaughan, hanna wallach, hal daumé iii, and
kate crawford. [n.d.]. datasheets for datasets. ([n. d.]).
[56] c. e. gehlke and katherine biehl. 1934. certain effects of grouping upon the size of the correlation coefficient in
census tract material. j. amer. statist. assoc. 29, 185a (1934), 169–170. https://doi.org/10.2307/2277827
[57] naman goel, mohammad yaghini, and boi faltings. 2018. non-discriminatory machine learning through convex
fairness criteria. in thirty-second aaai conference on artificial intelligence.
[58] hila gonen and yoav goldberg. 2019. lipstick on a pig: debiasing methods cover up systematic gender biases in
word embeddings but do not remove them. arxiv preprint arxiv:1903.03862 (2019).
[59] sandra gonzález-bailón, ning wang, alejandro rivero, javier borge-holthoefer, and yamir moreno. 2014. assessing
the bias in samples of large online networks. social networks 38 (2014), 16–27.
[60] susan t gooden. 2015. race and social equity: a nervous area of government. routledge.
[61] nina grgic-hlaca, muhammad bilal zafar, krishna p gummadi, and adrian weller. 2016. the case for process
fairness in learning: feature selection for fair decision making. in nips symposium on machine learning and the law,
vol. 1. 2.
[62] s. hajian and j. domingo-ferrer. 2013.
a methodology for direct and indirect discrimination prevention in
data mining. ieee transactions on knowledge and data engineering 25, 7 (july 2013), 1445–1459.
https:
//doi.org/10.1109/tkde.2012.72
[63] moritz hardt, eric price, nati srebro, et al. 2016. equality of opportunity in supervised learning. in advances in neural
information processing systems. 3315–3323.
[64] eszter hargittai. 2007. whose space? differences among users and non-users of social network sites. journal of
computer-mediated communication 13, 1 (10 2007), 276–297. https://doi.org/10.1111/j.1083-6101.2007.00396.x
arxiv:http://oup.prod.sis.lan/jcmc/article-pdf/13/1/276/22317170/jjcmcom0276.pdf
[65] yuzi he, keith burghardt, and kristina lerman. 2020. a geometric solution to fair representations. in proceedings
of the aaai/acm conference on ai, ethics, and society. 279–285.
[66] sarah holland, ahmed hosny, sarah newman, joshua joseph, and kasia chmielinski. 2018. the dataset nutrition
label: a framework to drive higher data quality standards. arxiv preprint arxiv:1805.03677 (2018).
[67] ayanna howard and jason borenstein. 2018. the ugly truth about ourselves and our robot creations: the problem of
bias and social inequity. science and engineering ethics 24, 5 (2018), 1521–1536.
[68] gary b huang, marwan mattar, tamara berg, and eric learned-miller. 2008. labeled faces in the wild: a database
forstudying face recognition in unconstrained environments.
[69] lingxiao huang and nisheeth vishnoi. 2019. stable and fair classification. in international conference on machine
learning. 2879–2890.
[70] ben hutchinson and margaret mitchell. 2019. 50 years of test (un) fairness: lessons for machine learning. in
proceedings of the conference on fairness, accountability, and transparency. acm, 49–58.
[71] l. introna and h. nissenbaum. 2000. defining the web: the politics of search engines. computer 33, 1 (jan 2000),
54–62. https://doi.org/10.1109/2.816269
[72] ayush jaiswal, yue wu, wael abdalmageed, and premkumar natarajan. 2018. unsupervised adversarial invariance.
arxiv:1809.10083 [cs.lg]
[73] ray jiang, aldo pacchiano, tom stepleton, heinrich jiang, and silvia chiappa. [n.d.]. wasserstein fair classification.
([n. d.]).
[74] f. kamiran and t. calders. 2009. classifying without discriminating. in 2009 2nd international conference on
computer, control and communication. 1–6. https://doi.org/10.1109/ic4.2009.4909197
[75] faisal kamiran and toon calders. 2010. classification with no discrimination by preferential sampling. in proc. 19th
machine learning conf. belgium and the netherlands. citeseer, 1–6.
[76] faisal kamiran and toon calders. 2012. data preprocessing techniques for classification without discrimination.
knowledge and information systems 33, 1 (01 oct 2012), 1–33. https://doi.org/10.1007/s10115-011-0463-8
[77] faisal kamiran and indr˙e žliobait˙e. 2013. explainable and non-explainable discrimination in classification. springer
berlin heidelberg, berlin, heidelberg, 155–170. https://doi.org/10.1007/978-3-642-30487-3_8
[78] toshihiro kamishima, shotaro akaho, hideki asoh, and jun sakuma. 2012. fairness-aware classifier with prejudice
remover regularizer. in joint european conference on machine learning and knowledge discovery in databases.
springer, 35–50.
[79] michael kearns, seth neel, aaron roth, and zhiwei steven wu. 2018. preventing fairness gerrymandering: auditing
and learning for subgroup fairness. in international conference on machine learning. 2569–2577.
a survey on bias and fairness in machine learning
31
[80] michael kearns, seth neel, aaron roth, and zhiwei steven wu. 2019. an empirical study of rich subgroup fairness for
machine learning. in proceedings of the conference on fairness, accountability, and transparency. acm, 100–109.
[81] rogier kievit, willem eduard frankenhuis, lourens waldorp, and denny borsboom. 2013. simpson’s paradox in
psychological science: a practical guide. frontiers in psychology 4 (2013), 513.
[82] niki kilbertus, mateo rojas carulla, giambattista parascandolo, moritz hardt, dominik janzing, and bernhard
schölkopf. 2017. avoiding discrimination through causal reasoning. in advances in neural information processing
systems. 656–666.
[83] jon kleinberg, sendhil mullainathan, and manish raghavan. 2016. inherent trade-offs in the fair determination of risk
scores. arxiv preprint arxiv:1609.05807 (2016).
[84] philipp koehn. 2005. europarl: a parallel corpus for statistical machine translation. in mt summit, vol. 5. 79–86.
[85] emmanouil krasanakis, eleftherios spyromitros-xioufis, symeon papadopoulos, and yiannis kompatsiaris. 2018.
adaptive sensitive reweighting to mitigate bias in fairness-aware classification. in proceedings of the 2018 world
wide web conference (lyon, france) (www ’18). international world wide web conferences steering committee,
republic and canton of geneva, switzerland, 853–862. https://doi.org/10.1145/3178876.3186133
[86] ivan krasin, tom duerig, neil alldrin, vittorio ferrari, sami abu-el-haija, alina kuznetsova, hassan rom, jasper
uijlings, stefan popov, andreas veit, et al. 2017. openimages: a public dataset for large-scale multi-label and
multi-class image classification. dataset available from https://github. com/openimages 2, 3 (2017), 2–3.
[87] matt j kusner, joshua loftus, chris russell, and ricardo silva. 2017. counterfactual fairness. in advances in neural
information processing systems 30, i. guyon, u. v. luxburg, s. bengio, h. wallach, r. fergus, s. vishwanathan, and
r. garnett (eds.). curran associates, inc., 4066–4076. http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf
[88] anja lambrecht and catherine e tucker. 2018. algorithmic bias? an empirical study into apparent gender-based
discrimination in the display of stem career ads. an empirical study into apparent gender-based discrimination in
the display of stem career ads (march 9, 2018) (2018).
[89] j larson, s mattu, l kirchner, and j angwin. 2016.
compas analysis.
github, available at: https://github.
com/propublica/compas-analysis[google scholar] (2016).
[90] blake lemoine, brian zhang, and m mitchell. 2018. mitigating unwanted biases with adversarial learning. (2018).
[91] kristina lerman. 2018. computational social scientist beware: simpson’s paradox in behavioral data. journal of
computational social science 1, 1 (2018), 49–58.
[92] kristina lerman and tad hogg. 2014. leveraging position bias to improve peer recommendation. plos one 9, 6
(2014), e98914. http://www.plosone.org/article/info%3adoi%2f10.1371%2fjournal.pone.0098914
[93] kristina lerman and tad hogg. 2014. leveraging position bias to improve peer recommendation. plos one 9, 6 (2014),
e98914.
[94] zachary c lipton, alexandra chouldechova, and julian mcauley. 2017. does mitigating ml’s disparate impact require
disparate treatment? stat 1050 (2017), 19.
[95] lydia t liu, sarah dean, esther rolf, max simchowitz, and moritz hardt. 2018. delayed impact of fair machine
learning. in proceedings of the 35th international conference on machine learning.
[96] joshua r loftus, chris russell, matt j kusner, and ricardo silva. 2018. causal reasoning for algorithmic fairness.
arxiv preprint arxiv:1805.05859 (2018).
[97] christos louizos, kevin swersky, yujia li, max welling, and richard zemel. 2016. the variational fair
autoencoder. stat 1050 (2016), 4.
[98] arjun k. manrai, birgit h. funke, heidi l. rehm, morten s. olesen, bradley a. maron, peter szolovits, david m.
margulies, joseph loscalzo, and isaac s. kohane. 2016. genetic misdiagnoses and the potential for health dis-
parities. new england journal of medicine 375, 7 (2016), 655–665.
https://doi.org/10.1056/nejmsa1507092
arxiv:https://doi.org/10.1056/nejmsa1507092 pmid: 27532831.
[99] ray marshall. 1974. the economics of racial discrimination: a survey. journal of economic literature 12, 3 (1974),
849–871.
[100] chandler may, alex wang, shikha bordia, samuel r bowman, and rachel rudinger. 2019. on measuring social
biases in sentence encoders. arxiv preprint arxiv:1903.10561 (2019).
[101] ninareh mehrabi, thamme gowda, fred morstatter, nanyun peng, and aram galstyan. 2019. man is to person as
woman is to location: measuring gender bias in named entity recognition. arxiv preprint arxiv:1910.10872 (2019).
[102] ninareh mehrabi, umang gupta, fred morstatter, greg ver steeg, and aram galstyan. 2021. attributing fair decisions
with attention interventions. arxiv preprint arxiv:2109.03952 (2021).
[103] ninareh mehrabi, yuzhong huang, and fred morstatter. 2020. statistical equity: a fairness classification objective.
arxiv preprint arxiv:2005.07293 (2020).
[104] ninareh mehrabi, fred morstatter, nanyun peng, and aram galstyan. 2019. debiasing community detection: the
importance of lowly-connected nodes. arxiv preprint arxiv:1903.08136 (2019).
32
mehrabi et al.
[105] ninareh mehrabi, pei zhou, fred morstatter, jay pujara, xiang ren, and aram galstyan. 2021. lawyers are dishonest?
quantifying representational harms in commonsense knowledge resources. in proceedings of the 2021 conference
on empirical methods in natural language processing. association for computational linguistics, online and punta
cana, dominican republic, 5016–5033. https://doi.org/10.18653/v1/2021.emnlp-main.410
[106] aditya krishna menon and robert c williamson. 2018. the cost of fairness in binary classification. in proceedings
of the 1st conference on fairness, accountability and transparency (proceedings of machine learning research,
vol. 81), sorelle a. friedler and christo wilson (eds.). pmlr, new york, ny, usa, 107–118. http://proceedings.mlr.
press/v81/menon18a.html
[107] michele merler, nalini ratha, rogerio s feris, and john r smith. 2019.
diversity in faces.
arxiv preprint
arxiv:1901.10436 (2019).
[108] hannah jean miller, jacob thebault-spieker, shuo chang, isaac johnson, loren terveen, and brent hecht. 2016.
“blissfully happy” or “ready tofight”: varying interpretations of emoji. in tenth international aaai conference on
web and social media.
[109] i minchev, g matijevic, dw hogg, g guiglion, m steinmetz, f anders, c chiappini, m martig, a queiroz, and c
scannapieco. 2019. yule-simpson’s paradox in galactic archaeology. arxiv preprint arxiv:1902.01421 (2019).
[110] margaret mitchell, simone wu, andrew zaldivar, parker barnes, lucy vasserman, ben hutchinson, elena spitzer,
inioluwa deborah raji, and timnit gebru. 2019. model cards for model reporting. in proceedings of the conference
on fairness, accountability, and transparency (atlanta, ga, usa) (fat* ’19). acm, new york, ny, usa, 220–229.
https://doi.org/10.1145/3287560.3287596
[111] fred morstatter, jürgen pfeffer, huan liu, and kathleen m carley. 2013. is the sample good enough? comparing data
from twitter’s streaming api with twitter’s firehose. in 7th international aaai conference on weblogs and social
media, icwsm 2013. aaai press.
[112] daniel moyer, shuyang gao, rob brekelmans, aram galstyan, and greg ver steeg. 2018. invariant representations
without adversarial training. in advances in neural information processing systems. 9084–9093.
[113] amitabha mukerjee, rita biswas, kalyanmoy deb, and amrit p mathur. 2002. multi–objective evolutionary algorithms
for the risk–return trade–off in bank loan management. international transactions in operational research 9, 5 (2002),
583–597.
[114] david b mustard. 2003. reexamining criminal behavior: the importance of omitted variable bias. review of economics
and statistics 85, 1 (2003), 205–211.
[115] razieh nabi, daniel malinsky, and ilya shpitser. 2018.
learning optimal fair policies.
arxiv preprint
arxiv:1809.02244 (2018).
[116] razieh nabi and ilya shpitser. 2018. fair inference on outcomes. in thirty-second aaai conference on artificial
intelligence.
[117] azadeh nematzadeh, giovanni luca ciampaglia, filippo menczer, and alessandro flammini. 2017. how algorithmic
popularity bias hinders or promotes quality. arxiv preprint arxiv:1707.00574 (2017).
[118] dong-phuong nguyen, rilana gravel, rudolf berend trieschnigg, and theo meder. 2013. "how old do you think
i am?": a study of language and age in twitter. in proceedings of the seventh international aaai conference on
weblogs and social media, icwsm 2013. aaai press, 439–448. eemcs-eprint-23604.
[119] anne o’keeffe and michael mccarthy. 2010. the routledge handbook of corpus linguistics. routledge.
[120] alexandra olteanu, carlos castillo, fernando diaz, and emre kiciman. 2016. social data: biases, methodological
pitfalls, and ethical boundaries. (2016).
[121] cathy o’neil. 2016. weapons of math destruction: how big data increases inequality and threatens democracy.
crown publishing group, new york, ny, usa.
[122] luca oneto, michele doninini, amon elders, and massimiliano pontil. 2019. taking advantage of multitask learning
for fair classification. in proceedings of the 2019 aaai/acm conference on ai, ethics, and society. 227–237.
[123] osonde a osoba and william welser iv. 2017. an intelligence in our image: the risks of bias and errors in artificial
intelligence. rand corporation.
[124] edmund s phelps. 1972. the statistical theory of racism and sexism. the american economic review 62, 4 (1972),
659–661.
[125] geoff pleiss, manish raghavan, felix wu, jon kleinberg, and kilian q weinberger. 2017.
on fairness and
calibration.
in advances in neural information processing systems 30, i. guyon, u. v. luxburg, s. bengio,
h. wallach, r. fergus, s. vishwanathan, and r. garnett (eds.). curran associates, inc., 5680–5689.
http:
//papers.nips.cc/paper/7151-on-fairness-and-calibration.pdf
[126] marcelo or prates, pedro h avelar, and luís c lamb. 2018. assessing gender bias in machine translation: a case
study with google translate. neural computing and applications (2018), 1–19.
[127] bilal qureshi, faisal kamiran, asim karim, and salvatore ruggieri. 2016. causal discrimination discovery through
propensity score analysis. arxiv preprint arxiv:1608.03735 (2016).
a survey on bias and fairness in machine learning
33
[128] inioluwa deborah raji and joy buolamwini. 2019. actionable auditing: investigating the impact of publicly naming
biased performance results of commercial ai products.
[129] m redmond. 2011. communities and crime unnormalized data set. uci machine learning repository. in website:
http://www. ics. uci. edu/mlearn/mlrepository. html (2011).
[130] willy e rice. 1996. race, gender, redlining, and the discriminatory access to loans, credit, and insurance:
an historical and empirical analysis of consumers who sued lenders and insurers in federal and state courts,
1950-1995. san diego l. rev. 33 (1996), 583.
[131] stephanie k riegg. 2008. causal inference and omitted variable bias in financial aid research: assessing solutions.
the review of higher education 31, 3 (2008), 329–354.
[132] lauren a rivera. 2012. hiring as cultural matching: the case of elite professional service firms. american sociological
review 77, 6 (2012), 999–1022.
[133] andrea romei and salvatore ruggieri. 2011. a multidisciplinary survey on discrimination analysis.
[134] rachel rudinger, jason naradowsky, brian leonard, and benjamin van durme. 2018. gender bias in coreference
resolution. in proceedings of the 2018 conference of the north american chapter of the association for computational
linguistics: human language technologies, volume 2 (short papers). association for computational linguistics, new
orleans, louisiana, 8–14. https://doi.org/10.18653/v1/n18-2002
[135] olga russakovsky, jia deng, hao su, jonathan krause, sanjeev satheesh, sean ma, zhiheng huang, andrej karpathy,
aditya khosla, michael bernstein, et al. 2015. imagenet large scale visual recognition challenge. international journal
of computer vision 115, 3 (2015), 211–252.
[136] pedro saleiro, benedict kuester, abby stevens, ari anisfeld, loren hinkson, jesse london, and rayid ghani. 2018.
aequitas: a bias and fairness audit toolkit. arxiv preprint arxiv:1811.05577 (2018).
[137] samira samadi, uthaipon tantipongpipat, jamie morgenstern, mohit singh, and santosh vempala. 2018. the price
of fair pca: one extra dimension. in proceedings of the 32nd international conference on neural information
processing systems (montr&#233;al, canada) (nips’18). curran associates inc., usa, 10999–11010. http://dl.acm.
org/citation.cfm?id=3327546.3327755
[138] nripsuta ani saxena. 2019. perceptions of fairness. in proceedings of the 2019 aaai/acm conference on ai, ethics,
and society (honolulu, hi, usa) (aies ’19). acm, new york, ny, usa, 537–538. https://doi.org/10.1145/3306618.
3314314
[139] nripsuta ani saxena, karen huang, evan defilippis, goran radanovic, david c parkes, and yang liu. 2019. how do
fairness definitions fare?: examining public attitudes towards algorithmic definitions of fairness. in proceedings of
the 2019 aaai/acm conference on ai, ethics, and society. acm, 99–106.
[140] tobias schnabel, adith swaminathan, ashudeep singh, navin chandak, and thorsten joachims. 2016. recom-
mendations as treatments: debiasing learning and evaluation. in international conference on machine learning.
1670–1679.
[141] andrew d selbst, danah boyd, sorelle a friedler, suresh venkatasubramanian, and janet vertesi. 2019. fairness and
abstraction in sociotechnical systems. in proceedings of the conference on fairness, accountability, and transparency.
acm, 59–68.
[142] shreya shankar, yoni halpern, eric breck, james atwood, jimbo wilson, and d sculley. 2017. no classification
without representation: assessing geodiversity issues in open data sets for the developing world. stat 1050 (2017),
22.
[143] richard shaw and manuel corpas. [n.d.]. further bias in personal genomics? ([n. d.]).
[144] harini suresh and john v guttag. 2019. a framework for understanding unintended consequences of machine
learning. arxiv preprint arxiv:1901.10002 (2019).
[145] songül tolan, marius miron, emilia gómez, and carlos castillo. 2019. why machine learning may lead to
unfairness: evidence from risk assessment for juvenile justice in catalonia. (2019).
[146] zeynep tufekci. 2014. big questions for social media big data: representativeness, validity and other methodological
pitfalls. in eighth international aaai conference on weblogs and social media.
[147] berk ustun, yang liu, and david parkes. 2019. fairness without harm: decoupled classifiers with preference
guarantees. in proceedings of the 36th international conference on machine learning (proceedings of machine
learning research, vol. 97), kamalika chaudhuri and ruslan salakhutdinov (eds.). pmlr, long beach, california,
usa, 6373–6382. http://proceedings.mlr.press/v97/ustun19a.html
[148] eva vanmassenhove, christian hardmeier, and andy way. 2018. getting gender right in neural machine translation. in
proceedings of the 2018 conference on empirical methods in natural language processing. 3003–3008.
[149] sahil verma and julia rubin. 2018. fairness definitions explained. in 2018 ieee/acm international workshop on
software fairness (fairware). ieee, 1–7.
[150] selwyn vickers, mona fouad, and moon s chen jr. 2014. enhancing minority participation in clinical trials
(empact): laying the groundwork for improving minority clinical trial accrual. cancer 120 (2014), vi–vii.
34
mehrabi et al.
[151] ting wang and dashun wang. 2014. why amazon’s ratings might mislead you: the story of herding effects. big data
2, 4 (2014), 196–204.
[152] steven l willborn. 1984. the disparate impact model of discrimination: theory and limits. am. ul rev. 34 (1984),
799.
[153] christo wilson, bryce boe, alessandra sala, krishna pn puttaswamy, and ben y zhao. 2009. user interactions in
social networks and their implications. in proceedings of the 4th acm european conference on computer systems.
acm, 205–218.
[154] blake woodworth, suriya gunasekar, mesrob i ohannessian, and nathan srebro. 2017. learning non-discriminatory
predictors. arxiv preprint arxiv:1702.06081 (2017).
[155] yongkai wu, lu zhang, and xintao wu. 2018. fairness-aware classification: criterion, convexity, and bounds.
arxiv:1809.04737 [cs.lg]
[156] depeng xu, shuhan yuan, lu zhang, and xintao wu. 2018. fairgan: fairness-aware generative adversarial networks.
in 2018 ieee international conference on big data (big data). ieee, 570–575.
[157] irene y chen, peter szolovits, and marzyeh ghassemi. 2019. can ai help reduce disparities in general medical and
mental health care? ama journal of ethics 21 (02 2019), e167–179. https://doi.org/10.1001/amajethics.2019.167
[158] muhammad bilal zafar, isabel valera, manuel gomez rodriguez, and krishna p gummadi. 2017. fairness beyond
disparate treatment & disparate impact: learning classification without disparate mistreatment. in proceedings of the
26th international conference on world wide web. 1171–1180.
[159] muhammad bilal zafar, isabel valera, manuel gomez rodriguez, and krishna p gummadi. 2015. fairness constraints:
mechanisms for fair classification. arxiv preprint arxiv:1507.05259 (2015).
[160] lu zhang and xintao wu. 2017. anti-discrimination learning: a causal modeling-based framework. international
journal of data science and analytics 4, 1 (01 aug 2017), 1–16. https://doi.org/10.1007/s41060-017-0058-x
[161] lu zhang, yongkai wu, and xintao wu. 2016. on discrimination discovery using causal networks. in social,
cultural, and behavioral modeling, kevin s. xu, david reitter, dongwon lee, and nathaniel osgood (eds.). springer
international publishing, cham, 83–93.
[162] lu zhang, yongkai wu, and xintao wu. 2016. situation testing-based discrimination discovery: a causal inference
approach. in proceedings of the twenty-fifth international joint conference on artificial intelligence (new york, new
york, usa) (ijcai’16). aaai press, 2718–2724. http://dl.acm.org/citation.cfm?id=3060832.3061001
[163] lu zhang, yongkai wu, and xintao wu. 2017. achieving non-discrimination in data release. in proceedings of the
23rd acm sigkdd international conference on knowledge discovery and data mining. acm, 1335–1344.
[164] lu zhang, yongkai wu, and xintao wu. 2017. a causal framework for discovering and removing direct and indirect
discrimination. in proceedings of the twenty-sixth international joint conference on artificial intelligence, ijcai-17.
3929–3935. https://doi.org/10.24963/ijcai.2017/549
[165] l. zhang, y. wu, and x. wu. 2018. causal modeling-based discrimination discovery and removal: criteria, bounds,
and algorithms. ieee transactions on knowledge and data engineering (2018), 1–1. https://doi.org/10.1109/tkde.
2018.2872988
[166] jieyu zhao, tianlu wang, mark yatskar, ryan cotterell, vicente ordonez, and kai-wei chang. 2019. gender bias
in contextualized word embeddings. in proceedings of the 2019 conference of the north american chapter of
the association for computational linguistics: human language technologies, volume 1 (long and short papers).
629–634.
[167] jieyu zhao, tianlu wang, mark yatskar, vicente ordonez, and kai-wei chang. 2017. men also like shopping:
reducing gender bias amplification using corpus-level constraints. in proceedings of the 2017 conference on
empirical methods in natural language processing.
[168] jieyu zhao, tianlu wang, mark yatskar, vicente ordonez, and kai-wei chang. 2018. gender bias in coreference
resolution: evaluation and debiasing methods. arxiv:1804.06876 [cs.cl]
[169] jieyu zhao, yichao zhou, zeyu li, wei wang, and kai-wei chang. 2018. learning gender-neutral word embeddings.
in proceedings of the 2018 conference on empirical methods in natural language processing. 4847–4853.
[170] james zou and londa schiebinger. 2018. ai can be sexist and racist it’s time to make it fair. nature publishing group.

conclusion: recommendations for researchers
in this work, we have laid the groundwork for a process to rigorously deﬁne and evaluate inter-
pretability. there are many open questions in creating the formal links between applications, the
science of human understanding, and more traditional machine learning regularizers. in the mean
time, we encourage the community to consider some general principles.
the claim of the research should match the type of the evaluation. just as one would be critical
of a reliability-oriented paper that only cites accuracy statistics, the choice of evaluation should
match the speciﬁcity of the claim being made.
a contribution that is focused on a particular
application should be expected to be evaluated in the context of that application (application-
grounded evaluation), or on a human experiment with a closely-related task (human-grounded
evaluation). a contribution that is focused on better optimizing a model class for some deﬁnition
of interpretability should be expected to be evaluated with functionally-grounded metrics. as a
community, we must be careful in the work on interpretability, both recognizing the need for and
the costs of human-subject experiments.
we should categorize our applications and methods with a common taxonomy. in section 4,
we hypothesized factors that may be the latent dimensions of interpretability. creating a shared
language around such factors is essential not only to evaluation, but also for the citation and
comparison of related work.
for example, work on creating a safe healthcare agent might be
framed as focused on the need for explanation due to unknown inputs at the local scale, evaluated
at the level of an application. in contrast, work on learning sparse linear models might also be
framed as focused on the need for explanation due to unknown inputs, but this time evaluated at
global scale. as we share each of our work with the community, we can do each other a service by
describing factors such as
1. how is the problem formulation incomplete? (section 2)
2. at what level is the evaluation being performed? (application, general user study, proxy;
section 3)
3. what are task-related relevant factors? (e.g. global vs. local, severity of incompleteness, level
of user expertise, time constraints; section 4.2)
4. what are method-related relevant factors being explored? (e.g. form of cognitive chunks,
number of cognitive chunks, compositionality, monotonicity, uncertainty; section 4.3)
and of course, adding and reﬁning these factors as our taxonomies evolve. these considerations
should move us away from vague claims about the interpretability of a particular model and toward
classifying applications by a common set of terms.
9
acknowledgments
this piece would not have been possible without the dozens of deep conver-
sations about interpretability with machine learning researchers and domain experts. our friends
and colleagues, we appreciate your support. we want to particularity thank ian goodfellow, kush
varshney, hanna wallach, solon barocas, stefan rping and jesse johnson for their feedback.
references
dario amodei, chris olah, jacob steinhardt, paul christiano, john schulman, and dan man´e.
concrete problems in ai safety. arxiv preprint arxiv:1606.06565, 2016.
pedro antunes, valeria herskovic, sergio f ochoa, and jose a pino. structuring dimensions for
collaborative systems evaluation. acm computing surveys, 2012.
william bechtel and adele abrahamsen. explanation: a mechanist alternative. studies in history
and philosophy of science part c: studies in history and philosophy of biological and biomedical
sciences, 2005.
catherine blake and christopher j merz. {uci} repository of machine learning databases. 1998.
nick bostrom and eliezer yudkowsky. the ethics of artiﬁcial intelligence. the cambridge handbook
of artiﬁcial intelligence, 2014.
greg brockman, vicki cheung, ludwig pettersson, jonas schneider, john schulman, jie tang,
and wojciech zaremba. openai gym. arxiv preprint arxiv:1606.01540, 2016.
cristian bucilu, rich caruana, and alexandru niculescu-mizil. model compression. in proceedings
of the 12th acm sigkdd international conference on knowledge discovery and data mining.
acm, 2006.
samuel carton, jennifer helsby, kenneth joseph, ayesha mahmud, youngsoo park, joe walsh,
crystal cody, cpt estella patterson, lauren haynes, and rayid ghani.
identifying police
oﬃcers at risk of adverse events.
in acm sigkdd international conference on knowledge
discovery and data mining. acm, 2016.
jonathan chang, jordan l boyd-graber, sean gerrish, chong wang, and david m blei. reading
tea leaves: how humans interpret topic models. in nips, 2009.
nick chater and mike oaksford. speculations on human causal learning and reasoning. information
sampling and adaptive cognition, 2006.
finale doshi-velez, yaorong ge, and isaac kohane.
comorbidity clusters in autism spectrum
disorders: an electronic health record time-series analysis. pediatrics, 133(1):e54–e63, 2014.
finale doshi-velez, byron wallace, and ryan adams.
graph-sparse lda: a topic model with
structured sparsity. association for the advancement of artiﬁcial intelligence, 2015.
cynthia dwork, moritz hardt, toniann pitassi, omer reingold, and richard zemel.
fairness
through awareness. in innovations in theoretical computer science conference. acm, 2012.
10
alex freitas. comprehensible classiﬁcation models: a position paper. acm sigkdd explorations,
2014.
vikas k garg and adam tauman kalai. meta-unsupervised-learning: a supervised approach to
unsupervised learning. arxiv preprint arxiv:1612.09030, 2016.
stuart glennan. rethinking mechanistic explanation. philosophy of science, 2002.
bryce goodman and seth flaxman. european union regulations on algorithmic decision-making
and a” right to explanation”. arxiv preprint arxiv:1606.08813, 2016.
maya gupta, andrew cotter, jan pfeifer, konstantin voevodski, kevin canini, alexander
mangylov, wojciech moczydlowski, and alexander van esbroeck.
monotonic calibrated in-
terpolated look-up tables. journal of machine learning research, 2016.
sean hamill.
cmu computer won poker battle over humans by statistically signiﬁcant mar-
gin.
http://www.post-gazette.com/business/tech-news/2017/01/31/cmu-computer-
won-poker-battle-over-humans-by-statistically-significant-margin/stories/
201701310250, 2017. accessed: 2017-02-07.
moritz hardt and kunal talwar. on the geometry of diﬀerential privacy. in acm symposium on
theory of computing. acm, 2010.
moritz hardt, eric price, and nati srebro. equality of opportunity in supervised learning. in
advances in neural information processing systems, 2016.
carl hempel and paul oppenheim. studies in the logic of explanation. philosophy of science, 1948.
tin kam ho and mitra basu. complexity measures of supervised classiﬁcation problems. ieee
transactions on pattern analysis and machine intelligence, 2002.
frank keil. explanation and understanding. annu. rev. psychol., 2006.
frank keil, leonid rozenblit, and candice mills. what lies beneath? understanding the limits of
understanding. thinking and seeing: visual metacognition in adults and children, 2004.
been kim, caleb chacha, and julie shah. inferring robot task plans from human team meetings:
a generative modeling approach with logic-based prior.
association for the advancement of
artiﬁcial intelligence, 2013.
been kim, elena glassman, brittney johnson, and julie shah. ibcm: interactive bayesian case
model empowering humans via intuitive interaction. 2015a.
been kim, julie shah, and finale doshi-velez. mind the gap: a generative approach to inter-
pretable feature selection and extraction. in advances in neural information processing systems,
2015b.
himabindu lakkaraju, stephen h bach, and jure leskovec. interpretable decision sets: a joint
framework for description and prediction. in proceedings of the 22nd acm sigkdd interna-
tional conference on knowledge discovery and data mining, pages 1675–1684. acm, 2016.
11
jonathan lazar, jinjuan heidi feng, and harry hochheiser. research methods in human-computer
interaction. john wiley & sons, 2010.
tao lei, regina barzilay, and tommi jaakkola. rationalizing neural predictions. arxiv preprint
arxiv:1606.04155, 2016.
tania lombrozo. the structure and function of explanations. trends in cognitive sciences, 10(10):
464–470, 2006.
yin lou, rich caruana, and johannes gehrke. intelligible models for classiﬁcation and regression.
in acm sigkdd international conference on knowledge discovery and data mining. acm, 2012.
volodymyr mnih, koray kavukcuoglu, david silver, alex graves, ioannis antonoglou, daan wier-
stra, and martin riedmiller. playing atari with deep reinforcement learning. arxiv preprint
arxiv:1312.5602, 2013.
ian neath and aimee surprenant. human memory. 2003.
clemens otte. safe and interpretable machine learning: a methodological review. in computational
intelligence in intelligent data analysis. springer, 2013.
parliament and council of the european union. general data protection regulation. 2016.
marco tulio ribeiro, sameer singh, and carlos guestrin. “why should i trust you?”: explaining
the predictions of any classiﬁer. arxiv preprint arxiv:1602.04938, 2016.
salvatore ruggieri, dino pedreschi, and franco turini. data mining for discrimination discovery.
acm transactions on knowledge discovery from data, 2010.
eric schulz, joshua tenenbaum, david duvenaud, maarten speekenbrink, and samuel gershman.
compositional inductive biases in function learning. biorxiv, 2016.
d sculley, gary holt, daniel golovin, eugene davydov, todd phillips, dietmar ebner, vinay
chaudhary, michael young, jean-fran¸cois crespo, and dan dennison. hidden technical debt in
machine learning systems. in advances in neural information processing systems, 2015.
david silver, aja huang, chris j maddison, arthur guez, laurent sifre, george van den driessche,
julian schrittwieser, ioannis antonoglou, veda panneershelvam, marc lanctot, et al. mastering
the game of go with deep neural networks and tree search. nature, 2016.
lior jacob strahilevitz.
privacy versus antidiscrimination.
university of chicago law school
working paper, 2008.
adi suissa-peleg, daniel haehn, seymour knowles-barley, verena kaynig, thouis r jones, alyssa
wilson, richard schalek, jeﬀery w lichtman, and hanspeter pﬁster. automatic neural recon-
struction from petavoxel of electron microscopy data. microscopy and microanalysis, 2016.
vincent toubiana, arvind narayanan, dan boneh, helen nissenbaum, and solon barocas. adnos-
tic: privacy preserving targeted advertising. 2010.
joaquin vanschoren, jan n van rijn, bernd bischl, and luis torgo. openml: networked science
in machine learning. acm sigkdd explorations newsletter, 15(2):49–60, 2014.
12
kush varshney and homa alemzadeh. on the safety of machine learning: cyber-physical systems,
decision sciences, and data products. corr, 2016.
fulton wang and cynthia rudin. falling rule lists. in aistats, 2015.
tong wang, cynthia rudin, finale doshi-velez, yimin liu, erica klampﬂ, and perry macneille.
bayesian rule sets for interpretable classiﬁcation. in international conference on data mining,
2017.
joseph jay williams, juho kim, anna raﬀerty, samuel maldonado, krzysztof z gajos, walter s
lasecki, and neil heﬀernan. axis: generating explanations at scale with learnersourcing and
machine learning. in acm conference on learning@ scale. acm, 2016.
andrew wilson, christoph dann, chris lucas, and eric xing. the human kernel. in advances in
neural information processing systems, 2015.
13