method of interaction or time. the differences and
biases in the networks can be a result of many factors, such as network sampling, as shown in
[59, 111], which can change the network measures and cause different types of problems.
3.1.2
algorithm to user. algorithms modulate user behavior. any biases in algorithms might
introduce biases in user behavior. in this section we talk about biases that are as a result of algorithmic
outcomes and affect user behavior as a consequence.
(1) algorithmic bias. algorithmic bias is when the bias is not present in the input data and is
added purely by the algorithm [9]. the algorithmic design choices, such as use of certain
optimization functions, regularizations, choices in applying regression models on the data as
a whole or considering subgroups, and the general use of statistically biased estimators in
algorithms [44], can all contribute to biased algorithmic decisions that can bias the outcome of
the algorithms.
(2) user interaction bias. user interaction bias is a type of bias that can not only be observant
on the web but also get triggered from two sourcesâ€”the user interface and through the user
itself by imposing his/her self-selected biased behavior and interaction [9]. this type of bias
can be influenced by other types and subtypes, such as presentation and ranking biases.
(a) presentation bias. presentation bias is a result of how information is presented [9]. for
example, on the web users can only click on content that they see, so the seen content gets
clicks, while everything else gets no click. and it could be the case that the user does not see
all the information on the web [9].
(b) ranking bias. the idea that top-ranked results are the most relevant and important will
result in attraction of more clicks than others. this bias affects search engines [9] and
crowdsourcing applications [93].
(3) popularity bias. items that are more popular tend to be exposed more. however, popularity
metrics are subject to manipulationâ€”for example, by fake reviews or social bots [117]. as an
instance, this type of bias can be seen in search engines [71, 117] or recommendation systems
where popular objects would be presented more to the public. but this presentation may not be
a result of good quality; instead, it may be due to other biased factors.
(4) emergent bias. emergent bias occurs as a result of use and interaction with real users. this
bias arises as a result of change in population, cultural values, or societal knowledge usually
some time after the completion of design [53]. this type of bias is more likely to be observed
in user interfaces, since interfaces tend to reflect the capacities, characteristics, and habits of
prospective users by design [53]. this type of bias can itself be divided into more subtypes, as
discussed in detail in [53].
8
mehrabi et al.
(5) evaluation bias. evaluation bias happens during model evaluation [144]. this includes the
use of inappropriate and disproportionate benchmarks for evaluation of applications such
as adience and ijb-a benchmarks. these benchmarks are used in the evaluation of facial
recognition systems that were biased toward skin color and gender [24], and can serve as
examples for this type of bias [144].
3.1.3
user to data. many data sources used for training ml models are user-generated. any
inherent biases in users might be reflected in the data they generate. furthermore, when user behavior
is affected/modulated by an algorithm, any biases present in those algorithm might introduce bias in
the data generation process. here we list several important types of such biases.
(1) historical bias. historical bias is the already existing bias and socio-technical issues in the
world and can seep into from the data generation process even given a perfect sampling and
feature selection [144]. an example of this type of bias can be found in a 2018 image search
result where searching for women ceos ultimately resulted in fewer female ceo images due
to the fact that only 5% of fortune 500 ceos were womanâ€”which would cause the search
results to be biased towards male ceos [144]. these search results were of course reflecting
the reality, but whether or not the search algorithms should reflect this reality is an issue worth
considering.
(2) population bias. population bias arises when statistics, demographics, representatives, and
user characteristics are different in the user population of the platform from the original target
population [120]. population bias creates non-representative data. an example of this type of
bias can arise from different user demographics on different social platforms, such as women
being more likely to use pinterest, facebook, instagram, while men being more active in online
forums like reddit or twitter. more such examples and statistics related to social media use
among young adults according to gender, race, ethnicity, and parental educational background
can be found in [64].
(3) self-selection bias. self-selection bias4 is a subtype of the selection or sampling bias in which
subjects of the research select themselves. an example of this type of bias can be observed in
an opinion poll to measure enthusiasm for a political candidate, where the most enthusiastic
supporters are more likely to complete the poll.
(4) social bias. social bias happens when othersâ€™ actions affect our judgment. [9]. an example
of this type of bias can be a case where we want to rate or review an item with a low score, but
when influenced by other high ratings, we change our scoring thinking that perhaps we are
being too harsh [9, 151].
(5) behavioral bias. behavioral bias arises from different user behavior across platforms, con-
texts, or different datasets [120]. an example of this type of bias can be observed in [108],
where authors show how differences in emoji representations among platforms can result in
different reactions and behavior from people and sometimes even leading to communication
errors.
(6) temporal bias. temporal bias arises from differences in populations and behaviors over time
[120]. an example can be observed in twitter where people talking about a particular topic
start using a hashtag at some point to capture attention, then continue the discussion about the
event without using the hashtag [120, 146].
(7) content production bias. content production bias arises from structural, lexical, semantic,
and syntactic differences in the contents generated by users [120]. an example of this type of
bias can be seen in [118] where the differences in use of language across different gender and
4https://data36.com/statistical-bias-types-explained/
a survey on bias and fairness in machine learning
9
age groups is discussed. the differences in use of language can also be seen across and within
countries and populations.
existing work tries to categorize these bias definitions into groups, such as definitions falling solely
under data or user interaction. however, due to the existence of the feedback loop phenomenon [36],
these definitions are intertwined, and we need a categorization which closely models this situation.
this feedback loop is not only existent between the data and the algorithm, but also between the
algorithms and user interaction [29]. inspired by these papers, we modeled categorization of bias
definitions, as shown in figure 1, and grouped these definitions on the arrows of the loop where we
thought they were most effective. we emphasize the fact again that these definitions are intertwined,
and one should consider how they affect each other in this cycle, and address them accordingly.
3.2
data bias examples
there are multiple ways that discriminatory bias can seep into data. for instance, using unbalanced
data can create biases against underrepresented groups. [170] analyzes some examples of the biases
that can exist in the data and algorithms and offer some recommendations and suggestions toward
mitigating these issues.
3.2.1
examples of bias in machine learning data. in [24], the authors show that datasets like
ijb-a and adience are imbalanced and contain mainly light-skinned subjectsâ€”79.6% in ijb-a and
86.2% in adience. this can bias the analysis towards dark-skinned groups who are underrepresented
in the data. in another instance, the way we use and analyze our data can create bias when we do not
consider different subgroups in the data. in [24], the authors also show that considering only male-
female groups is not enough, but there is also a need to use race to further subdivide the gender groups
into light-skinned females, light-skinned males, dark-skinned males, and dark-skinned females. itâ€™s
only in this case that we can clearly observe the bias towards dark-skinned females, as previously
dark-skinned males would compromise for dark-skinned females and would hide the underlying
bias towards this subgroup. popular machine-learning datasets that serve as a base for most of
the developed algorithms and tools can also be biasedâ€”which can be harmful to the downstream
applications that are based on these datasets. for instance, imagenet [135] and open images [86] are
two widely used datasets in machine-learning. in [142], researchers showed that these datasets suffer
from representation bias and advocate for the need to incorporate geographic diversity and inclusion
while creating such datasets. in addition, authors in [105] write about the existing representational
biases in different knowledge bases that are widely used in natural language processing (nlp)
applications for different commonsense reasoning tasks.
3.2.2
examples of data bias in medical applications. these data biases can be more dangerous
in other sensitive applications. for example, in medical domains there are many instances in which
the data studied and used are skewed toward certain populationsâ€”which can have dangerous conse-
quences for the underrepresented communities. [98] showed how exclusion of african-americans
resulted in their misclassification in clinical studies, so they became advocates for sequencing the
genomes of diverse populations in the data to prevent harm to underrepresented populations. authors
in [143] studied the 23andme genotype dataset and found that out of 2,399 individuals, who have
openly shared their genotypes in public repositories, 2,098 (87%) are european, while only 58
(2%) are asian and 50 (2%) african. other such studies were conducted in [54] which states that
uk biobank, a large and widely used genetic dataset, may not represent the sampling population.
researchers found evidence of a â€œhealthy volunteerâ€ selection bias. [150] has other examples of
studies on existing biases in the data used in the medical domain. [157] also looks at machine-learning
10
mehrabi et al.
algorithms and data utilized in medical fields, and writes about how artificial intelligence in health
care has not impacted all patients equally.
3.3
discrimination
similar to bias, discrimination is also a source of unfairness. discrimination can be considered as a
source for unfairness that is due to human prejudice and stereotyping based on the sensitive attributes,
which may happen intentionally or unintentionally, while bias can be considered as a source for
unfairness that is due to the data collection, sampling, and measurement. although bias can also be
seen as a source of unfairness that is due to human prejudice and stereotyping, in the algorithmic
fairness literature it is more intuitive to categorize them as such according to the existing research
in these areas. in this survey, we mainly focus on concepts that are relevant to algorithmic fairness
issues. [99, 133, 152] contain more broad information on discrimination theory that involve more
multidisciplinary concepts from legal theory, economics, and social sciences which can be referenced
by the interested readers.
3.3.1
explainable discrimination. differences in treatment and outcomes amongst different
groups can be justified and explained via some attributes in some cases. in situations where these
differences are justified and explained, it is not considered to be illegal discrimination and hence
called explainable [77]. for instance, authors in [77] state that in the uci adult dataset [7], a widely
used dataset in the fairness domain, males on average have a higher annual income than females.
however, this is because on average females work fewer hours than males per week. work hours
per week is an attribute that can be used to explain low income which needs to be considered. if we
make decisions, without considering working hours, such that males and females end up averaging
the same income, we will lead to reverse discrimination since we would cause male employees to get
lower salary than females. therefore, explainable discrimination is acceptable and legal as it can
be explained through other attributes like working hours. in [77], authors present a methodology
to quantify the explainable and illegal discrimination in data. they argue that methods that do not
take the explainable part of the discrimination into account may result in non-desirable outcomes, so
they introduce a reverse discrimination which is equally harmful and undesirable. they explain how
to quantify and measure discrimination in data or a classifierâ€™s decisions which directly considers
illegal and explainable discrimination.
3.3.2
unexplainable discrimination. in contrast to explainable discrimination, there is unexplain-
able discrimination in which the discrimination toward a group is unjustified and therefore considered
illegal. authors in [77] also present local techniques for removing only the illegal or unexplainable
discrimination, allowing only for explainable differences in decisions. these are preprocessing tech-
niques that change the training data such that it contains no unexplainable discrimination. we expect
classifiers trained on this preprocessed data to not capture illegal or unexplainable discrimination.
unexplainable discrimination consists of direct and indirect discrimination.
(1) direct discrimination. direct discrimination happens when protected attributes of individuals
explicitly result in non-favorable outcomes toward them [164]. typically, there are some traits
identified by law on which it is illegal to discriminate against, and it is usually these traits that
are considered to be â€œprotectedâ€ or â€œsensitiveâ€ attributes in computer science literature. a list
of some of these protected attributes is provided in table 3 as specified in the fair housing
and equal credit opportunity acts (fha and ecoa) [30].
(2) indirect discrimination. in indirect discrimination, individuals appear to be treated based
on seemingly neutral and non-protected attributes; however, protected groups, or individuals
still get to be treated unjustly as a result of implicit effects from their protected attributes (e.g.,
a survey on bias and fairness in machine learning
11
the residential zip code of a person can be used in decision making processes such as loan
applications. however, this can still lead to racial discrimination, such as redlining, as despite
the fact that zip code appears to be a non-sensitive attribute, it may correlate with race because
of the population of residential areas.) [130, 164].
3.3.3
sources of discrimination.
(1) systemic discrimination. systemic discrimination refers to policies, customs, or behaviors
that are a part of the culture or structure of an organization that may perpetuate discrimination
against certain subgroups of the population [40]. [132] found that employers overwhelmingly
preferred competent candidates that were culturally similar to them, and shared similar ex-
periences and hobbies. if the decision-makers happen to belong overwhelmingly to certain
subgroups, this may result in discrimination against competent candidates that do not belong
to these subgroups.
(2) statistical discrimination. statistical discrimination is a phenomenon where decision-makers
use average group statistics to judge an individual belonging to that group. it usually occurs
when the decision-makers (e.g., employers, or law enforcement officers) use an individualâ€™s
obvious, recognizable characteristics as a proxy for either hidden or more-difficult-to-determine
characteristics, that may actually be relevant to the outcome [124].
4
algorithmic fairness
fighting against bias and discrimination has a long history in philosophy and psychology, and
recently in machine-learning. however, in order to be able to fight against discrimination and achieve
fairness, one should first define fairness. philosophy and psychology have tried to define the concept
of fairness long before computer science. the fact that no universal definition of fairness exists shows
the difficulty of solving this problem [138]. different preferences and outlooks in different cultures
lend a preference to different ways of looking at fairness, which makes it harder to come up with just
a single definition that is acceptable to everyone in a situation. indeed, even in computer science,
where most of the work on proposing new fairness constraints for algorithms has come from the
west, and a lot of these papers use the same datasets and problems to show how their constraints
perform, there is still no clear agreement on which constraints are the most appropriate for those
problems. broadly, fairness is the absence of any prejudice or favoritism towards an individual or a
group based on their intrinsic or acquired traits in the context of decision-making [139]. even though
fairness is an incredibly desirable quality in society, it can be surprisingly difficult to achieve in
practice. with these challenges in mind, many fairness definitions are proposed to address different
algorithmic bias and discrimination issues discussed in the previous section.
4.1
definitions of fairness
in [17], authors studied fairness definitions in political philosophy and tried to tie them to machine-
learning. authors in [70] studied the 50-year history of fairness definitions in the areas of education
and machine-learning. in [149], authors listed and explained some of the definitions used for fairness
in algorithmic classification problems. in [139], authors studied the general publicâ€™s perception of
some of these fairness definitions in computer science literature. here we will reiterate and provide
some of the most widely used definitions, along with their explanations inspired from [149].
definition 1. (equalized odds). the definition of equalized odds, provided by [63], states that
â€œa predictor Ë†y satisfies equalized odds with respect to protected attribute a and outcome y, if Ë†y and a
are independent conditional on y. p( Ë†y=1|a=0,y =y) = p( Ë†y=1|a=1,y =y) , yâˆˆ{0,1}â€. this means
that the probability of a person in the positive class being correctly assigned a positive outcome
12
mehrabi et al.
and the probability of a person in a negative class being incorrectly assigned a positive outcome
should both be the same for the protected and unprotected group members [149]. in other words, the
equalized odds definition states that the protected and unprotected groups should have equal rates for
true positives and false positives.
definition 2. (equal opportunity). â€œa binary predictor Ë†y satisfies equal opportunity with respect
to a and y if p( Ë†y=1|a=0,y=1) = p( Ë†y=1|a=1,y=1)â€ [63]. this means that the probability of a
person in a positive class being assigned to a positive outcome should be equal for both protected
and unprotected (female and male) group members [149]. in other words, the equal opportunity
definition states that the protected and unprotected groups should have equal true positive rates.
definition 3. (demographic parity). also known as statistical parity. â€œa predictor Ë†y satisfies demo-
graphic parity if p( Ë†y |a = 0) = p( Ë†y|a = 1)â€ [48, 87]. the likelihood of a positive outcome [149]
should be the same regardless of whether the person is in the protected (e.g., female) group.
definition 4. (fairness through awareness). â€œan algorithm is fair if it gives similar predictions to
similar individualsâ€ [48, 87]. in other words, any two individuals who are similar with respect to a
similarity (inverse distance) metric defined for a particular task should receive a similar outcome.
definition 5. (fairness through unawareness). â€œan algorithm is fair as long as any protected
attributes a are not explicitly used in the decision-making processâ€ [61, 87].
definition 6. (treatment equality). â€œtreatment equality is achieved when the ratio of false negatives
and false positives is the same for both protected group categoriesâ€ [15].
definition 7. (test fairness). â€œa score s = s(x) is test fair (well-calibrated) if it reflects the same
likelihood of recidivism irrespective of the individualâ€™s group membership, r. that is, if for all values
of s, p(y =1|s=s,r=b)=p(y =1|s=s,r=w)â€ [34]. in other words, the test fairness definition states
that for any predicted probability score s, people in both protected and unprotected groups must have
equal probability of correctly belonging to the positive class [149].
definition 8. (counterfactual fairness). â€œpredictor Ë†y is counterfactually fair if under any con-
text x =x and a=a, p( Ë†ğ‘Œğ´â†âˆ’ğ‘(u)=y|x =x,a=a)=p( Ë†ğ‘Œğ´â†âˆ’ğ‘â€²(u)=y|x =x,a=a), (for all y and for any
value ğ‘â€² attainable by aâ€ [87]. the counterfactual fairness definition is based on the â€œintuition that a
decision is fair towards an individual if it is the same in both the actual world and a counterfactual
world where the individual belonged to a different demographic group.â€
definition 9. (fairness in relational domains). â€œa notion of fairness that is able to capture the
relational structure in a domainâ€”not only by taking attributes of individuals into consideration but
by taking into account the social, organizational, and other connections between individualsâ€ [50].
definition 10. (conditional statistical parity). for a set of legitimate factors l, predictor Ë†y satisfies
conditional statistical parity if p( Ë†y |l=1,a = 0) = p( Ë†y|l=1,a = 1) [41]. conditional statistical parity
states that people in both protected and unprotected (female and male) groups should have equal
probability of being assigned to a positive outcome given a set of legitimate factors l [149].
fairness definitions fall under different types as follows:
a survey on bias and fairness in machine learning
13
name
reference
group
subgroup
individual
demographic parity
[87][48]
âœ“
conditional statistical parity
[41]
âœ“
equalized odds
[63]
âœ“
equal opportunity
[63]
âœ“
treatment equality
[15]
âœ“
test fairness
[34]
âœ“
subgroup fairness
[79][80]
âœ“
fairness through unawareness
[87][61]
âœ“
fairness through awareness
[48]
âœ“
counterfactual fairness
[87]
âœ“
table 1. categorizing different fairness notions into group, subgroup, and individual types.
(1) individual fairness. give similar predictions to similar individuals [48, 87].
(2) group fairness. treat different groups equally [48, 87].
(3) subgroup fairness. subgroup fairness intends to obtain the best properties of the group and
individual notions of fairness. it is different than these notions but uses them in order to obtain
better outcomes. it picks a group fairness constraint like equalizing false positive and asks
whether this constraint holds over a large collection of subgroups [79, 80].
it is important to note that according to [83], it is impossible to satisfy some of the fairness con-
straints at once except in highly constrained special cases. in [83], the authors show the inherent
incompatibility of two conditions: calibration and balancing the positive and negative classes. these
cannot be satisfied simultaneously with each other unless under certain constraints; therefore, it
is important to take the context and application in which fairness definitions need to be used into
consideration and use them accordingly [141]. another important aspect to consider is time and
temporal analysis of the impacts that these definitions may have on individuals or groups. in [95]
authors show that current fairness definitions are not always helpful and do not promote improvement
for sensitive groupsâ€”and can actually be harmful when analyzed over time in some cases. they
also show that measurement errors can also act in favor of these fairness definitions; therefore, they
show how temporal modeling and measurement are important in evaluation of fairness criteria and
introduce a new range of trade-offs and challenges toward this direction. it is also important to pay
attention to the sources of bias and their types when trying to solve fairness-related questions.
5
methods for fair machine learning
there have been numerous attempts to address bias in artificial intelligence in order to achieve
fairness; these stem from domains of ai. in this section we will enumerate different domains of
ai, and the work that has been produced by each community to combat bias and unfairness in their
methods. table 2 provides an overview of the different areas that we focus upon in this survey.
while this section is largely domain-specific, it can be useful to take a cross-domain view. gener-
ally, methods that target biases in the algorithms fall under three categories:
(1) pre-processing. pre-processing techniques try to transform the data so that the underlying
discrimination is removed [43]. if the algorithm is allowed to modify the training data, then
pre-processing can be used [11].
(2) in-processing. in-processing techniques try to modify and change state-of-the-art learning
algorithms in order to remove discrimination during the model training process [43]. if it is
14
mehrabi et al.
allowed to change the learning procedure for a machine learning model, then in-processing
can be used during the training of a modelâ€” either by incorporating changes into the objective
function or imposing a constraint [11, 14].
(3) post-processing. post-processing is performed after training by accessing a holdout set which
was not involved during the training of the model [43]. if the algorithm can only treat the
learned model as a black box without any ability to modify the training data or learning
algorithm, then only post-processing can be used in which the labels assigned by the black-box
model initially get reassigned based on a function during the post-processing phase [11, 14].
examples of some existing work and their categorization into these types is shown in table 4. these
methods are not just limited to general machine learning techniques, but because of aiâ€™s popularity,
they have expanded to different domains such as natural language processing and deep learning. from
learning fair representations [42, 97, 112] to learning fair word embeddings [20, 58, 169], debiasing
methods have been proposed in different ai applications and domains. most of these methods try
to avoid unethical interference of sensitive or protected attributes into the decision-making process,
while others target exclusion bias by trying to include users from sensitive groups. in addition, some
works try to satisfy one or more of the fairness notions in their methods, such as disparate learning
processes (dlps) which try to satisfy notions of treatment disparity and impact disparity by allowing
the protected attributes during the training phase but avoiding them during prediction time [94]. a
list of protected or sensitive attributes is provided in table 3. they point out what attributes should
not affect the outcome of the decision in housing loan or credit card decision-making [30] according
to the law. some of the existing work tries to treat sensitive attributes as noise to disregard their
effect on decision-making, while some causal methods use causal graphs, and disregard some paths
in the causal graph that result in sensitive attributes affecting the outcome of the decision. different
bias-mitigating methods and techniques are discussed below for different domainsâ€”each targeting a
different problem in different areas of machine learning in detail. this can expand the horizon of
the reader on where and how bias can affect the system and try to help researchers carefully look
at various new problems concerning potential places where discrimination and bias can affect the
outcome of a system.
5.1
unbiasing data
every dataset is the result of several design decisions made by the data curator. those decisions have
consequences for the fairness of the resulting dataset, which in turn affects the resulting algorithms. in
order to mitigate the effects of bias in data, some general methods have been proposed that advocate
having good practices while using data, such as having datasheets that would act like a supporting
document for the data reporting the dataset creation method, its characteristics, motivations, and its
skews [13, 55]. [12] proposes a similar approach for the nlp applications. a similar suggestion has
been proposed for models in [110]. authors in [66] also propose having labels, just like nutrition
labels on food, in order to better categorize each data for each task. in addition to these general
techniques, some work has targeted more specific types of biases. for example, [81] has proposed
methods to test for cases of simpsonâ€™s paradox in the data, and [3, 4] proposed methods to discover
simpsonâ€™s paradoxes in data automatically. causal models and graphs were also used in some work
to detect direct discrimination in the data along with its prevention technique that modifies the data
such that the predictions would be absent from direct discrimination [163]. [62] also worked on
preventing discrimination in data mining, targeting direct, indirect, and simultaneous effects. other
pre-processing approaches, such as messaging [74], preferential sampling [75, 76], disparate impact
removal [51], also aim to remove biases from the data.
a survey on bias and fairness in machine learning
15
area
reference(s)
classification
[78] [106] [57] [85] [147] [63] [159] [154] [69]
[25] [155] [122] [49] [73] [75]
regression
[14] [1]
pca
[137]
community detection
[104]
clustering
[31] [8]
graph embedding
[22]
causal inference
[96] [164] [165] [160] [116] [115] [162] [82] [127]
[161]
variational auto encoders
[97] [5] [112] [42]
adversarial learning
[90] [156]
word embedding
[20] [169] [58] [23] [166]
coreference resolution
[168] [134]
language model
[21]
sentence embedding
[100]
machine translation
[52]
semantic role labeling
[167]
named entity recognition
[101]
table 2. list of papers targeting and talking about bias and fairness in different areas.
attribute
fha
ecoa
race
âœ“
âœ“
color
âœ“
âœ“
national origin
âœ“
âœ“
religion
âœ“
âœ“
sex
âœ“
âœ“
familial status
âœ“
disability
âœ“
exercised rights under ccpa
âœ“
marital status
âœ“
recipient of public assistance
âœ“
age
âœ“
table 3. a list of the protected attributes as specified in the fair housing and equal credit opportunity
acts (fha and ecoa), from [30].
5.2
fair machine learning
to address this issue, a variety of methods have been proposed that satisfy some of the fairness
definitions or other new definitions depending on the application.
5.2.1
fair classification. since classification is a canonical task in machine learning and is
widely used in different areas that can be in direct contact with humans, it is important that these
types of methods be fair and be absent from biases that can harm some populations. therefore,
certain methods have been proposed [57, 78, 85, 106] that satisfy certain definitions of fairness in
classification. for instance, in [147] authors try to satisfy subgroup fairness in classification, equality
of opportunity and equalized odds in [63], both disparate treatment and disparate impact in [2, 159],
16
mehrabi et al.
and equalized odds in [154]. other methods try to not only satisfy some fairness constraints but to
also be stable toward change in the test set [69]. the authors in [155], propose a general framework
for learning fair classifiers. this framework can be used for formulating fairness-aware classification
with fairness guarantees. in another work [25], authors propose three different modifications to the
existing naive bayes classifier for discrimination-free classification. [122] takes a new approach
into fair classification by imposing fairness constraints into a multitask learning (mtl) framework.
in addition to imposing fairness during training, this approach can benefit the minority groups by
focusing on maximizing the average accuracy of each group as opposed to maximizing the accuracy
as a whole without attention to accuracy across different groups. in a similar work [49], authors
propose a decoupled classification system where a separate classifier is learned for each group. they
use transfer learning to reduce the issue of having less data for minority groups. in [73] authors
propose to achieve fair classification by mitigating the dependence of the classification outcome on
the sensitive attributes by utilizing the wasserstein distance measure. in [75] authors propose the
preferential sampling (ps) method to create a discrimination free train data set. they then learn
a classifier on this discrimination free dataset to have a classifier with no discrimination. in [102],
authors propose a post-processing bias mitigation strategy that utilizes attention mechanism for
classification and that can provide interpretability.
algorithm
reference
pre-processing
in-processing
post-processing
community detection
[104]
âœ“
word embedding
[23]
âœ“
optimized pre-processing
[27]
âœ“
data pre-processing
[76]
âœ“
classification
[159]
âœ“
regression
[14]
âœ“
classification
[78]
âœ“
classification
[155]
âœ“
adversarial learning
[90]
âœ“
classification
[63]
âœ“
word embedding
[20]
âœ“
classification
[125]
âœ“
classification
[102]
âœ“
table 4. algorithms categorized into their appropriate groups based on being pre-processing, in-
processing, or post-processing.
5.2.2
fair regression. [14] proposes a fair regression method along with evaluating it with a
measure introduced as the â€œprice of fairnessâ€ (pof) to measure accuracy-fairness trade-offs. they
introduce three fairness penalties as follows:
individual fairness: the definition for individual fairness as stated in [14], â€œfor every cross pair
(ğ‘¥,ğ‘¦) âˆˆğ‘†1, (ğ‘¥â€²,ğ‘¦â€²) âˆˆğ‘†2, a model ğ‘¤is penalized for how differently it treats ğ‘¥and ğ‘¥â€² (weighted by a
function of |ğ‘¦âˆ’ğ‘¦â€²|) where ğ‘†1 and ğ‘†2 are different groups from the sampled population.â€ formally,
this is operationalized as
ğ‘“1(ğ‘¤,ğ‘†) =
1
ğ‘›1ğ‘›2
âˆ‘ï¸
(ğ‘¥ğ‘–,ğ‘¦ğ‘–) âˆˆğ‘†1
(ğ‘¥ğ‘—,ğ‘¦ğ‘—) âˆˆğ‘†2
ğ‘‘(ğ‘¦ğ‘–,ğ‘¦ğ‘—)(ğ‘¤.ğ‘¥ğ‘–âˆ’ğ‘¤.ğ‘¥ğ‘—)2
a survey on bias and fairness in machine learning
17
group fairness: "on average, the two groupsâ€™ instances should have similar labels (weighted by the
nearness of the labels of the instances)" [14].
ğ‘“2(ğ‘¤,ğ‘†) =
 
1
ğ‘›1ğ‘›2
âˆ‘ï¸
(ğ‘¥ğ‘–,ğ‘¦ğ‘–) âˆˆğ‘†1
(ğ‘¥ğ‘—,ğ‘¦ğ‘—) âˆˆğ‘†2
ğ‘‘(ğ‘¦ğ‘–,ğ‘¦ğ‘—)(ğ‘¤.ğ‘¥ğ‘–âˆ’ğ‘¤.ğ‘¥ğ‘—)
!2
hybrid fairness: "hybrid fairness requires both positive and both negatively labeled cross pairs to be
treated similarly in an average over the two groups" [14].
ğ‘“3(ğ‘¤,ğ‘†) =
 
âˆ‘ï¸
(ğ‘¥ğ‘–,ğ‘¦ğ‘–) âˆˆğ‘†1
(ğ‘¥ğ‘—,ğ‘¦ğ‘—) âˆˆğ‘†2
ğ‘¦ğ‘–=ğ‘¦ğ‘—=1
ğ‘‘(ğ‘¦ğ‘–,ğ‘¦ğ‘—)(ğ‘¤.ğ‘¥ğ‘–âˆ’ğ‘¤.ğ‘¥ğ‘—)
ğ‘›1,1ğ‘›2,1
!2
+
 
âˆ‘ï¸
(ğ‘¥ğ‘–,ğ‘¦ğ‘–) âˆˆğ‘†1
(ğ‘¥ğ‘—,ğ‘¦ğ‘—) âˆˆğ‘†2
ğ‘¦ğ‘–=ğ‘¦ğ‘—=âˆ’1
ğ‘‘(ğ‘¦ğ‘–,ğ‘¦ğ‘—)(ğ‘¤.ğ‘¥ğ‘–âˆ’ğ‘¤.ğ‘¥ğ‘—)
ğ‘›1,âˆ’1ğ‘›2,âˆ’1
!2
in addition to the previous work, [1] considers the fair regression problem formulation with regards
to two notions of fairness statistical (demographic) parity and bounded group loss. [2] uses decision
trees to satisfy disparate impact and treatment in regression tasks in addition to classification.
5.2.3
structured prediction. in [167], authors studied the semantic role-labeling models and a
famous dataset, imsitu, and realized that only 33% of agent roles in cooking images are man, and
the rest of 67% cooking images have woman as agents in the imsitu training set. they also noticed
that in addition to the existing bias in the dataset, the model would amplify the bias such that after
training a model5 on the dataset, bias is magnified for â€œmanâ€, filling only 16% of cooking images.
under these observations, the authors of the paper [167] show that structured prediction models
have the risk of leveraging social bias. therefore, they propose a calibration algorithm called rba
(reducing bias amplification); rba is a technique for debiasing models by calibrating prediction in
structured prediction. the idea behind rba is to ensure that the model predictions follow the same
distribution in the training data. they study two cases: multi-label object and visual semantic role
labeling classification. they show how these methods amplify the existing bias in data.
5.2.4
fair pca. in [137] authors show that vanilla pca can exaggerate the error in reconstruction
in one group of people over a different group of equal size, so they propose a fair method to create
representations with similar richness for different populationsâ€”not to make them indistinguishable,
or to hide dependence on a sensitive or protected attribute. they show that vanilla pca on the
labeled faces in the wild (lfw) dataset [68] has a lower reconstruction error rate for men than for
women faces, even if the sampling is done with an equal weight for both genders. they intend to
introduce a dimensionality reduction technique which maintains similar fidelity for different groups
and populations in the dataset. therefore, they introduce fair pca and define a fair dimensionality
reduction algorithm. their definition of fair pca (as an optimization function) is as follows, in
which ğ´and ğµdenote two subgroups, ğ‘ˆğ´and ğ‘ˆğµdenote matrices whose rows correspond to rows of
ğ‘ˆthat contain members of subgroups ğ´and ğµgiven ğ‘šdata points in ğ‘…ğ‘›:
ğ‘šğ‘–ğ‘›ğ‘ˆâˆˆğ‘…ğ‘šÃ—ğ‘›,ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘ˆ) â‰¤ğ‘‘ğ‘šğ‘ğ‘¥
(
1
|ğ´|ğ‘™ğ‘œğ‘ ğ‘ (ğ´,ğ‘ˆğ´), 1
|ğµ|ğ‘™ğ‘œğ‘ ğ‘ (ğµ,ğ‘ˆğµ)
)
and their proposed algorithm is a two-step process listed below:
(1) relax the fair pca objective to a semidefinite program (sdp) and solve it.
(2) solve a linear program that would reduce the rank of the solution.
5specifically, a conditional random field (crf)
18
mehrabi et al.
5.2.5
community detection/graph embedding/clustering. inequalities in online communities
and social networks can also potentially be another place where bias and discrimination can affect the
populations. for example, in online communities users with a fewer number of friends or followers
face a disadvantage of being heard in online social media [104]. in addition, existing methods, such
as community detection methods, can amplify this bias by ignoring these low-connected users in
the network or by wrongfully assigning them to the irrelevant and small communities. in [104]
authors show how this type of bias exists and is perpetuated by the existing community detection
methods. they propose a new attributed community detection method, called clan, to mitigate
the harm toward disadvantaged groups in online social communities. clan is a two-step process
that considers the network structure alongside node attributes to address exclusion bias, as indicated
below:
(1) detect communities using modularity values (step 1-unsupervised using only network struc-
ture).
(2) train a classifier to classify users in the minor groups, putting them into one of the major
groups using held-out node attributes (step 2-supervised using other node attributes).
fair methods in domains similar to community detection are also proposed, such as graph embedding
[22] and clustering [8, 31].
5.2.6
causal approach to fairness. causal models can ascertain causal relationships between
variables. using causal graphs one can represent these causal relationships between variables (nodes
of the graph) through the edges of the graph. these models can be used to remove unwanted causal
dependence of outcomes on sensitive attributes such as gender or race in designing systems or policies
[96]. many researchers have used causal models and graphs to solve fairness-related concerns in
machine learning. in [33, 96], authors discuss in detail the subject of causality and its importance
while designing fair algorithms. there has been much research on discrimination discovery and re-
moval that uses causal models and graphs in order to make decisions that are irrespective of sensitive
attributes of groups or individuals. for instance, in [164] authors propose a causal-based framework
that detects direct and indirect discrimination in the data along with their removal techniques. [165]
is an extension to the previous work. [160] gives a nice overview of most of the previous work done
in this area by the authors, along with discussing system-, group-, and individual-level discrimination
and solving each using their previous methods, in addition to targeting direct and indirect discrimi-
nation. by expanding on the previous work and generalizing it, authors in [116] propose a similar
pathway approach for fair inference using causal graphs; this would restrict certain problematic and
discriminative pathways in the causal graph flexibly given any set of constraints. this holds when
the path-specific effects can be identified from the observed distribution. in [32] authors introduce
the path-specific counterfactual fairness definition which is an extension to counterfactual fairness
definition [87] and propose a method to achieve it further extending the work in [116]. in [115]
authors extended a formalization of algorithmic fairness from their previous work to the setting of
learning optimal policies that are subject to constraints based on definitions of fairness. they describe
several strategies for learning optimal policies by modifying some of the existing strategies, such as
q-learning, value search, and g-estimation, based on some fairness considerations. in [162] authors
only target discrimination discovery and no removal by finding instances similar to another instance
and observing if a change in the protected attribute will change the outcome of the decision. if so,
they declare the existence of discrimination. in [82], authors define the following two notions of
discriminationâ€”unresolved discrimination and proxy discriminationâ€”as follows:
unresolved discrimination: "a variable v in a causal graph exhibits unresolved discrimination if
there exists a directed path from a to v that is not blocked by a resolving variable, and v itself is
non-resolving" [82].
a survey on bias and fairness in machine learning
19
proxy discrimination: "a variable v in a causal graph exhibits potential proxy discrimination, if
there exists a directed path from a to v that is blocked by a proxy variable and v itself is not a
proxy" [82]. they proposed methods to prevent and avoid them. they also show that no observational
criterion can determine whether a predictor exhibits unresolved discrimination; therefore, a causal
reasoning framework needs to be incorporated.
in [127], instead of using the usual risk difference ğ‘…ğ·= ğ‘1 âˆ’ğ‘2, authors propose a causal risk
difference ğ‘…ğ·ğ‘= ğ‘1 âˆ’ğ‘ğ‘
2 for causal discrimination discovery. they define ğ‘ğ‘
2 to be:
ğ‘ğ‘
2 =
Ã­
sâˆˆğ‘†,ğ‘‘ğ‘’ğ‘(s)=âŠ–ğ‘¤(s)
Ã­
sâˆˆğ‘†ğ‘¤(s)
ğ‘…ğ·ğ‘not close to zero means that there is a bias in decision value due to group membership (causal
discrimination) or to covariates that have not been accounted for in the analysis (omitted variable
bias). this ğ‘…ğ·ğ‘then becomes their causal discrimination measure for discrimination discovery. [161]
is another work of this type that uses causal networks for discrimination discovery.
5.3
fair representation learning
5.3.1
variational auto encoders. learning fair representations and avoiding the unfair interfer-
ence of sensitive attributes has been introduced in many different research papers. a well-known
example is the variational fair autoencoder introduced in [97]. here,they treat the sensitive variable
as the nuisance variable, so that by removing the information about this variable they will get a fair
representation. they use a maximum mean discrepancy regularizer to obtain invariance in the poste-
rior distribution over latent variables. adding this maximum mean discrepancy (mmd) penalty into
the lower bound of their vae architecture satisfies their proposed model for having the variational
fair autoencoder. similar work, but not targeting fairness specifically, has been introduced in [72].
in [5] authors also propose a debiased vae architecture called db-vae which learns sensitive latent
variables that can bias the model (e.g., skin tone, gender, etc.) and propose an algorithm on top of this
db-vae using these latent variables to debias systems like facial detection systems. in [112] authors
model their representation-learning task as an optimization objective that would minimize the loss of
the mutual information between the encoding and the sensitive variable. the relaxed version of this
assumption is shown in equation 1. they use this in order to learn fair representation and show that
adversarial training is unnecessary and in some cases even counter-productive. in equation 1, c is the
sensitive variable and z the encoding of x.
ğ‘šğ‘–ğ‘›
ğ‘
l(ğ‘,ğ‘¥) + ğœ†ğ¼(ğ‘§,ğ‘)
(1)
in [42], authors introduce flexibly fair representation learning by disentanglement that disentangles
information from multiple sensitive attributes. their flexible and fair variational autoencoder is
not only flexible with respect to downstream task labels but also flexible with respect to sensitive
attributes. they address the demographic parity notion of fairness, which can target multiple sensitive
attributes or any subset combination of them.
5.3.2
adversarial learning. in [90] authors present a framework to mitigate bias in models
learned from data with stereotypical associations. they propose a model in which they are trying to
maximize accuracy of the predictor on y, and at the same time minimize the ability of the adversary
to predict the protected or sensitive variable (stereotyping variable z). the model consists of two
partsâ€”the predictor and the adversaryâ€”as shown in figure 6. in their model, the predictor is trained
to predict y given x. with the help of a gradient-based approach like stochastic gradient descent,
the model tries to learn the weights w by minimizing some loss function lp(Ë†ğ‘¦, y). the output layer
is passed to an adversary, which is another network. this network tries to predict z. the adversary
20
mehrabi et al.
may have different inputs depending on the fairness definition needing to be achieved. for instance,
in order to satisfy demographic parity, the adversary would try to predict the protected variable z
using only the predicted label Ë†ğ‘Œpassed as an input to it, while preventing the adversary from learning
this is the goal of the predictor. similarly, to achieve equality of odds, the adversary would get
the true label y in addition to the predicted label Ë†ğ‘Œ. to satisfy equality of opportunity for a given
class y, they would only select instances for the adversary where y=y. [156] takes an interesting
and different direction toward solving fairness issues using adversarial networks by introducing
fairgan which generates synthetic data that is free from discrimination and is similar to the real
data. they use their newly generated synthetic data from fairgan, which is now debiased, instead
of the real data for training and testing. they do not try to remove discrimination from the dataset,
unlike many of the existing approaches, but instead generate new datasets similar to the real one
which is debiased and preserves good data utility. the architecture of their fairgan model is shown
in figure 5. fairgan consists of two components: a generator ğºğ·ğ‘’ğ‘which generates the fake data
conditioned on the protected attribute ğ‘ƒğº(ğ‘¥,ğ‘¦,ğ‘ ) = ğ‘ƒğº(ğ‘¥,ğ‘¦|ğ‘ )ğ‘ƒğº(ğ‘ ) where ğ‘ƒğº(ğ‘ ) = ğ‘ƒğ‘‘ğ‘ğ‘¡ğ‘(ğ‘ ), and two
discriminators ğ·1 and ğ·2. ğ·1 is trained to differentiate the real data denoted by ğ‘ƒğ‘‘ğ‘ğ‘¡ğ‘(ğ‘¥,ğ‘¦,ğ‘ ) from
the generated fake data denoted by ğ‘ƒğº(ğ‘¥,ğ‘¦,ğ‘ ).
pz
noise
ps
protected attribute
gdec
generator
pg (!, #|%)
pdata (!, #|%)
d2
discriminator
d1
discriminator
real: (!, #, %)
fake: (&!, &#, Ì‚%)
(&!, &#| Ì‚% = 0)
(&!, &#| Ì‚% = 1)
fig. 5. structure of fairgan as proposed in [156].
fig. 6. the architecture of adversarial network proposed in [90] Â© brian hu zhang.
in addition to that, for achieving fairness constraints, such as statistical parity, ğ‘ƒğº(ğ‘¥,ğ‘¦|ğ‘ = 1) =
ğ‘ƒğº(ğ‘¥,ğ‘¦|ğ‘ = 0), the training of ğ·2 is such that it emphasizes differentiation of the two types of
a survey on bias and fairness in machine learning
21
synthetic (generated by the model) samples ğ‘ƒğº(ğ‘¥,ğ‘¦|ğ‘ = 1) and ğ‘ƒğº(ğ‘¥,ğ‘¦|ğ‘ = 0) indicating if the
synthetic samples are from the unprotected or protected groups. here s denotes the protected or the
sensitive variable, and we adapted the same notation as in [156].
5.4
fair nlp
5.4.1
word embedding. in [20] authors noticed that while using state-of-the-art word embeddings
in word analogy tests, â€œmanâ€ would be mapped to â€œcomputer programmerâ€ and â€œwomanâ€ would
be mapped to â€œhomemaker.â€ this bias toward woman triggered the authors to propose a method to
debias word embeddings by proposing a method that respects the embeddings for gender-specific
words but debiases embeddings for gender-neutral words by following these steps: (notice that step
2 has two different options. depending on whether you target hard debiasing or soft debiasing, you
would use either step 2a or 2b)
(1) identify gender subspace. identifying a direction of the embedding that captures the bias
[20].
(2) hard debiasing or soft debiasing:
(a) hard debiasing (neutralize and equalize). neutralize puts away the gender subspace
from gender-neutral words and makes sure that all the gender-neutral words are removed
and zeroed out in the gender subspace [20]. equalize makes gender-neutral words to be
equidistant from the equality set of gendered words [20].
(b) soft bias correction. tries to move as little as possible to retain its similarity to the original
embedding as much as possible, while reducing the gender bias. this trade-off is controlled
by a parameter [20].
following on the footsteps of these authors, other future work attempted to tackle this problem
[169] by generating a gender-neutral version of (glove called gn-glove) that tries to retain gender
information in some of the word embeddingâ€™s learned dimensions, while ensuring that other dimen-
sions are free from this gender effect. this approach primarily relies on glove as its base model
with gender as the protected attribute. however, a recent paper [58] argues against these debiasing
techniques and states that many recent works on debiasing word embeddings have been superficial,
that those techniques just hide the bias and donâ€™t actually remove it. a recent work [23] took a new
direction and proposed a preprocessing method for the discovery of the problematic documents in
the training corpus that have biases in them, and tried to debias the system by perturbing or removing
these documents efficiently from the training corpus. in a very recent work [166], authors target bias
in elmoâ€™s contextualized word vectors and attempt to analyze and mitigate the observed bias in the
embeddings. they show that the corpus used for training of elmo has a significant gender skew,
with male entities being nearly three times more common than female entities. this automatically
leads to gender bias in these pretrained contextualized embeddings. they propose the following two
methods for mitigating the existing bias while using the pretrained embeddings in a downstream task,
coreference resolution: (1) train-time data augmentation approach, and (2) test-time neutralization
approach.
5.4.2
coreference resolution. the [168] paper shows that coreference systems have a gender
bias. they introduce a benchmark, called winobias, focusing on gender bias in coreference resolution.
in addition to that, they introduce a data-augmentation technique that removes bias in the existing
state-of-the-art coreferencing methods, in combination with using word2vec debiasing techniques.
their general approach is as follows: they first generate auxiliary datasets using a rule-based
approach in which they replace all the male entities with female entities and the other way around.
then they train models with a combination of the original and the auxiliary datasets. they use the
above solution in combination with word2vec debiasing techniques to generate word embeddings.
22
mehrabi et al.
they also point out sources of gender bias in coreference systems and propose solutions to them.
they show that the first source of bias comes from the training data and propose a solution that
generates an auxiliary data set by swapping male and female entities. another case arises from
the resource bias (word embeddings are bias), so the proposed solution is to replace glove with a
debiased embedding method. last, another source of bias can come from unbalanced gender lists, and
balancing the counts in the lists is a solution they proposed. in another work [134], authors also show
the existence of gender bias in three state-of-the-art coreference resolution systems by observing that
for many occupations, these systems resolve pronouns in a biased fashion by preferring one gender
over the other.
5.4.3
language model. in [21] authors introduce a metric for measuring gender bias in a
generated text from a language model based on recurrent neural networks that is trained on a text
corpus along with measuring the bias in the training text itself. they use equation 2, where ğ‘¤is any
word in the corpus, ğ‘“is a set of gendered words that belong to the female category, such as she, her,
woman, etc., and ğ‘što the male category, and measure the bias using the mean absolute and standard
deviation of the proposed metric along with fitting a univariate linear regression model over it and
then analyzing the effectiveness of each of those metrics while measuring the bias.
ğ‘ğ‘–ğ‘ğ‘ (ğ‘¤) = ğ‘™ğ‘œğ‘”( ğ‘ƒ(ğ‘¤|ğ‘“)
ğ‘ƒ(ğ‘¤|ğ‘š) )
(2)
in their language model, they also introduce a regularization loss term that would minimize the
projection of embeddings trained by the encoder onto the embedding of the gender subspace following
the soft debiasing technique introduced in [20]. finally, they evaluate the effectiveness of their method
on reducing gender bias and conclude by stating that in order to reduce bias, there is a compromise
on perplexity. they also point out the effectiveness of word-level bias metrics over the corpus-level
metrics.
5.4.4
sentence encoder. in [100] authors extend the research in detecting bias in word embed-
ding techniques to that of sentence embedding. they try to generalize bias-measuring techniques,
such as using the word embedding association test (weat [26]) in the context of sentence en-
coders by introducing their new sentence encoding bias-measuring techniques, the sentence encoder
association test (seat). they used state-of-the-art sentence encoding techniques, such as cbow,
gpt, elmo, and bert, and find that although there was varying evidence of human-like bias in
sentence encoders using seat, more recent methods like bert are more immune to biases. that
being said, they are not claiming that these models are bias-free, but state that more sophisticated
bias discovery techniques may be used in these cases, thereby encouraging more future work in this
area.
5.4.5
machine translation. in [52] authors noticed that when translating the word "friend" in the
following two sentences from english to spanish, they achieved different resultsâ€”although in both
cases this word should be translated the same way.
"she works in a hospital, my friend is a nurse."
"she works in a hospital, my friend is a doctor."
in both of these sentences, "friend" should be translated to the female version of spanish friend
"amiga," but the results were not reflecting this expectation. for the second sentence, friend was
translated to "amigo,"â€”the male version of friend in spanish. this is because doctor is more
stereotypical to males and nurse to females, and the model picks this bias or stereotype and reflects it
in its performance. to solve this, authors in [52] build an approach that leverages the fact that machine
translation uses word embeddings. they use the existing debiasing methods in word embedding and
apply them in the machine translation pipeline. this not only helped them to mitigate the existing bias
a survey on bias and fairness in machine learning
23
in their system, but also boosted the performance of their system by one blue score. in [126] authors
show that googleâ€™s translate system can suffer from gender bias by making sentences taken from the
u.s. bureau of labor statistics into a dozen languages that are gender neutral, including yoruba,
hungarian, and chinese, translating them into english, and showing that google translate shows
favoritism toward males for stereotypical fields such as stem jobs. in [148] authors annotated and
analyzed the europarl dataset [84], a large political, multilingual dataset used in machine translation,
and discovered that with the exception of the youngest age group (20-30), which represents only a
very small percentage of the total amount of sentences (0.71%), more male data is available in all age
groups. they also looked at the entire dataset and showed that 67.39% of the sentences are produced
by male speakers. furthermore, to mitigate the gender-related issues and to improve morphological
agreement in machine translation, they augmented every sentence with a tag on the english source
side, identifying the gender of the speaker. this helped the system in most of the cases, but not
always, so further work has been suggested for integrating speaker information in other ways.
5.4.6
named entity recognition. in [101], authors investigate a type of existing bias in various
named entity recognition (ner) systems. in particular, they observed that in a context where an
entity should be tagged as a person entity, such as "john is a person" or "john is going to school",
more female names as opposed to male names are being tagged as non-person entities or not being
tagged at all. to further formalize their observations, authors propose six different evaluation metrics
that would measure amount of bias among different genders in ner systems. they curated templated
sentences pertaining to human actions and applied these metrics on names from u.s census data
incorporated into the templates. the six introduced measures each aim to demonstrate a certain type
of bias and serve a specific purpose in showing various results as follows:
â€¢ error type-1 unweighted: through this type of error, authors wanted to recognize the pro-
portion of entities that are tagged as anything other than the person entity in each of the male
vs female demographic groups. this could be the entity not being tagged or tagged as other
entities, such as location.
Ã­
ğ‘›âˆˆğ‘ğ‘“ğ¼(ğ‘›ğ‘¡ğ‘¦ğ‘ğ‘’â‰ ğ‘ƒğ¸ğ‘…ğ‘†ğ‘‚ğ‘)
|ğ‘ğ‘“|
â€¢ error type-1 weighted: this type of error is similar to its unweighted case except authors
considered the frequency or popularity of names so that they could penalize if a more popular
name is being tagged wrongfully.
Ã­
ğ‘›âˆˆğ‘ğ‘“ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘“(ğ‘›ğ‘¡ğ‘¦ğ‘ğ‘’â‰ ğ‘ƒğ¸ğ‘…ğ‘†ğ‘‚ğ‘)
Ã­
ğ‘›âˆˆğ‘ğ‘“ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘“(ğ‘›)
,
where ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘“(Â·) indicates the frequency of a name for a particular year in the female census
data. likewise, ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘š(Â·) indicates the frequency of a name for a particular year in the male
census data.
â€¢ error type-2 unweighted: this is a type of error in which the entity is tagged as other entities,
such as location or city. notice that this error does not count if the entity is not tagged.
Ã­
ğ‘›âˆˆğ‘ğ‘“ğ¼(ğ‘›ğ‘¡ğ‘¦ğ‘ğ‘’âˆ‰{âˆ…, ğ‘ƒğ¸ğ‘…ğ‘†ğ‘‚ğ‘})
|ğ‘ğ‘“|
,
where âˆ…indicates that the name is not tagged.
24
mehrabi et al.
â€¢ error type-2 weighted: this error is again similar to its unweighted case except the frequency
is taken into consideration.
Ã­
ğ‘›âˆˆğ‘ğ‘“ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘“(ğ‘›ğ‘¡ğ‘¦ğ‘ğ‘’âˆ‰{âˆ…, ğ‘ƒğ¸ğ‘…ğ‘†ğ‘‚ğ‘})
Ã­
ğ‘›âˆˆğ‘ğ‘“ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘“(ğ‘›)
â€¢ error type-3 unweighted: this is a type of error in which it reports if the entity is not tagged
at all. notice that even if the entity is tagged as a non-person entity this error type would not
consider it.
Ã­
ğ‘›âˆˆğ‘ğ‘“ğ¼(ğ‘›ğ‘¡ğ‘¦ğ‘ğ‘’= âˆ…)
|ğ‘ğ‘“|
â€¢ error type-3 weighted: again, this error is similar to its unweighted case with frequency taken
into consideration.
Ã­
ğ‘›âˆˆğ‘ğ‘“ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘“(ğ‘›ğ‘¡ğ‘¦ğ‘ğ‘’= âˆ…)
Ã­
ğ‘›âˆˆğ‘ğ‘“ğ‘“ğ‘Ÿğ‘’ğ‘ğ‘“(ğ‘›)
authors also investigate the data that these ner systems are trained on and find that the data is also
biased toward female gender by not including as versatile names as there should be to represent
female names.
5.5
comparison of different mitigation algorithms
the field of algorithmic fairness is a relatively new area of research and work still needs to be done
for its improvement. with that being said, there are already papers that propose fair ai algorithms and
bias mitigation techniques and compare different mitigation algorithms using different benchmark
datasets in the fairness domain. for instance, authors in [65] propose a geometric solution to learn fair
representations that removes correlation between protected and unprotected features. the proposed
approach can control the trade-off between fairness and accuracy via an adjustable parameter. in
this work, authors evaluate the performance of their approach on different benchmark datasets, such
as compas, adult and german, and compare them against various different approaches for fair
learning algorithms considering fairness and accuracy measures [65, 72, 158, 159]. in addition,
ibmâ€™s ai fairness 360 (aif360) toolkit [11] has implemented many of the current fair learning
algorithms and has demonstrated some of the results as demos which can be utilized by interested
users to compare different methods with regards to different fairness measures.
6
challenges and opportunities for fairness research
while there have been many definitions of, and approaches to, fairness in the literature, the study
in this area is anything but complete. fairness and algorithmic bias still holds a number of research
opportunities. in this section, we provide pointers to outstanding challenges in fairness research, and
an overview of opportunities for development of understudied problems.
6.1
challenges
there are several remaining challenges to be addressed in the fairness literature. among them are:
(1) synthesizing a definition of fairness. several definitions of what would constitute fairness
from a machine learning perspective have been proposed in the literature. these definitions
cover a wide range of use cases, and as a result are somewhat disparate in their view of fairness.
because of this, it is nearly impossible to understand how one fairness solution would fare
under a different definition of fairness. synthesizing these definitions into one remains an open
research problem since it can make evaluation of these systems more unified and comparable.
a survey on bias and fairness in machine learning
25
having a more unified fairness definition and framework can also help with the incompatibility
issue of some current fairness definitions.
(2) from equality to equity. the definitions presented in the literature mostly focus on equality,
ensuring that each individual or group is given the same amount of resources, attention or
outcome. however, little attention has been paid to equity, which is the concept that each
individual or group is given the resources they need to succeed [60, 103]. operationalizing this
definition and studying how it augments or contradicts existing definitions of fairness remains
an exciting future direction.
(3) searching for unfairness. given a definition of fairness, it should be possible to identify
instances of this unfairness in a particular dataset. inroads toward this problem have been made
in the areas of data bias by detecting instances of simpsonâ€™s paradox in arbitrary datasets [3];
however, unfairness may require more consideration due to the variety of definitions and the
nuances in detecting each one.
fig. 7. heatmap depicting distribution of previous work in fairness, grouped by domain and fairness
definition.
6.2
opportunities
in this work we have taxonomized and summarized the current state of research into algorithmic
biases and fairnessâ€”with a particular focus on machine learning. even in this area alone, the research
is broad. subareas, from natural language processing, to representation learning, to community
detection, have all seen efforts to make their methodologies more fair. nevertheless, every area
has not received the same amount of attention from the research community. figure 7 provides an
overview of what has been done in different areas to address fairnessâ€”categorized by the fairness
definition type and domain. some areas (e.g., community detection at the subgroup level) have
received no attention in the literature, and could be fertile future research areas.
7

method with application to large-
sirignano, justin; macart, jonathan f.; freund, jonathan b.
journal of computational physics, vol. 423 https://doi.org/10.1016/j.jcp.202
on some neural network architectures that can represent viscosity solutions
partial differential equations
darbon, jÃ©rÃ´me; meng, tingwei
journal of computational physics, vol. 425 https://doi.org/10.1016/j.jcp.202
b-pinns: bayesian physics-informed neural networks for forward and inver
yang, liu; meng, xuhui; karniadakis, george em
journal of computational physics, vol. 425 https://doi.org/10.1016/j.jcp.202
deep learning of free boundary and stefan problems
wang, sifan; perdikaris, paris
journal of computational physics, vol. 428 https://doi.org/10.1016/j.jcp.202
learning constitutive relations using symmetric positive definite neural netw
xu, kailai; huang, daniel z.; darve, eric
journal of computational physics, vol. 428 https://doi.org/10.1016/j.jcp.202
pfnn: a penalty-free neural network method for solving a class of second-
geometries
sheng, hailong; yang, chao
journal of computational physics, vol. 428 https://doi.org/10.1016/j.jcp.202
a method for representing periodic functions and enforcing exactly periodic
dong, suchuan; ni, naxian
journal of computational physics, vol. 435 https://doi.org/10.1016/j.jcp.202
deepm&mnet: inferring the electroconvection multiphysics fields based on 
cai, shengze; wang, zhicheng; lu, lu
journal of computational physics, vol. 436 https://doi.org/10.1016/j.jcp.202
deepm&mnet for hypersonics: predicting the coupled flow and finite-rate ch
network approximation of operators
mao, zhiping; lu, lu; marxen, olaf
journal of computational physics, vol. 447 https://doi.org/10.1016/j.jcp.202
solving initial-boundary value problems for systems of partial differential eq
techniques
shekari beidokhti, r.; malek, a.
journal of the franklin institute, vol. 346, issue 9 https://doi.org/10.1016/j.jf
automl: a survey of the state-of-the-art
he, xin; zhao, kaiyong; chu, xiaowen
knowledge-based systems, vol. 212 https://doi.org/10.1016/j.knosys.2020
backpropagation algorithms and reservoir computing in recurrent neural 
spatiotemporal dynamics
vlachas, p. r.; pathak, j.; hunt, b. r.
neural networks, vol. 126 https://doi.org/10.1016/j.neunet.2020.02.016
sympnets: intrinsic structure-preserving symplectic networks for identifying
jin, pengzhan; zhang, zhen; zhu, aiqing
neural networks, vol. 132 https://doi.org/10.1016/j.neunet.2020.08.017
learning dynamical systems from data: a simple cross-validation perspectiv
hamzi, boumediene; owhadi, houman
physica d: nonlinear phenomena, vol. 421 https://doi.org/10.1016/j.physd.
transfer learning enhanced physics informed neural network for phase-field
goswami, somdatta; anitescu, cosmin; chakraborty, souvik
theoretical and applied fracture mechanics, vol. 106 https://doi.org/10.101
operator-adapted wavelets, fast solvers, and numerical homogenization
owhadi, houman; scovel, clint
cambridge monographs on applied and computational mathematics, vol. 3
inverse problems: a bayesian perspective
stuart, a. m.
acta numerica, vol. 19 https://doi.org/10.1017/s0962492910000061
reynolds averaged turbulence modelling using deep neural networks with e
ling, julia; kurzawski, andrew; templeton, jeremy
journal of fluid mechanics, vol. 807 https://doi.org/10.1017/jfm.2016.615
flow over an espresso cup: inferring 3-d velocity and pressure fields from t
physics-informed neural networks
cai, shengze; wang, zhicheng; fuest, frederik
journal of fluid mechanics, vol. 915 https://doi.org/10.1017/jfm.2021.135
the harvard clean energy project: large-scale computational screening a
community grid
hachmann, johannes; olivares-amaya, roberto; atahan-evrenk, sule
the journal of physical chemistry letters, vol. 2, issue 17 https://doi.org/1
physicsâ€informed deep neural networks for learning parameters and con
problems
tartakovsky, a. m.; marrero, c. ortiz; perdikaris, paris
water resources research, vol. 56, issue 5 https://doi.org/10.1029/2019w
coupled timeâ€lapse fullâ€waveform inversion for subsurface flow problem
li, dongzhuo; xu, kailai; harris, jerry m.
water resources research, vol. 56, issue 8 https://doi.org/10.1029/2019w
deep learning for universal linear embeddings of nonlinear dynamics
lusch, bethany; kutz, j. nathan; brunton, steven l.
nature communications, vol. 9, issue 1 https://doi.org/10.1038/s41467-018
physicallyÂ informed artificial neural networks for atomistic modeling of mate
pun, g. p. purja; batra, r.; ramprasad, r.
nature communications, vol. 10, issue 1 https://doi.org/10.1038/s41467-01
coarse-graining auto-encoders for molecular dynamics
wang, wujie; gÃ³mez-bombarelli, rafael
npj computational materials, vol. 5, issue 1 https://doi.org/10.1038/s41524
deep learning and process understanding for data-driven earth system scie
reichstein, markus; camps-valls, gustau; stevens, bjorn
nature, vol. 566, issue 7743 https://doi.org/10.1038/s41586-019-0912-1
integrating machine learning and multiscale modelingâ€”perspectives, challe
biomedical, and behavioral sciences
alber, mark; buganza tepole, adrian; cannon, william r.
npj digital medicine, vol. 2, issue 1 https://doi.org/10.1038/s41746-019-019
learning nonlinear operators via deeponet based on the universal approxi
lu, lu; jin, pengzhan; pang, guofei
nature machine intelligence, vol. 3, issue 3 https://doi.org/10.1038/s42256
predicting molecular properties with covariant compositional networks
hy, truong son; trivedi, shubhendu; pan, horace
the journal of chemical physics, vol. 148, issue 24 https://doi.org/10.1063
large scale and linear scaling dft with the conquest code
nakata, ayako; baker, jack s.; mujahed, shereif y.
the journal of chemical physics, vol. 152, issue 16 https://doi.org/10.1063
a point-cloud deep learning framework for prediction of fluid flow fields on ir
kashefi, ali; rempe, davis; guibas, leonidas j.
physics of fluids, vol. 33, issue 2 https://doi.org/10.1063/5.0033376
discovering governing equations from data by sparse identification of nonlin
brunton, steven l.; proctor, joshua l.; kutz, j. nathan
proceedings of the national academy of sciences, vol. 113, issue 15 https
solving high-dimensional partial differential equations using deep learning
han, jiequn; jentzen, arnulf; e., weinan
proceedings of the national academy of sciences, vol. 115, issue 34 https
a mean field view of the landscape of two-layer neural networks
mei, song; montanari, andrea; nguyen, phan-minh
proceedings of the national academy of sciences, vol. 115, issue 33 https
reconciling modern machine-learning practice and the classical biasâ€“varia
belkin, mikhail; hsu, daniel; ma, siyuan
proceedings of the national academy of sciences, vol. 116, issue 32 https
extraction of mechanical properties of materials through deep learning from
lu, lu; dao, ming; kumar, punit
proceedings of the national academy of sciences, vol. 117, issue 13 https
discrete- vs. continuous-time nonlinear signal processing
rico-martÃ­nez, r.; krischer, k.; kevrekidis, i. g.
chemical engineering communications, vol. 118, issue 1 https://doi.org/10
scaling description of generalization with number of parameters in deep lea
geiger, mario; jacot, arthur; spigler, stefano
journal of statistical mechanics: theory and experiment, vol. 2020, issue 2
a jamming transition from under- to over-parametrization affects generaliza
spigler, s.; geiger, m.; dâ€™ascoli, s.
journal of physics a: mathematical and theoretical, vol. 52, issue 47 https
error estimates for deeponets: a deep learning framework in infinite dimen
lanthaler, samuel; mishra, siddhartha; karniadakis, george e.
transactions of mathematics and its applications, vol. 6, issue 1 https://doi
bayesian differential programming for robust systems identification under u
yang, yibo; aziz bhouri, mohamed; perdikaris, paris
proceedings of the royal society a: mathematical, physical and engineerin
doi.org/10.1098/rspa.2020.0290
locally adaptive activation functions with slope recovery for deep and phys
jagtap, ameya d.; kawaguchi, kenji; em karniadakis, george
proceedings of the royal society a: mathematical, physical and engineerin
doi.org/10.1098/rspa.2020.0334
understanding deep convolutional networks
mallat, stÃ©phane
philosophical transactions of the royal society a: mathematical, physical 
https://doi.org/10.1098/rsta.2015.0203
jamming transition as a paradigm to understand the loss landscape of deep
geiger, mario; spigler, stefano; d'ascoli, stÃ©phane
physical review e, vol. 100, issue 1 https://doi.org/10.1103/physreve.100
uncovering turbulent plasma dynamics via deep learning from partial obser
mathews, a.; francisquez, m.; hughes, j. w.
physical review e, vol. 104, issue 2 https://doi.org/10.1103/physreve.104
learning unknown physics of non-newtonian fluids
reyes, brandon; howard, amanda a.; perdikaris, paris
physical review fluids, vol. 6, issue 7 https://doi.org/10.1103/physrevflui
deep potential molecular dynamics: a scalable model with the accuracy o
zhang, linfeng; han, jiequn; wang, han
physical review letters, vol. 120, issue 14 https://doi.org/10.1103/physre
discovering physical concepts with neural networks
iten, raban; metger, tony; wilming, henrik
physical review letters, vol. 124, issue 1 https://doi.org/10.1103/physrev
generalized neural-network representation of high-dimensional potential
behler, jÃ¶rg; parrinello, michele
physical review letters, vol. 98, issue 14 https://doi.org/10.1103/physrev
ab initio solution of the many-electron schrÃ¶dinger equation with deep neur
pfau, david; spencer, james s.; matthews, alexander g. d. g.
physical review research, vol. 2, issue 3 https://doi.org/10.1103/physrev
artificial neural networks for solving ordinary and partial differential equation
lagaris, i. e.; likas, a.; fotiadis, d. i.
ieee transactions on neural networks, vol. 9, issue 5 https://doi.org/10.11
benchmark analysis of representative deep neural network architectures
bianco, simone; cadene, remi; celona, luigi
ieee access, vol. 6 https://doi.org/10.1109/access.2018.2877890
an emergent space for distributed data with hidden internal order throug
kemeth, felix p.; haugland, sindre w.; dietrich, felix
ieee access, vol. 6 https://doi.org/10.1109/access.2018.2882777
deep residual learning for image recognition
he, kaiming; zhang, xiangyu; ren, shaoqing
2016 ieee conference on computer vision and pattern recognition (cvpr
unpaired image-to-image translation using cycle-consistent adversarial n
zhu, jun-yan; park, taesung; isola, phillip
2017 ieee international conference on computer vision (iccv) https://doi
geometric deep learning: going beyond euclidean data
bronstein, michael m.; bruna, joan; lecun, yann
ieee signal processing magazine, vol. 34, issue 4 https://doi.org/10.1109/
exascale deep learning for climate analytics
kurth, thorsten; treichler, sean; romero, joshua
sc18: international conference for high performance computing, network
sc.2018.00054
pushing the limit of molecular dynamics with ab initio accuracy to 100 mill
jia, weile; wang, han; chen, mohan
sc20: international conference for high performance computing, network
sc41405.2020.00009
artificial neural network method for solution of boundary value problems w
conditions
mcfall, k. s.; mahan, j. r.
ieee transactions on neural networks, vol. 20, issue 8 https://doi.org/10.1
invariant scattering convolution networks
bruna, joan; mallat, s.
ieee transactions on pattern analysis and machine intelligence, vol. 35, is
distilling free-form natural laws from experimental data
schmidt, michael; lipson, hod
science, vol. 324, issue 5923 https://doi.org/10.1126/science.1165893
hidden fluid mechanics: learning velocity and pressure fields from flow visu
raissi, maziar; yazdani, alireza; karniadakis, george em
science, vol. 367, issue 6481 https://doi.org/10.1126/science.aaw4741
a robotic intelligent towing tank for learning complex fluid-structure dynam
fan, d.; jodin, g.; consi, t. r.
science robotics, vol. 4, issue 36 https://doi.org/10.1126/scirobotics.aay50
a limited memory algorithm for bound constrained optimization
byrd, richard h.; lu, peihuang; nocedal, jorge
siam journal on scientific computing, vol. 16, issue 5 https://doi.org/10.11
bayesian numerical homogenization
owhadi, houman
multiscale modeling & simulation, vol. 13, issue 3 https://doi.org/10.1137/1
multigrid with rough coefficients and multiresolution operator decompositi
owhadi, houman
siam review, vol. 59, issue 1 https://doi.org/10.1137/15m1013894
numerical gaussian processes for time-dependent and nonlinear partial 
raissi, maziar; perdikaris, paris; karniadakis, george em
siam journal on scientific computing, vol. 40, issue 1 https://doi.org/10.11
physics-informed generative adversarial networks for stochastic differenti
yang, liu; zhang, dongkun; karniadakis, george em
siam journal on scientific computing, vol. 42, issue 1 https://doi.org/10.11
fpinns: fractional physics-informed neural networks
pang, guofei; lu, lu; karniadakis, george em
siam journal on scientific computing, vol. 41, issue 4 https://doi.org/10.11
learning in modal space: solving time-dependent stochastic pdes using
zhang, dongkun; guo, ling; karniadakis, george em
siam journal on scientific computing, vol. 42, issue 2 https://doi.org/10.11
deepxde: a deep learning library for solving differential equations
lu, lu; meng, xuhui; mao, zhiping
siam review, vol. 63, issue 1 https://doi.org/10.1137/19m1274067
a phase shift deep neural network for high frequency approximation and
cai, wei; li, xiaoguang; liu, lizuo
siam journal on scientific computing, vol. 42, issue 5 https://doi.org/10.11
the wiener--askey polynomial chaos for stochastic differential equations
xiu, dongbin; karniadakis, george em
siam journal on scientific computing, vol. 24, issue 2 https://doi.org/10.11
systematic construction of neural forms for solving partial differential equ
initial, boundary and interface conditions
lagari, pola lydia; tsoukalas, lefteri h.; safarkhani, salar
international journal on artificial intelligence tools, vol. 29, issue 05 https:/
a correspondence between bayesian estimation on stochastic processes 
kimeldorf, george s.; wahba, grace
the annals of mathematical statistics, vol. 41, issue 2 https://doi.org/10.12
gaussian measure in hilbert space and applications in numerical analysis
larkin, f. m.
rocky mountain journal of mathematics, vol. 2, issue 3 https://doi.org/10.1
seqgan: sequence generative adversarial nets with policy gradient
yu, lantao; zhang, weinan; wang, jun
proceedings of the aaai conference on artificial intelligence, vol. 31, issue
transformer dissection: an unified understanding for transformerâ€™s attent
tsai, yao-hung hubert; bai, shaojie; yamada, makoto
proceedings of the 2019 conference on empirical methods in natural lang
conference on natural language processing (emnlp-ijcnlp) https://doi.
neurodiffeq: a python package for solving differential equations with neura
chen, feiyu; sondak, david; protopapas, pavlos
journal of open source software, vol. 5, issue 46 https://doi.org/10.21105/
universal differential equations for scientific machine learning
rackauckas, christopher; ma, yingbo; martensen, julius
https://doi.org/10.21203/rs.3.rs-55125/v1
deep neural network approach to forward-inverse problems
jo, hyeontae; son, hwijae; ju hwang, hyung
networks & heterogeneous media, vol. 15, issue 2 https://doi.org/10.3934/
frequency principle: fourier analysis sheds light on deep neural network
xu, zhi-qin john
communications in computational physics, vol. 28, issue 5 https://doi.org/
extended physics-informed neural networks (xpinns): a generalized spa
learning framework for nonlinear partial differential equations
karniadakis, ameya d. jagtap & george em
communications in computational physics, vol. 28, issue 5 https://doi.org/
multi-scale deep neural network (mscalednn) for solving poisson-boltzm
liu, ziqi
communications in computational physics, vol. 28, issue 5 https://doi.org/
multi-scale deep neural network (mscalednn) methods for oscillatory sto
wang, bo
communications in computational physics, vol. 28, issue 5 https://doi.org/
on the convergence of physics informed neural networks for linear secon
shin, yeonjong
communications in computational physics, vol. 28, issue 5 https://doi.org/
relu deep neural networks and linear finite elements
sci, juncai he
journal of computational mathematics, vol. 38, issue 3 https://doi.org/10.4
differentialequations.jl â€“ a performant and feature-rich ecosystem for so
rackauckas, christopher; nie, qing
journal of open research software, vol. 5 https://doi.org/10.5334/jors.151
 cited by (4)
search citations:
resource types preprintjournaltext
sort by date sort by title
â†º
extreme vorticity events in turbulent rayleigh-bÃ©nard convection from stere
valori, valentina; krÃ¤uter, robert; schumacher, jÃ¶rg
physical review research, vol. 4, issue 2 https://doi.org/10.1103/physrevr
solving and learning nonlinear pdes with gaussian processes
chen, yifan; hosseini, bamdad; owhadi, houman
arxiv https://doi.org/10.48550/arxiv.2103.12959
emerging directions in geophysical inversion
valentine, andrew; sambridge, malcolm
arxiv https://doi.org/10.48550/arxiv.2110.06017
physics-informed neural operator for fast and scalable optical fiber chan
song, yuchen; wang, danshi; fan, qirui
arxiv https://doi.org/10.48550/arxiv.2208.08868
similar records
physics-informed neural networks (pinns) for fluid
mechanics: a review
journal article Â· sat jan 22 23:00:00 est 2022 Â· acta
mechanica sinica Â· osti id:2282982
multi-fidelity bayesian neural networks: algorithms and
applications
journal article Â· fri apr 16 00:00:00 edt 2021 Â· journal of
computational physics Â· osti id:2281996
scalable algorithms for physics-informed neural and graph
networks
journal article Â· wed jun 29 00:00:00 edt 2022 Â· data-
centric engineering Â· osti id:1872036
related subjects
97 mathematics and computing
applied mathematics
computational science
[image]
â€¢  
 website policies / important links
â€¢  
 contact us
â€¢  
â€¢  vulnerability disclosure program
â€¢  
â€¢  
facebook
â€¢  
x / twitter
â€¢  
youtube

method
predicts recidivism best? a comparison of statistical, machine
learning and data mining predictive models. j. r. stat. soc.
ser. a stat. soc. 176, 565â€“584 (2013).
articleÂ  mathscinetÂ  google scholarÂ 
31. mannshardt, e. & naess, l. air quality in the usa. significance
15, 24â€“27 (october, 2018).
32. zech, j. r. et al. variable generalization performance of a
deep learning model to detect pneumonia in chest
radiographs: a cross-sectional study. plos med. 15, e1002683
(2018).
articleÂ  google scholarÂ 
33. chang, a., rudin, c., cavaretta, m., thomas, r. & chou, g.
how to reverse-engineer quality rankings. mach. learn. 88,
369â€“398 (2012).
articleÂ  mathscinetÂ  google scholarÂ 
34. goodman, b. & flaxman, s. eu regulations on algorithmic
decision-making and a â€˜right to explanationâ€™. ai magazine 38,
3 (2017).
articleÂ  google scholarÂ 
35. wachter, s., mittelstadt, b. & russell, c. counterfactual
explanations without opening the black box: automated
decisions and the gdpr. harvard journal of law & technology
1 (2018).
36. quinlan, j. r. c4. 5: programs for machine learning vol. 1
(morgan kaufmann, 1993).
37. breiman, l., friedman, j., stone, c. j. & olshen, r. a.
classification and regression trees (crc press, 1984).
38. auer, p., holte, r. c. & maass, w. theory and applications of
agnostic pac-learning with small decision trees. in
proceedings of 12th international conference on machine
learning 21â€“29 (morgan kaufmann, 1995).
39. angelino, e., larus-stone, n., alabi, d., seltzer, m. & rudin,
c. certifiably optimal rule lists for categorical data. j. mach.
learn. res. 19, 1â€“79 (2018).
mathscinetÂ  mathÂ  google scholarÂ 
40. wang, f. & rudin, c. falling rule lists. in proceedings of
machine learning research vol. 38: artificial intelligence and
statistics 1013â€“1022 (pmlr, 2015).
41. chen, c. & rudin, c. an optimization approach to learning
falling rule lists. in proceedings of machine learning research
vol. 84 : artificial intelligence and statistics 604â€“612 (pmlr,
2018).
42. hu, x. (s.), rudin, c. & seltzer, m. optimal sparse decision
trees. preprint at https://arxiv.org/abs/1904.12847 (2019).
43. burgess, e. w. factors determining success or failure on parole
(illinois committee on indeterminate-sentence law and
parole, 1928).
44. carrizosa, e., martn-barragÃ¡n, b. & morales, d. r. binarized
support vector machines. informs j. comput. 22, 154â€“167
(2010).
articleÂ  mathscinetÂ  google scholarÂ 
45. sokolovska, n., chevaleyre, y. & zucker, j. d. a provable
algorithm for learning interpretable scoring systems. in
proceedings of machine learning research vol. 84: artificial
intelligence and statistics 566â€“574 (pmlr, 2018).
46. ustun, b. & rudin, c. optimized risk scores. in proceedings of
the 23rd acm sigkdd international conference on knowledge
discovery and data mining (acm, 2017).
47. ustun, b. et al. the world health organization adult
attention-deficit/hyperactivity disorder self-report screening
scale for dsm-5. jama psychiatr. 74, 520â€“526 (2017).
articleÂ  google scholarÂ 
48. chen, c. et al. this looks like that: deep learning for
interpretable image recognition. preprint at https://arxiv.org/
abs/1806.10574 (2018).
49. li, o., liu, h., chen, c. & rudin, c. deep learning for case-
based reasoning through prototypes: a neural network that
explains its predictions. in proceedings of aaai conference on
artificial intelligence 3530â€“3537 (aaai, 2018).
50. gallagher, n. et al. cross-spectral factor analysis. in
proceedings of advances in neural information processing
systems 30 (neurips) 6842â€“6852 (curran associates, 2017).
51. wang, f., rudin, c., mccormick, t. h. & gore, j. l. modeling
recovery curves with application to prostatectomy.
biostatistics https://doi.org/10.1093/biostatistics/kxy002
(2018).
52. lou, y., caruana, r. & gehrke, j. intelligible models for
classification and regression. in proceedings of the 18th acm
sigkdd international conference on knowledge discovery and
data mining (acm, 2012).
download references
acknowledgements
the author thanks f. wang, t. wang, c. chen, o. li, a. barnett, t.
dietterich, m. seltzer, e. angelino, n. larus-stone, e. mannshart, m.
gupta and several others who helped my thought processes in
various ways, and particularly b. ustun, r. parr, r. holte and my
father, s. rudin, who went to considerable efforts to provide
thoughtful comments and discussion. the author acknowledges
funding from the laura and john arnold foundation, nih, nsf,
darpa, the lord foundation of north carolina and mit-lincoln
laboratory.
author information
authors and affiliations
1. duke university, durham, nc, usa
cynthia rudin
authors
1. cynthia rudin
view author publications
search author on:pubmedÂ google scholar
corresponding author
correspondence to cynthia rudin.
ethics declarations
competing interests
the author declares no competing interests.
additional information
publisherâ€™s note: springer nature remains neutral with regard to
jurisdictional claims in published maps and institutional affiliations.
supplementary information
supplementary information
rights and permissions
reprints and permissions
about this article
[image]
cite this article
rudin, c. stop explaining black box machine learning models for
high stakes decisions and use interpretable models instead. nat
mach intell 1, 206â€“215 (2019). https://doi.org/10.1038/
s42256-019-0048-x
download citation
â€¢  received: 30 december 2018
â€¢  accepted: 26 march 2019
â€¢  published: 13 may 2019
â€¢  version of record: 13 may 2019
â€¢  issue date: may 2019
â€¢  doi: https://doi.org/10.1038/s42256-019-0048-x
share this article
anyone you share the following link with will be able to read this
content:
get shareable link
sorry, a shareable link is not currently available for this article.
copy shareable link to clipboard
provided by the springer nature sharedit content-sharing initiative
this article is cited by
â€¢  rethinking sepsis prediction in the era of large
language models
â—‹  andrew wong
â—‹  karandeep singh
â—‹  shamim nemati
npj health systems (2026)
â€¢  digital twins for personalized treatment in uro-
oncology in the era of artificial intelligence
â—‹  magdalena gÃ¶rtz
â—‹  carlos brandl
â—‹  matthias weidemÃ¼ller
nature reviews urology (2026)
â€¢  interpretable deep learning reveals distinct
spectral and temporal drivers of perceived
musical emotion
â—‹  yiming gu
â—‹  chen shao
â—‹  yinghan fan
scientific reports (2026)
â€¢  interpretable multimodal zero shot ecg
diagnosis via structured clinical knowledge
alignment
â—‹  jialu tang
â—‹  hung manh pham
â—‹  aaqib saeed
npj cardiovascular health (2026)
â€¢  critical engagement: the value of transparency
of ai in healthcare
â—‹  james edgar lim
â—‹  owen schaefer
â—‹  julian savulescu
philosophy & technology (2026)
 access through your institution
buy or subscribe
associated content
special
one year anniversary collection
advertisement
[image]
explore content
â€¢  research articles
â€¢  reviews & analysis
â€¢  news & comment
â€¢  videos
â€¢  current issue
â€¢  collections
â€¢  follow us on twitter
â€¢  subscribe
â€¢  sign up for alerts
â€¢  rss feed
about the journal
â€¢  aims & scope
â€¢  journal information
â€¢  journal metrics
â€¢  about the editors
â€¢  our publishing models
â€¢  editorial values statement
â€¢  editorial policies
â€¢  content types
â€¢  contact
â€¢  research cross-journal editorial team
â€¢  reviews cross-journal editorial team
publish with us
â€¢  submission guidelines
â€¢  for reviewers
â€¢  language editing services
â€¢  open access funding
â€¢  submit manuscript
search
search articles by subject, keyword or author
show results from all journals this journal
search
advanced search
quick links
â€¢  explore articles by subject
â€¢  find a job
â€¢  guide to authors
â€¢  editorial policies
nature machine intelligence (nat mach intell)
issn 2522-5839 (online)
nature.com sitemap
about nature portfolio
â€¢  about us
â€¢  press releases
â€¢  press office
â€¢  contact us
discover content
â€¢  journals a-z
â€¢  articles by subject
â€¢  protocols.io
â€¢  nature index
publishing policies
â€¢  nature portfolio policies
â€¢  open access
author & researcher services
â€¢  reprints & permissions
â€¢  research data
â€¢  language editing
â€¢  scientific editing
â€¢  nature masterclasses
â€¢  research solutions
libraries & institutions
â€¢  librarian service & tools
â€¢  librarian portal
â€¢  open research
â€¢  recommend to library
advertising & partnerships
â€¢  advertising
â€¢  partnerships & services
â€¢  media kits
â€¢  branded content
professional development
â€¢  nature awards
â€¢  nature careers
â€¢  nature conferences
regional websites
â€¢  nature africa
â€¢  nature china
â€¢  nature india
â€¢  nature japan
â€¢  nature middle east
â€¢  privacy policy
â€¢  use of cookies
â€¢  your privacy choices/manage cookies
â€¢  legal notice
â€¢  accessibility statement
â€¢  terms & conditions
â€¢  your us state privacy rights
[image]
Â© 2026 springer nature limited
 close
[image]
sign up for the nature briefing: ai and robotics newsletter â€” what
matters in ai and robotics research, free to your inbox weekly.
email address
sign up
i agree my information will be processed in accordance with the
nature and springer nature limited privacy policy.
 close
get the most important science stories of the day, free in your
inbox. sign up for nature briefing: ai and robotics
[image]

method protects sensitive information in the data. properties such as reliability and
robustness ascertain whether algorithms reach certain levels of performance in the face of parameter
or input variation. causality implies that the predicted change in output due to a perturbation
will occur in the real system. usable methods provide information that assist users to accomplish
a taskâ€”e.g. a knob to tweak image lightingâ€”while trusted systems have the conï¬dence of human
usersâ€”e.g. aircraft collision avoidance systems. some areas, such as the fairness [hardt et al.,
1google scholar ï¬nds more than 20,000 publications related to interpretability in ml in the last ï¬ve years.
2merriam-webster dictionary, accessed 2017-02-07
2
2016] and privacy [toubiana et al., 2010, dwork et al., 2012, hardt and talwar, 2010] the research
communities have formalized their criteria, and these formalizations have allowed for a blossoming
of rigorous research in these ï¬elds (without the need for interpretability). however, in many cases,
formal deï¬nitions remain elusive. following the psychology literature, where keil et al. [2004] notes
â€œexplanations may highlight an incompleteness,â€ we argue that interpretability can assist in qual-
itatively ascertaining whether other desiderataâ€”such as fairness, privacy, reliability, robustness,
causality, usability and trustâ€”are met. for example, one can provide a feasible explanation that
fails to correspond to a causal structure, exposing a potential concern.
2
why interpretability? incompleteness
not all ml systems require interpretability.
ad servers, postal code sorting, air craft collision
avoidance systemsâ€”all compute their output without human intervention.
explanation is not
necessary either because (1) there are no signiï¬cant consequences for unacceptable