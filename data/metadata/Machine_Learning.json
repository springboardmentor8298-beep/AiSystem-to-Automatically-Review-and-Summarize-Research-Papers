[
  {
    "title": "Physics-informed machine learning",
    "year": 2021,
    "authors": [
      "G. Karniadakis",
      "I. Kevrekidis",
      "Lu Lu",
      "P. Perdikaris",
      "Sifan Wang",
      "Liu Yang"
    ],
    "abstract": null,
    "url": "https://www.semanticscholar.org/paper/53c9f3c34d8481adaf24df3b25581ccf1bc53f5c",
    "citations": 5224,
    "externalIds": {
      "MAG": "3163993681",
      "DOI": "10.1038/s42254-021-00314-5",
      "CorpusId": 236407461
    },
    "openAccessPdf": {
      "url": "https://www.osti.gov/biblio/2282016",
      "status": "GREEN",
      "license": "other-oa",
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s42254-021-00314-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s42254-021-00314-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    }
  },
  {
    "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
    "year": 2017,
    "authors": [
      "Han Xiao",
      "Kashif Rasul",
      "Roland Vollgraf"
    ],
    "abstract": "We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL",
    "url": "https://www.semanticscholar.org/paper/f9c602cc436a9ea2f9e7db48c77d924e09ce3c32",
    "citations": 10012,
    "externalIds": {
      "MAG": "2750384547",
      "DBLP": "journals/corr/abs-1708-07747",
      "ArXiv": "1708.07747",
      "CorpusId": 702279
    },
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1708.07747, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    }
  },
  {
    "title": "A Survey on Bias and Fairness in Machine Learning",
    "year": 2019,
    "authors": [
      "Ninareh Mehrabi",
      "Fred Morstatter",
      "N. Saxena",
      "Kristina Lerman",
      "A. Galstyan"
    ],
    "abstract": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",
    "url": "https://www.semanticscholar.org/paper/0090023afc66cd2741568599057f4e82b566137c",
    "citations": 5256,
    "externalIds": {
      "ArXiv": "1908.09635",
      "MAG": "2969896603",
      "DBLP": "journals/csur/MehrabiMSLG21",
      "DOI": "10.1145/3457607",
      "CorpusId": 201666566
    },
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1908.09635",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.09635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    }
  },
  {
    "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
    "year": 2018,
    "authors": [
      "C. Rudin"
    ],
    "abstract": null,
    "url": "https://www.semanticscholar.org/paper/bc00ff34ec7772080c7039b17f7069a2f7df0889",
    "citations": 7577,
    "externalIds": {
      "DBLP": "journals/natmi/Rudin19",
      "MAG": "2974440810",
      "DOI": "10.1038/s42256-019-0048-x",
      "CorpusId": 182656421,
      "PubMed": "35603010"
    },
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s42256-019-0048-x.pdf",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1038/s42256-019-0048-x?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1038/s42256-019-0048-x, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    }
  },
  {
    "title": "Towards A Rigorous Science of Interpretable Machine Learning",
    "year": 2017,
    "authors": [
      "F. Doshi-Velez",
      "Been Kim"
    ],
    "abstract": "As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",
    "url": "https://www.semanticscholar.org/paper/5c39e37022661f81f79e481240ed9b175dec6513",
    "citations": 4512,
    "externalIds": {
      "MAG": "2594475271",
      "ArXiv": "1702.08608",
      "CorpusId": 11319376
    },
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1702.08608, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    }
  }
]