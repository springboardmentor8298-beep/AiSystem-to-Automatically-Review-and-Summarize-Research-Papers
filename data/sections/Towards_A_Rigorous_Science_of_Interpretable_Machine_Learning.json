{
  "method": "method protects sensitive information in the data. properties such as reliability and\nrobustness ascertain whether algorithms reach certain levels of performance in the face of parameter\nor input variation. causality implies that the predicted change in output due to a perturbation\nwill occur in the real system. usable methods provide information that assist users to accomplish\na task\u2014e.g. a knob to tweak image lighting\u2014while trusted systems have the con\ufb01dence of human\nusers\u2014e.g. aircraft collision avoidance systems. some areas, such as the fairness [hardt et al.,\n1google scholar \ufb01nds more than 20,000 publications related to interpretability in ml in the last \ufb01ve years.\n2merriam-webster dictionary, accessed 2017-02-07\n2\n2016] and privacy [toubiana et al., 2010, dwork et al., 2012, hardt and talwar, 2010] the research\ncommunities have formalized their criteria, and these formalizations have allowed for a blossoming\nof rigorous research in these \ufb01elds (without the need for interpretability). however, in many cases,\nformal de\ufb01nitions remain elusive. following the psychology literature, where keil et al. [2004] notes\n\u201cexplanations may highlight an incompleteness,\u201d we argue that interpretability can assist in qual-\nitatively ascertaining whether other desiderata\u2014such as fairness, privacy, reliability, robustness,\ncausality, usability and trust\u2014are met. for example, one can provide a feasible explanation that\nfails to correspond to a causal structure, exposing a potential concern.\n2\nwhy interpretability? incompleteness\nnot all ml systems require interpretability.\nad servers, postal code sorting, air craft collision\navoidance systems\u2014all compute their output without human intervention.\nexplanation is not\nnecessary either because (1) there are no signi\ufb01cant consequences for unacceptable",
  "results": "results or (2)\nthe problem is su\ufb03ciently well-studied and validated in real applications that we trust the system\u2019s\ndecision, even if the system is not perfect.\nso when is explanation necessary and appropriate? we argue that the need for interpretability\nstems from an incompleteness in the problem formalization, creating a fundamental barrier to\noptimization and evaluation.\nnote that incompleteness is distinct from uncertainty: the fused\nestimate of a missile location may be uncertain, but such uncertainty can be rigorously quanti\ufb01ed\nand formally reasoned about.\nin machine learning terms, we distinguish between cases where\nunknowns result in quanti\ufb01ed variance\u2014e.g. trying to learn from small data set or with limited\nsensors\u2014and incompleteness that produces some kind of unquanti\ufb01ed bias\u2014e.g.\nthe e\ufb00ect of\nincluding domain knowledge in a model selection process. below are some illustrative scenarios:\n\u2022 scienti\ufb01c understanding: the human\u2019s goal is to gain knowledge. we do not have a complete\nway of stating what knowledge is; thus the best we can do is ask for explanations we can\nconvert into knowledge.\n\u2022 safety: for complex tasks, the end-to-end system is almost never completely testable; one\ncannot create a complete list of scenarios in which the system may fail. enumerating all\npossible outputs given all possible inputs be computationally or logistically infeasible, and we\nmay be unable to \ufb02ag all undesirable outputs.\n\u2022 ethics: the human may want to guard against certain kinds of discrimination, and their\nnotion of fairness may be too",
  "abstract": "abstract to be completely encoded into the system (e.g., one\nmight desire a \u2018fair\u2019 classi\ufb01er for loan approval). even if we can encode protections for speci\ufb01c\nprotected classes into the system, there might be biases that we did not consider a priori (e.g.,\none may not build gender-biased word embeddings on purpose, but it was a pattern in data\nthat became apparent only after the fact).\n\u2022 mismatched objectives: the agent\u2019s algorithm may be optimizing an incomplete objective\u2014\nthat is, a proxy function for the ultimate goal. for example, a clinical system may be opti-\nmized for cholesterol control, without considering the likelihood of adherence; an automotive\nengineer may be interested in engine data not to make predictions about engine failures but\nto more broadly build a better car.\n3\n\u2022 multi-objective trade-o\ufb00s: two well-de\ufb01ned desiderata in ml systems may compete with\neach other, such as privacy and prediction quality [hardt et al., 2016] or privacy and non-\ndiscrimination [strahilevitz, 2008]. even if each objectives are fully-speci\ufb01ed, the exact dy-\nnamics of the trade-o\ufb00may not be fully known, and the decision may have to be case-by-case.\nin the presence of an incompleteness, explanations are one of ways to ensure that e\ufb00ects of gaps in\nproblem formalization are visible to us.\n3\nhow? a taxonomy of interpretability evaluation\neven in standard ml settings, there exists a taxonomy of evaluation that is considered appropriate.\nin particular, the evaluation should match the claimed contribution. evaluation of applied work\nshould demonstrate success in the application: a game-playing agent might best a human player, a\nclassi\ufb01er may correctly identify star types relevant to astronomers. in contrast, core methods work\nshould demonstrate generalizability via careful evaluation on a variety of synthetic and standard\nbenchmarks.\nin this section we lay out an analogous taxonomy of evaluation approaches for interpretabil-\nity: application-grounded, human-grounded, and functionally-grounded. these range from task-\nrelevant to general, also acknowledge that while human evaluation is essential to assessing in-\nterpretability, human-subject evaluation is not an easy task. a human experiment needs to be\nwell-designed to minimize confounding factors, consumed time, and other resources. we discuss\nthe trade-o\ufb00s between each type of evaluation and when each would be appropriate.\n3.1\napplication-grounded evaluation: real humans, real tasks\napplication-grounded evaluation involves conducting human experiments within a real application.\nif the researcher has a concrete application in mind\u2014such as working with doctors on diagnosing\npatients with a particular disease\u2014the best way to show that the model works is to evaluate it\nwith respect to the task: doctors performing diagnoses. this reasoning aligns with the methods of\nevaluation common in the human-computer interaction and visualization communities, where there\nexists a strong ethos around making sure that the system delivers on its intended task [antunes\net al., 2012, lazar et al., 2010]. for example, a visualization for correcting segmentations from\nmicroscopy data would be evaluated via user studies on segmentation on the target image task\n[suissa-peleg et al., 2016]; a homework-hint system is evaluated on whether the student achieves\nbetter post-test performance [williams et al., 2016].\nspeci\ufb01cally, we evaluate the quality of an explanation in the context of its end-task, such as\nwhether it results in better identi\ufb01cation of errors, new facts, or less discrimination. examples of\nexperiments include:\n\u2022 domain expert experiment with the exact application task.\n\u2022 domain expert experiment with a simpler or partial task to shorten experiment time and\nincrease the pool of potentially-willing subjects.\nin both cases, an important baseline is how well human-produced explanations assist in other\nhumans trying to complete the task. to make high impact in real world applications, it is essential\nthat we as a community respect the time and e\ufb00ort involved to do such evaluations, and also demand\n4\nhigh standards of experimental design when such evaluations are performed. as hci community\nrecognizes [antunes et al., 2012], this is not an easy evaluation metric. nonetheless, it directly\ntests the objective that the system is built for, and thus performance with respect to that objective\ngives strong evidence of success.\n3.2\nhuman-grounded metrics: real humans, simpli\ufb01ed tasks\nhuman-grounded evaluation is about conducting simpler human-subject experiments that maintain\nthe essence of the target application. such an evaluation is appealing when experiments with the\ntarget community is challenging. these evaluations can be completed with lay humans, allowing\nfor both a bigger subject pool and less expenses, since we do not have to compensate highly trained\ndomain experts. human-grounded evaluation is most appropriate when one wishes to test more\ngeneral notions of the quality of an explanation. for example, to study what kinds of explanations\nare best understood under severe time constraints, one might create abstract tasks in which other\nfactors\u2014such as the overall task complexity\u2014can be controlled [kim et al., 2013, lakkaraju et al.,\n2016]\nthe key question, of course, is how we can evaluate the quality of an explanation without a\nspeci\ufb01c end-goal (such as identifying errors in a safety-oriented task or identifying relevant patterns\nin a science-oriented task). ideally, our evaluation approach will depend only on the quality of the\nexplanation, regardless of whether the explanation is the model itself or a post-hoc interpretation\nof a black-box model, and regardless of the correctness of the associated prediction. examples of\npotential experiments include:\n\u2022 binary forced choice: humans are presented with pairs of explanations, and must choose the\none that they \ufb01nd of higher quality (basic face-validity test made quantitative).\n\u2022 forward simulation/prediction: humans are presented with an explanation and an input, and\nmust correctly simulate the model\u2019s output (regardless of the true output).\n\u2022 counterfactual simulation: humans are presented with an explanation, an input, and an\noutput, and are asked what must be changed to change the method\u2019s prediction to a desired\noutput (and related variants).\nhere is a concrete example. the common intrusion-detection test [chang et al., 2009] in topic\nmodels is a form of the forward simulation/prediction task: we ask the human to \ufb01nd the di\ufb00erence\nbetween the model\u2019s true output and some corrupted output as a way to determine whether the\nhuman has correctly understood what the model\u2019s true output is.\n3.3\nfunctionally-grounded evaluation: no humans, proxy tasks\nfunctionally-grounded evaluation requires no human experiments; instead, it uses some formal\nde\ufb01nition of interpretability as a proxy for explanation quality. such experiments are appealing\nbecause even general human-subject experiments require time and costs both to perform and to\nget necessary approvals (e.g., irbs), which may be beyond the resources of a machine learning\nresearcher. functionally-grounded evaluations are most appropriate once we have a class of models\nor regularizers that have already been validated, e.g. via human-grounded experiments. they may\nalso be appropriate when a method is not yet mature or when human subject experiments are\nunethical.\n5\nthe challenge, of course, is to determine what proxies to use.\nfor example, decision trees\nhave been considered interpretable in many situations [freitas, 2014]. in section 4, we describe\nopen problems in determining what proxies are reasonable. once a proxy has been formalized,\nthe challenge is squarely an optimization problem, as the model class or regularizer is likely to be\ndiscrete, non-convex and often non-di\ufb00erentiable. examples of experiments include\n\u2022 show the improvement of prediction performance of a model that is already proven to be\ninterpretable (assumes that someone has run human experiments to show that the model\nclass is interpretable).\n\u2022 show that one\u2019s method performs better with respect to certain regularizers\u2014for example, is\nmore sparse\u2014compared to other baselines (assumes someone has run human experiments to\nshow that the regularizer is appropriate).\n4\nopen problems in the science of interpretability, theory and\npractice\nit is essential that the three types of evaluation in the previous section inform each other: the\nfactors that capture the essential needs of real world tasks should inform what kinds of simpli\ufb01ed\ntasks we perform, and the performance of our methods with respect to functional proxies should\nre\ufb02ect their performance in real-world settings. in this section, we describe some important open\nproblems for creating these links between the three types of evaluations:\n1. what proxies are best for what real-world applications? (functionally to application-grounded)\n2. what are the important factors to consider when designing simpler tasks that maintain the\nessence of the real end-task? (human to application-grounded)\n3. what are the important factors to consider when characterizing proxies for explanation qual-\nity? (human to functionally-grounded)\nbelow, we describe a path to answering each of these questions.\n4.1\ndata-driven approach to discover factors of interpretability\nimagine a matrix where rows are speci\ufb01c real-world tasks, columns are speci\ufb01c methods, and the\nentries are the performance of the method on the end-task. for example, one could represent how\nwell a decision tree of depth less than 4 worked in assisting doctors in identifying pneumonia patients\nunder age 30 in us. once constructed, methods in machine learning could be used to identify latent\ndimensions that represent factors that are important to interpretability. this approach is similar to\ne\ufb00orts to characterize classi\ufb01cation [ho and basu, 2002] and clustering problems [garg and kalai,\n2016]. for example, one might perform matrix factorization to embed both tasks and methods\nrespectively in low-dimensional spaces (which we can then seek to interpret), as shown in figure 2.\nthese embeddings could help predict what methods would be most promising for a new problem,\nsimilarly to collaborative \ufb01ltering.\nthe challenge, of course, is in creating this matrix. for example, one could imagine creating a\nrepository of clinical cases in which the ml system has access to the patient\u2019s record but not certain\n6\nfigure 2: an example of data-driven approach to discover factors in interpretability\ncurrent features that are only accessible to the clinician, or a repository of discrimination-in-loan\ncases where the ml system must provide outputs that assist a lawyer in their decision. ideally\nthese would be linked to domain experts who have agreed to be employed to evaluate methods when\napplied to their domain of expertise. just as there are now large open repositories for problems\nin classi\ufb01cation, regression, and reinforcement learning [blake and merz, 1998, brockman et al.,\n2016, vanschoren et al., 2014], we advocate for the creation of repositories that contain problems\ncorresponding to real-world tasks in which human-input is required. creating such repositories will\nbe more challenging than creating collections of standard machine learning datasets because they\nmust include a system for human assessment, but with the availablity of crowdsourcing tools these\ntechnical challenges can be surmounted.\nin practice, constructing such a matrix will be expensive since each cell must be evaluated in\nthe context of a real application, and interpreting the latent dimensions will be an iterative e\ufb00ort\nof hypothesizing why certain tasks or methods share dimensions and then checking whether our\nhypotheses are true. in the next two open problems, we lay out some hypotheses about what latent\ndimensions may correspond to; these hypotheses can be tested via much less expensive human-\ngrounded evaluations on simulated tasks.\n4.2\nhypothesis: task-related latent dimensions of interpretability\ndisparate-seeming applications may share common categories: an application involving preventing\nmedical error at the bedside and an application involving support for identifying inappropriate\nlanguage on social media might be similar in that they involve making a decision about a speci\ufb01c\ncase\u2014a patient, a post\u2014in a relatively short period of time. however, when it comes to time\nconstraints, the needs in those scenarios might be di\ufb00erent from an application involving the un-\nderstanding of the main characteristics of a large omics data set, where the goal\u2014science\u2014is much\nmore abstract and the scientist may have hours or days to inspect the model outputs.\nbelow, we list a (non-exhaustive!) set of hypotheses about what might make tasks similar in\ntheir explanation needs:\n\u2022 global vs. local. global interpretability implies knowing what patterns are present in general\n(such as key features governing galaxy formation), while local interpretability implies knowing\nthe reasons for a speci\ufb01c decision (such as why a particular loan application was rejected).\nthe former may be important for when scienti\ufb01c understanding or bias detection is the goal;\nthe latter when one needs a justi\ufb01cation for a speci\ufb01c decision.\n\u2022 area, severity of incompleteness. what part of the problem formulation is incomplete, and\nhow incomplete is it? we hypothesize that the types of explanations needed may vary de-\npending on whether the source of concern is due to incompletely speci\ufb01ed inputs, constraints,\n7\ndomains, internal model structure, costs, or even in the need to understand the training al-\ngorithm. the severity of the incompleteness may also a\ufb00ect explanation needs. for example,\none can imagine a spectrum of questions about the safety of self-driving cars. on one end,\none may have general curiosity about how autonomous cars make decisions. at the other, one\nmay wish to check a speci\ufb01c list of scenarios (e.g., sets of sensor inputs that causes the car to\ndrive o\ufb00of the road by 10cm). in between, one might want to check a general property\u2014safe\nurban driving\u2014without an exhaustive list of scenarios and safety criteria.\n\u2022 time constraints. how long can the user a\ufb00ord to spend to understand the explanation? a\ndecision that needs to be made at the bedside or during the operation of a plant must be\nunderstood quickly, while in scienti\ufb01c or anti-discrimination applications, the end-user may\nbe willing to spend hours trying to fully understand an explanation.\n\u2022 nature of user expertise. how experienced is the user in the task? the user\u2019s experience will\na\ufb00ect what kind of cognitive chunks they have, that is, how they organize individual elements\nof information into collections [neath and surprenant, 2003]. for example, a clinician may\nhave a notion that autism and adhd are both developmental diseases. the nature of the\nuser\u2019s expertise will also in\ufb02uence what level of sophistication they expect in their explana-\ntions. for example, domain experts may expect or prefer a somewhat larger and sophisticated\nmodel\u2014which con\ufb01rms facts they know\u2014over a smaller, more opaque one. these preferences\nmay be quite di\ufb00erent from hospital ethicist who may be more narrowly concerned about\nwhether decisions are being made in an ethical manner. more broadly, decison-makers, sci-\nentists, compliance and safety engineers, data scientists, and machine learning researchers all\ncome with di\ufb00erent background knowledge and communication styles.\neach of these factors can be isolated in human-grounded experiments in simulated tasks to deter-\nmine which methods work best when they are present.\n4.3\nhypothesis: method-related latent dimensions of interpretability\njust as disparate applications may share common categories, disparate methods may share common\nqualities that correlate to their utility as explanation. as before, we provide a (non-exhaustive!)\nset of factors that may correspond to di\ufb00erent explanation needs: here, we de\ufb01ne cognitive chunks\nto be the basic units of explanation.\n\u2022 form of cognitive chunks. what are the basic units of the explanation? are they raw features?\nderived features that have some semantic meaning to the expert (e.g. \u201cneurological disorder\u201d\nfor a collection of diseases or \u201cchair\u201d for a collection of pixels)? prototypes?\n\u2022 number of cognitive chunks. how many cognitive chunks does the explanation contain? how\ndoes the quantity interact with the type: for example, a prototype can contain a lot more\ninformation than a feature; can we handle them in similar quantities?\n\u2022 level of compositionality. are the cognitive chunks organized in a structured way? rules,\nhierarchies, and other abstractions can limit what a human needs to process at one time. for\nexample, part of an explanation may involve de\ufb01ning a new unit (a chunk) that is a function\nof raw units, and then providing an explanation in terms of that new unit.\n8\n\u2022 monotonicity and other interactions between cognitive chunks. does it matter if the cognitive\nchunks are combined in linear or nonlinear ways? in monotone ways [gupta et al., 2016]?\nare some functions more natural to humans than others [wilson et al., 2015, schulz et al.,\n2016]?\n\u2022 uncertainty and stochasticity. how well do people understand uncertainty measures? to\nwhat extent is stochasticity understood by humans?\n5",
  "conclusion": "conclusion: recommendations for researchers\nin this work, we have laid the groundwork for a process to rigorously de\ufb01ne and evaluate inter-\npretability. there are many open questions in creating the formal links between applications, the\nscience of human understanding, and more traditional machine learning regularizers. in the mean\ntime, we encourage the community to consider some general principles.\nthe claim of the research should match the type of the evaluation. just as one would be critical\nof a reliability-oriented paper that only cites accuracy statistics, the choice of evaluation should\nmatch the speci\ufb01city of the claim being made.\na contribution that is focused on a particular\napplication should be expected to be evaluated in the context of that application (application-\ngrounded evaluation), or on a human experiment with a closely-related task (human-grounded\nevaluation). a contribution that is focused on better optimizing a model class for some de\ufb01nition\nof interpretability should be expected to be evaluated with functionally-grounded metrics. as a\ncommunity, we must be careful in the work on interpretability, both recognizing the need for and\nthe costs of human-subject experiments.\nwe should categorize our applications and methods with a common taxonomy. in section 4,\nwe hypothesized factors that may be the latent dimensions of interpretability. creating a shared\nlanguage around such factors is essential not only to evaluation, but also for the citation and\ncomparison of related work.\nfor example, work on creating a safe healthcare agent might be\nframed as focused on the need for explanation due to unknown inputs at the local scale, evaluated\nat the level of an application. in contrast, work on learning sparse linear models might also be\nframed as focused on the need for explanation due to unknown inputs, but this time evaluated at\nglobal scale. as we share each of our work with the community, we can do each other a service by\ndescribing factors such as\n1. how is the problem formulation incomplete? (section 2)\n2. at what level is the evaluation being performed? (application, general user study, proxy;\nsection 3)\n3. what are task-related relevant factors? (e.g. global vs. local, severity of incompleteness, level\nof user expertise, time constraints; section 4.2)\n4. what are method-related relevant factors being explored? (e.g. form of cognitive chunks,\nnumber of cognitive chunks, compositionality, monotonicity, uncertainty; section 4.3)\nand of course, adding and re\ufb01ning these factors as our taxonomies evolve. these considerations\nshould move us away from vague claims about the interpretability of a particular model and toward\nclassifying applications by a common set of terms.\n9\nacknowledgments\nthis piece would not have been possible without the dozens of deep conver-\nsations about interpretability with machine learning researchers and domain experts. our friends\nand colleagues, we appreciate your support. we want to particularity thank ian goodfellow, kush\nvarshney, hanna wallach, solon barocas, stefan rping and jesse johnson for their feedback.\nreferences\ndario amodei, chris olah, jacob steinhardt, paul christiano, john schulman, and dan man\u00b4e.\nconcrete problems in ai safety. arxiv preprint arxiv:1606.06565, 2016.\npedro antunes, valeria herskovic, sergio f ochoa, and jose a pino. structuring dimensions for\ncollaborative systems evaluation. acm computing surveys, 2012.\nwilliam bechtel and adele abrahamsen. explanation: a mechanist alternative. studies in history\nand philosophy of science part c: studies in history and philosophy of biological and biomedical\nsciences, 2005.\ncatherine blake and christopher j merz. {uci} repository of machine learning databases. 1998.\nnick bostrom and eliezer yudkowsky. the ethics of arti\ufb01cial intelligence. the cambridge handbook\nof arti\ufb01cial intelligence, 2014.\ngreg brockman, vicki cheung, ludwig pettersson, jonas schneider, john schulman, jie tang,\nand wojciech zaremba. openai gym. arxiv preprint arxiv:1606.01540, 2016.\ncristian bucilu, rich caruana, and alexandru niculescu-mizil. model compression. in proceedings\nof the 12th acm sigkdd international conference on knowledge discovery and data mining.\nacm, 2006.\nsamuel carton, jennifer helsby, kenneth joseph, ayesha mahmud, youngsoo park, joe walsh,\ncrystal cody, cpt estella patterson, lauren haynes, and rayid ghani.\nidentifying police\no\ufb03cers at risk of adverse events.\nin acm sigkdd international conference on knowledge\ndiscovery and data mining. acm, 2016.\njonathan chang, jordan l boyd-graber, sean gerrish, chong wang, and david m blei. reading\ntea leaves: how humans interpret topic models. in nips, 2009.\nnick chater and mike oaksford. speculations on human causal learning and reasoning. information\nsampling and adaptive cognition, 2006.\nfinale doshi-velez, yaorong ge, and isaac kohane.\ncomorbidity clusters in autism spectrum\ndisorders: an electronic health record time-series analysis. pediatrics, 133(1):e54\u2013e63, 2014.\nfinale doshi-velez, byron wallace, and ryan adams.\ngraph-sparse lda: a topic model with\nstructured sparsity. association for the advancement of arti\ufb01cial intelligence, 2015.\ncynthia dwork, moritz hardt, toniann pitassi, omer reingold, and richard zemel.\nfairness\nthrough awareness. in innovations in theoretical computer science conference. acm, 2012.\n10\nalex freitas. comprehensible classi\ufb01cation models: a position paper. acm sigkdd explorations,\n2014.\nvikas k garg and adam tauman kalai. meta-unsupervised-learning: a supervised approach to\nunsupervised learning. arxiv preprint arxiv:1612.09030, 2016.\nstuart glennan. rethinking mechanistic explanation. philosophy of science, 2002.\nbryce goodman and seth flaxman. european union regulations on algorithmic decision-making\nand a\u201d right to explanation\u201d. arxiv preprint arxiv:1606.08813, 2016.\nmaya gupta, andrew cotter, jan pfeifer, konstantin voevodski, kevin canini, alexander\nmangylov, wojciech moczydlowski, and alexander van esbroeck.\nmonotonic calibrated in-\nterpolated look-up tables. journal of machine learning research, 2016.\nsean hamill.\ncmu computer won poker battle over humans by statistically signi\ufb01cant mar-\ngin.\nhttp://www.post-gazette.com/business/tech-news/2017/01/31/cmu-computer-\nwon-poker-battle-over-humans-by-statistically-significant-margin/stories/\n201701310250, 2017. accessed: 2017-02-07.\nmoritz hardt and kunal talwar. on the geometry of di\ufb00erential privacy. in acm symposium on\ntheory of computing. acm, 2010.\nmoritz hardt, eric price, and nati srebro. equality of opportunity in supervised learning. in\nadvances in neural information processing systems, 2016.\ncarl hempel and paul oppenheim. studies in the logic of explanation. philosophy of science, 1948.\ntin kam ho and mitra basu. complexity measures of supervised classi\ufb01cation problems. ieee\ntransactions on pattern analysis and machine intelligence, 2002.\nfrank keil. explanation and understanding. annu. rev. psychol., 2006.\nfrank keil, leonid rozenblit, and candice mills. what lies beneath? understanding the limits of\nunderstanding. thinking and seeing: visual metacognition in adults and children, 2004.\nbeen kim, caleb chacha, and julie shah. inferring robot task plans from human team meetings:\na generative modeling approach with logic-based prior.\nassociation for the advancement of\narti\ufb01cial intelligence, 2013.\nbeen kim, elena glassman, brittney johnson, and julie shah. ibcm: interactive bayesian case\nmodel empowering humans via intuitive interaction. 2015a.\nbeen kim, julie shah, and finale doshi-velez. mind the gap: a generative approach to inter-\npretable feature selection and extraction. in advances in neural information processing systems,\n2015b.\nhimabindu lakkaraju, stephen h bach, and jure leskovec. interpretable decision sets: a joint\nframework for description and prediction. in proceedings of the 22nd acm sigkdd interna-\ntional conference on knowledge discovery and data mining, pages 1675\u20131684. acm, 2016.\n11\njonathan lazar, jinjuan heidi feng, and harry hochheiser. research methods in human-computer\ninteraction. john wiley & sons, 2010.\ntao lei, regina barzilay, and tommi jaakkola. rationalizing neural predictions. arxiv preprint\narxiv:1606.04155, 2016.\ntania lombrozo. the structure and function of explanations. trends in cognitive sciences, 10(10):\n464\u2013470, 2006.\nyin lou, rich caruana, and johannes gehrke. intelligible models for classi\ufb01cation and regression.\nin acm sigkdd international conference on knowledge discovery and data mining. acm, 2012.\nvolodymyr mnih, koray kavukcuoglu, david silver, alex graves, ioannis antonoglou, daan wier-\nstra, and martin riedmiller. playing atari with deep reinforcement learning. arxiv preprint\narxiv:1312.5602, 2013.\nian neath and aimee surprenant. human memory. 2003.\nclemens otte. safe and interpretable machine learning: a methodological review. in computational\nintelligence in intelligent data analysis. springer, 2013.\nparliament and council of the european union. general data protection regulation. 2016.\nmarco tulio ribeiro, sameer singh, and carlos guestrin. \u201cwhy should i trust you?\u201d: explaining\nthe predictions of any classi\ufb01er. arxiv preprint arxiv:1602.04938, 2016.\nsalvatore ruggieri, dino pedreschi, and franco turini. data mining for discrimination discovery.\nacm transactions on knowledge discovery from data, 2010.\neric schulz, joshua tenenbaum, david duvenaud, maarten speekenbrink, and samuel gershman.\ncompositional inductive biases in function learning. biorxiv, 2016.\nd sculley, gary holt, daniel golovin, eugene davydov, todd phillips, dietmar ebner, vinay\nchaudhary, michael young, jean-fran\u00b8cois crespo, and dan dennison. hidden technical debt in\nmachine learning systems. in advances in neural information processing systems, 2015.\ndavid silver, aja huang, chris j maddison, arthur guez, laurent sifre, george van den driessche,\njulian schrittwieser, ioannis antonoglou, veda panneershelvam, marc lanctot, et al. mastering\nthe game of go with deep neural networks and tree search. nature, 2016.\nlior jacob strahilevitz.\nprivacy versus antidiscrimination.\nuniversity of chicago law school\nworking paper, 2008.\nadi suissa-peleg, daniel haehn, seymour knowles-barley, verena kaynig, thouis r jones, alyssa\nwilson, richard schalek, je\ufb00ery w lichtman, and hanspeter p\ufb01ster. automatic neural recon-\nstruction from petavoxel of electron microscopy data. microscopy and microanalysis, 2016.\nvincent toubiana, arvind narayanan, dan boneh, helen nissenbaum, and solon barocas. adnos-\ntic: privacy preserving targeted advertising. 2010.\njoaquin vanschoren, jan n van rijn, bernd bischl, and luis torgo. openml: networked science\nin machine learning. acm sigkdd explorations newsletter, 15(2):49\u201360, 2014.\n12\nkush varshney and homa alemzadeh. on the safety of machine learning: cyber-physical systems,\ndecision sciences, and data products. corr, 2016.\nfulton wang and cynthia rudin. falling rule lists. in aistats, 2015.\ntong wang, cynthia rudin, finale doshi-velez, yimin liu, erica klamp\ufb02, and perry macneille.\nbayesian rule sets for interpretable classi\ufb01cation. in international conference on data mining,\n2017.\njoseph jay williams, juho kim, anna ra\ufb00erty, samuel maldonado, krzysztof z gajos, walter s\nlasecki, and neil he\ufb00ernan. axis: generating explanations at scale with learnersourcing and\nmachine learning. in acm conference on learning@ scale. acm, 2016.\nandrew wilson, christoph dann, chris lucas, and eric xing. the human kernel. in advances in\nneural information processing systems, 2015.\n13"
}