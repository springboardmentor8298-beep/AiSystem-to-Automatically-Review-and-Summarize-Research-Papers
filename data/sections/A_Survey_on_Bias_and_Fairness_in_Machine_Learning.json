{
  "introduction": "introduction\nmachine learning algorithms have penetrated every aspect of our lives. algorithms make movie\nrecommendations, suggest products to buy, and who to date. they are increasingly used in high-stakes\nscenarios such as loans [113] and hiring decisions [19, 39]. there are clear benefits to algorithmic\ndecision-making; unlike people, machines do not become tired or bored [45, 119], and can take into\naccount orders of magnitude more factors than people can. however, like people, algorithms are\nvulnerable to biases that render their decisions \u201cunfair\u201d [6, 121]. in the context of decision-making,\nfairness is the absence of any prejudice or favoritism toward an individual or group based on\ntheir inherent or acquired characteristics. thus, an unfair algorithm is one whose decisions are\nskewed toward a particular group of people. a canonical example comes from a tool used by courts\nin the united states to make pretrial detention and release decisions. the software, correctional\noffender management profiling for alternative sanctions (compas), measures the risk of a person\nto recommit another crime. judges use compas to decide whether to release an offender, or to keep\nhim or her in prison. an investigation into the software found a bias against african-americans:1\n1https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\nauthors\u2019 address: usc, information sciences institute 4676 admiralty way, suite 1001 marina del rey, ca 90292\nthis material is based upon work supported by the defense advanced research projects agency (darpa) under agreement\nno. hr0011890019.\narxiv:1908.09635v3  [cs.lg]  25 jan 2022\n2\nmehrabi et al.\ncompas is more likely to have higher false positive rates for african-american offenders than\ncaucasian offenders in falsely predicting them to be at a higher risk of recommitting a crime or\nrecidivism. similar findings have been made in other areas, such as an ai system that judges beauty\npageant winners but was biased against darker-skinned contestants,2 or facial recognition software in\ndigital cameras that overpredicts asians as blinking.3 these biased predictions stem from the hidden\nor neglected biases in data or algorithms.\nin this survey we identify two potential sources of unfairness in machine learning outcomes\u2014\nthose that arise from biases in the data and those that arise from the algorithms. we review research\ninvestigating how biases in data skew what is learned by machine learning algorithms, and nuances\nin the way the algorithms themselves work to prevent them from making fair decisions\u2014even when\nthe data is unbiased. furthermore, we observe that biased algorithmic outcomes might impact user\nexperience, thus generating a feedback loop between data, algorithms and users that can perpetuate\nand even amplify existing sources of bias.\nwe begin the review with several highly visible real-world cases of where unfair machine learning\nalgorithms have led to suboptimal and discriminatory outcomes in section 2. in section 3, we\ndescribe the different types and sources of biases that occur within the data-algorithms-users loop\nmentioned above. next, in section 4, we present the different ways that the concept of fairness has\nbeen operationalized and studied in the literature. we discuss the ways in which these two concepts\nare coupled. last, we will focus on different families of machine learning approaches, how fairness\nmanifests differently in each one, and the current state-of-the-art for tackling them in section 5,\nfollowed by potential areas of future work in each of the domains in section 6.\n2\nreal-world examples of algorithmic unfairness\nwith the popularity of ai and machine learning over the past decades, and their prolific spread in\ndifferent applications, safety and fairness constraints have become a significant issue for researchers\nand engineers. machine learning is used in courts to assess the probability that a defendant recommits\na crime. it is used in different medical fields, in childhood welfare systems [35], and autonomous\nvehicles. all of these applications have a direct effect in our lives and can harm our society if not\ndesigned and engineered correctly, that is with considerations to fairness. [123] has a list of the\napplications and the ways these ai systems affect our daily lives with their inherent biases, such as\nthe existence of bias in ai chatbots, employment matching, flight routing, and automated legal aid for\nimmigration algorithms, and search and advertising placement algorithms. [67] discusses examples\nof how bias in the real world can creep into ai and robotic systems, such as bias in face recognition\napplications, voice recognition, and search engines. therefore, it is important for researchers and\nengineers to be concerned about the downstream applications and their potential harmful effects\nwhen modeling an algorithm or a system.\n2.1\nsystems that demonstrate discrimination\ncompas is an exemplar of a discriminatory system. in addition to this, discriminatory behavior was\nalso evident in an algorithm that would deliver advertisements promoting jobs in science, technology,\nengineering, and math (stem) fields [88]. this advertisement was designed to deliver advertise-\nments in a gender-neutral way. however, less women compared to men saw the advertisement due to\ngender-imbalance which would result in younger women being considered as a valuable subgroup\nand more expensive to show advertisements to. this optimization algorithm would deliver ads in\na discriminatory way although its original and pure intention was to be gender-neutral. bias in\n2https://www.theguardian.com/technology/2016/sep/08/artificial-intelligence-beauty-contest-doesnt-like-black-people\n3http://content.time.com/time/business/article/0,8599,1954643,00.html\na survey on bias and fairness in machine learning\n3\nfacial recognition systems [128] and recommender systems [140] have also been largely studied and\nevaluated and in many cases shown to be discriminative towards certain populations and subgroups.\nin order to be able to address the bias issue in these applications, it is important for us to know where\nthese biases are coming from and what we can do to prevent them.\nwe have enumerated the bias in compas, which is a widely used commercial risk assessment\nsoftware. in addition to its bias, it also contains performance issues when compared to humans. when\ncompared to non-expert human judgment in a study, it was discovered to be not any better than a\nnormal human [46]. it is also interesting to note that although compas uses 137 features, only 7 of\nthose were presented to the people in the study. [46] further argues that compas is not any better\nthan a simple logistic regression model when making decisions. we should think responsibly, and\nrecognize that the application of these tools, and their subsequent decisions affect peoples\u2019 lives;\ntherefore, considering fairness constraints is a crucial task while designing and engineering these\ntypes of sensitive tools. in another similar study, while investigating sources of group unfairness\n(unfairness across different groups is defined later), the authors in [145] compared savry, a tool\nused in risk assessment frameworks that includes human intervention in its process, with automatic\nmachine learning methods in order to see which one is more accurate and more fair. conducting\nthese types of studies should be done more frequently, but prior to releasing the tools in order to\navoid doing harm.\n2.2\nassessment tools\nan interesting direction that researchers have taken is introducing tools that can assess the amount\nof fairness in a tool or system. for example, aequitas [136] is a toolkit that lets users to test\nmodels with regards to several bias and fairness metrics for different population subgroups. aequitas\nproduces reports from the obtained data that helps data scientists, machine learning researchers, and\npolicymakers to make conscious decisions and avoid harm and damage toward certain populations.\nai fairness 360 (aif360) is another toolkit developed by ibm in order to help moving fairness\nresearch algorithms into an industrial setting and to create a benchmark for fairness algorithms to\nget evaluated and an environment for fairness researchers to share their ideas [11]. these types of\ntoolkits can be helpful for learners, researchers, and people working in the industry to move towards\ndeveloping fair machine learning application away from discriminatory behavior.\n3\nbias in data, algorithms, and user experiences\nmost ai systems and algorithms are data driven and require data upon which to be trained. thus,\ndata is tightly coupled to the functionality of these algorithms and systems. in the cases where the\nunderlying training data contains biases, the algorithms trained on them will learn these biases and\nreflect them into their predictions. as a result, existing biases in data can affect the algorithms using\nthe data, producing biased outcomes. algorithms can even amplify and perpetuate existing biases\nin the data. in addition, algorithms themselves can display biased behavior due to certain design\nchoices, even if the data itself is not biased. the outcomes of these biased algorithms can then be fed\ninto real-world systems and affect users\u2019 decisions, which will result in more biased data for training\nfuture algorithms. for example, imagine a web search engine that puts specific",
  "results": "results at the top of\nits list. users tend to interact most with the top results and pay little attention to those further down\nthe list [92]. the interactions of users with items will then be collected by the web search engine,\nand the data will be used to make future decisions on how information should be presented based\non popularity and user interest. as a result, results at the top will become more and more popular,\nnot because of the nature of the result but due to the biased interaction and placement of results by\nthese algorithms [92]. the loop capturing this feedback between biases in data, algorithms, and user\n4\nmehrabi et al.\ndata\nalgorithm\nuser \ninteraction\nbehavioral bias\ncontent production bias\nranking bias\nemergent bias\naggregation bias\nlongitudinal data fallacy\nfig. 1. examples of bias definitions placed in the data, algorithm, and user interaction feedback loop.\ninteraction is illustrated in figure 1. we use this loop to categorize definitions of bias in the section\nbelow.\n3.1\ntypes of bias\nbias can exist in many shapes and forms, some of which can lead to unfairness in different down-\nstream learning tasks. in [144], authors talk about sources of bias in machine learning with their\ncategorizations and descriptions in order to motivate future solutions to each of the sources of bias\nintroduced in the paper. in [120], the authors prepare a complete list of different types of biases with\ntheir corresponding definitions that exist in different cycles from data origins to its collection and its\nprocessing. here we will reiterate the most important sources of bias introduced in these two papers\nand also add in some work from other existing research papers. additionally, we will introduce a\ndifferent categorization of these definitions in the paper according to the data, algorithm, and user\ninteraction loop.\n3.1.1\ndata to algorithm. in this section we talk about biases in data, which, when used by ml\ntraining algorithms, might result in biased algorithmic outcomes.\n(1) measurement bias. measurement, or reporting, bias arises from how we choose, utilize,\nand measure particular features [144]. an example of this type of bias was observed in the\nrecidivism risk prediction tool compas, where prior arrests and friend/family arrests were\nused as proxy variables to measure level of \u201criskiness\u201d or \u201ccrime\u201d\u2014-which on its own can\nbe viewed as mismeasured proxies. this is partly due to the fact that minority communities\nare controlled and policed more frequently, so they have higher arrest rates. however, one\nshould not conclude that because people coming from minority groups have higher arrest rates\ntherefore they are more dangerous as there is a difference in how these groups are assessed\nand controlled [144].\n(2) omitted variable bias. omitted variable bias4 occurs when one or more important variables\nare left out of the model [38, 114, 131]. an example for this case would be when someone\na survey on bias and fairness in machine learning\n5\ndesigns a model to predict, with relatively high accuracy, the annual percentage rate at which\ncustomers will stop subscribing to a service, but soon observes that the majority of users are\ncanceling their subscription without receiving any warning from the designed model. now\nimagine that the reason for canceling the subscriptions is appearance of a new strong competitor\nin the market which offers the same solution, but for half the price. the appearance of the\ncompetitor was something that the model was not ready for; therefore, it is considered to be an\nomitted variable.\n(3) representation bias. representation bias arises from how we sample from a population\nduring data collection process [144]. non-representative samples lack the diversity of the\npopulation, with missing subgroups and other anomalies. lack of geographical diversity in\ndatasets like imagenet (as shown in figures 3 and 4) results in demonstrable bias towards\nwestern cultures.\n(4) aggregation bias. aggregation bias (or ecological fallacy) arises when false conclusions are\ndrawn about individuals from observing the entire population. an example of this type of\nbias can be seen in clinical aid tools. consider diabetes patients who have apparent morbidity\ndifferences across ethnicities and genders. specifically, hba1c levels, that are widely used\nto diagnose and monitor diabetes, differ in complex ways across genders and ethnicities.\ntherefore, a model that ignores individual differences will likely not be well-suited for all\nethnic and gender groups in the population [144]. this is true even when they are represented\nequally in the training data. any general assumptions about subgroups within the population\ncan result in aggregation bias.\n0\n50\n100\n150\n200\n250\n300\nx1\n0\n50\n100\n150\n200\n250\n300\ny\nmultivariate linear regression\nclusterwise linear regression\ncluster regression\ndata\n0\n50\n100\n150\n200\n250\n300\nx1\n0\n50\n100\n150\n200\n250\n300\ny\nmultivariate linear regression\nclusterwise linear regression\ncluster regression\ndata\nfig. 2. illustration of biases in data. the red line shows the regression (mlr) for the entire population,\nwhile dashed green lines are regressions for each subgroup, and the solid green line is the unbiased\nregression. (a) when all subgroups are of equal size, then mlr shows a positive relationship between\nthe outcome and the independent variable. (b) regression shows almost no relationship in less\nbalanced data. the relationships between variables within each subgroup, however, remain the same.\n(credit: nazanin alipourfard)\n(a) simpson\u2019s paradox. simpson\u2019s paradox is a type of aggregation bias that arises in the\nanalysis of heterogeneous data [18]. the paradox arises when an association observed\nin aggregated data disappears or reverses when the same data is disaggregated into its\nunderlying subgroups (fig. 2(a)). one of the better-known examples of the type of paradox\narose during the gender bias lawsuit in university admissions against uc berkeley [16]. after\nanalyzing graduate school admissions data, it seemed like there was bias toward women,\na smaller fraction of whom were being admitted to graduate programs compared to their\nmale counterparts. however, when admissions data was separated and analyzed over the\ndepartments, women applicants had equality and in some cases even a small advantage\n6\nmehrabi et al.\nfig. 3. fraction of each country, represented by their two-letter iso codes, in open images and\nimagenet image datasets. in both datasets, us and great britain represent the top locations, from\n[142] \u00a9 shreya shankar.\nfig. 4. geographic distribution of countries in the open images data set. in their sample, almost one\nthird of the data was us-based, and 60% of the data was from the six most represented countries\nacross north america and europe, from [142] \u00a9 shreya shankar.\nover men. the paradox happened as women tended to apply to departments with lower\nadmission rates for both genders. simpson\u2019s paradox has been observed in a variety of\ndomains, including biology [37], psychology [81], astronomy [109], and computational\nsocial science [91].\n(b) modifiable areal unit problem is a statistical bias in geospatial analysis, which arises\nwhen modeling data at different levels of spatial aggregation [56]. this bias results in\ndifferent trends learned when data is aggregated at different spatial scales.\n(5) sampling bias. sampling bias is similar to representation bias, and it arises due to non-\nrandom sampling of subgroups. as a consequence of sampling bias, the trends estimated for\none population may not generalize to data collected from a new population. for the intuition,\nconsider the example in figure 2. the left plot represents data collected during a study from\nthree subgroups, which were uniformly sampled (fig. 2(a)). suppose the next time the study\nwas conducted, one of the subgroups was sampled more frequently than the rest (fig. 2(b)). the\npositive trend found by the regression model in the first study almost completely disappears\n(solid red line in plot on the right), although the subgroup trends (dashed green lines) are\nunaffected.\n(6) longitudinal data fallacy. researchers analyzing temporal data must use longitudinal anal-\nysis to track cohorts over time to learn their behavior. instead, temporal data is often modeled\na survey on bias and fairness in machine learning\n7\nusing cross-sectional analysis, which combines diverse cohorts at a single time point. the\nheterogeneous cohorts can bias cross-sectional analysis, leading to different conclusions than\nlongitudinal analysis. as an example, analysis of bulk reddit data [10] revealed that comment\nlength decreased over time on average. however, bulk data represented a cross-sectional\nsnapshot of the population, which in reality contained different cohorts who joined reddit\nin different years. when data was disaggregated by cohorts, the comment length within each\ncohort was found to increase over time.\n(7) linking bias. linking bias arises when network attributes obtained from user connections,\nactivities, or interactions differ and misrepresent the true behavior of the users [120]. in\n[104] authors show how social networks can be biased toward low-degree nodes when only\nconsidering the links in the network and not considering the content and behavior of users in\nthe network. [153] also shows that user interactions are significantly different from social link\npatterns that are based on features, such as",
  "method": "method of interaction or time. the differences and\nbiases in the networks can be a result of many factors, such as network sampling, as shown in\n[59, 111], which can change the network measures and cause different types of problems.\n3.1.2\nalgorithm to user. algorithms modulate user behavior. any biases in algorithms might\nintroduce biases in user behavior. in this section we talk about biases that are as a result of algorithmic\noutcomes and affect user behavior as a consequence.\n(1) algorithmic bias. algorithmic bias is when the bias is not present in the input data and is\nadded purely by the algorithm [9]. the algorithmic design choices, such as use of certain\noptimization functions, regularizations, choices in applying regression models on the data as\na whole or considering subgroups, and the general use of statistically biased estimators in\nalgorithms [44], can all contribute to biased algorithmic decisions that can bias the outcome of\nthe algorithms.\n(2) user interaction bias. user interaction bias is a type of bias that can not only be observant\non the web but also get triggered from two sources\u2014the user interface and through the user\nitself by imposing his/her self-selected biased behavior and interaction [9]. this type of bias\ncan be influenced by other types and subtypes, such as presentation and ranking biases.\n(a) presentation bias. presentation bias is a result of how information is presented [9]. for\nexample, on the web users can only click on content that they see, so the seen content gets\nclicks, while everything else gets no click. and it could be the case that the user does not see\nall the information on the web [9].\n(b) ranking bias. the idea that top-ranked results are the most relevant and important will\nresult in attraction of more clicks than others. this bias affects search engines [9] and\ncrowdsourcing applications [93].\n(3) popularity bias. items that are more popular tend to be exposed more. however, popularity\nmetrics are subject to manipulation\u2014for example, by fake reviews or social bots [117]. as an\ninstance, this type of bias can be seen in search engines [71, 117] or recommendation systems\nwhere popular objects would be presented more to the public. but this presentation may not be\na result of good quality; instead, it may be due to other biased factors.\n(4) emergent bias. emergent bias occurs as a result of use and interaction with real users. this\nbias arises as a result of change in population, cultural values, or societal knowledge usually\nsome time after the completion of design [53]. this type of bias is more likely to be observed\nin user interfaces, since interfaces tend to reflect the capacities, characteristics, and habits of\nprospective users by design [53]. this type of bias can itself be divided into more subtypes, as\ndiscussed in detail in [53].\n8\nmehrabi et al.\n(5) evaluation bias. evaluation bias happens during model evaluation [144]. this includes the\nuse of inappropriate and disproportionate benchmarks for evaluation of applications such\nas adience and ijb-a benchmarks. these benchmarks are used in the evaluation of facial\nrecognition systems that were biased toward skin color and gender [24], and can serve as\nexamples for this type of bias [144].\n3.1.3\nuser to data. many data sources used for training ml models are user-generated. any\ninherent biases in users might be reflected in the data they generate. furthermore, when user behavior\nis affected/modulated by an algorithm, any biases present in those algorithm might introduce bias in\nthe data generation process. here we list several important types of such biases.\n(1) historical bias. historical bias is the already existing bias and socio-technical issues in the\nworld and can seep into from the data generation process even given a perfect sampling and\nfeature selection [144]. an example of this type of bias can be found in a 2018 image search\nresult where searching for women ceos ultimately resulted in fewer female ceo images due\nto the fact that only 5% of fortune 500 ceos were woman\u2014which would cause the search\nresults to be biased towards male ceos [144]. these search results were of course reflecting\nthe reality, but whether or not the search algorithms should reflect this reality is an issue worth\nconsidering.\n(2) population bias. population bias arises when statistics, demographics, representatives, and\nuser characteristics are different in the user population of the platform from the original target\npopulation [120]. population bias creates non-representative data. an example of this type of\nbias can arise from different user demographics on different social platforms, such as women\nbeing more likely to use pinterest, facebook, instagram, while men being more active in online\nforums like reddit or twitter. more such examples and statistics related to social media use\namong young adults according to gender, race, ethnicity, and parental educational background\ncan be found in [64].\n(3) self-selection bias. self-selection bias4 is a subtype of the selection or sampling bias in which\nsubjects of the research select themselves. an example of this type of bias can be observed in\nan opinion poll to measure enthusiasm for a political candidate, where the most enthusiastic\nsupporters are more likely to complete the poll.\n(4) social bias. social bias happens when others\u2019 actions affect our judgment. [9]. an example\nof this type of bias can be a case where we want to rate or review an item with a low score, but\nwhen influenced by other high ratings, we change our scoring thinking that perhaps we are\nbeing too harsh [9, 151].\n(5) behavioral bias. behavioral bias arises from different user behavior across platforms, con-\ntexts, or different datasets [120]. an example of this type of bias can be observed in [108],\nwhere authors show how differences in emoji representations among platforms can result in\ndifferent reactions and behavior from people and sometimes even leading to communication\nerrors.\n(6) temporal bias. temporal bias arises from differences in populations and behaviors over time\n[120]. an example can be observed in twitter where people talking about a particular topic\nstart using a hashtag at some point to capture attention, then continue the discussion about the\nevent without using the hashtag [120, 146].\n(7) content production bias. content production bias arises from structural, lexical, semantic,\nand syntactic differences in the contents generated by users [120]. an example of this type of\nbias can be seen in [118] where the differences in use of language across different gender and\n4https://data36.com/statistical-bias-types-explained/\na survey on bias and fairness in machine learning\n9\nage groups is discussed. the differences in use of language can also be seen across and within\ncountries and populations.\nexisting work tries to categorize these bias definitions into groups, such as definitions falling solely\nunder data or user interaction. however, due to the existence of the feedback loop phenomenon [36],\nthese definitions are intertwined, and we need a categorization which closely models this situation.\nthis feedback loop is not only existent between the data and the algorithm, but also between the\nalgorithms and user interaction [29]. inspired by these papers, we modeled categorization of bias\ndefinitions, as shown in figure 1, and grouped these definitions on the arrows of the loop where we\nthought they were most effective. we emphasize the fact again that these definitions are intertwined,\nand one should consider how they affect each other in this cycle, and address them accordingly.\n3.2\ndata bias examples\nthere are multiple ways that discriminatory bias can seep into data. for instance, using unbalanced\ndata can create biases against underrepresented groups. [170] analyzes some examples of the biases\nthat can exist in the data and algorithms and offer some recommendations and suggestions toward\nmitigating these issues.\n3.2.1\nexamples of bias in machine learning data. in [24], the authors show that datasets like\nijb-a and adience are imbalanced and contain mainly light-skinned subjects\u201479.6% in ijb-a and\n86.2% in adience. this can bias the analysis towards dark-skinned groups who are underrepresented\nin the data. in another instance, the way we use and analyze our data can create bias when we do not\nconsider different subgroups in the data. in [24], the authors also show that considering only male-\nfemale groups is not enough, but there is also a need to use race to further subdivide the gender groups\ninto light-skinned females, light-skinned males, dark-skinned males, and dark-skinned females. it\u2019s\nonly in this case that we can clearly observe the bias towards dark-skinned females, as previously\ndark-skinned males would compromise for dark-skinned females and would hide the underlying\nbias towards this subgroup. popular machine-learning datasets that serve as a base for most of\nthe developed algorithms and tools can also be biased\u2014which can be harmful to the downstream\napplications that are based on these datasets. for instance, imagenet [135] and open images [86] are\ntwo widely used datasets in machine-learning. in [142], researchers showed that these datasets suffer\nfrom representation bias and advocate for the need to incorporate geographic diversity and inclusion\nwhile creating such datasets. in addition, authors in [105] write about the existing representational\nbiases in different knowledge bases that are widely used in natural language processing (nlp)\napplications for different commonsense reasoning tasks.\n3.2.2\nexamples of data bias in medical applications. these data biases can be more dangerous\nin other sensitive applications. for example, in medical domains there are many instances in which\nthe data studied and used are skewed toward certain populations\u2014which can have dangerous conse-\nquences for the underrepresented communities. [98] showed how exclusion of african-americans\nresulted in their misclassification in clinical studies, so they became advocates for sequencing the\ngenomes of diverse populations in the data to prevent harm to underrepresented populations. authors\nin [143] studied the 23andme genotype dataset and found that out of 2,399 individuals, who have\nopenly shared their genotypes in public repositories, 2,098 (87%) are european, while only 58\n(2%) are asian and 50 (2%) african. other such studies were conducted in [54] which states that\nuk biobank, a large and widely used genetic dataset, may not represent the sampling population.\nresearchers found evidence of a \u201chealthy volunteer\u201d selection bias. [150] has other examples of\nstudies on existing biases in the data used in the medical domain. [157] also looks at machine-learning\n10\nmehrabi et al.\nalgorithms and data utilized in medical fields, and writes about how artificial intelligence in health\ncare has not impacted all patients equally.\n3.3\ndiscrimination\nsimilar to bias, discrimination is also a source of unfairness. discrimination can be considered as a\nsource for unfairness that is due to human prejudice and stereotyping based on the sensitive attributes,\nwhich may happen intentionally or unintentionally, while bias can be considered as a source for\nunfairness that is due to the data collection, sampling, and measurement. although bias can also be\nseen as a source of unfairness that is due to human prejudice and stereotyping, in the algorithmic\nfairness literature it is more intuitive to categorize them as such according to the existing research\nin these areas. in this survey, we mainly focus on concepts that are relevant to algorithmic fairness\nissues. [99, 133, 152] contain more broad information on discrimination theory that involve more\nmultidisciplinary concepts from legal theory, economics, and social sciences which can be referenced\nby the interested readers.\n3.3.1\nexplainable discrimination. differences in treatment and outcomes amongst different\ngroups can be justified and explained via some attributes in some cases. in situations where these\ndifferences are justified and explained, it is not considered to be illegal discrimination and hence\ncalled explainable [77]. for instance, authors in [77] state that in the uci adult dataset [7], a widely\nused dataset in the fairness domain, males on average have a higher annual income than females.\nhowever, this is because on average females work fewer hours than males per week. work hours\nper week is an attribute that can be used to explain low income which needs to be considered. if we\nmake decisions, without considering working hours, such that males and females end up averaging\nthe same income, we will lead to reverse discrimination since we would cause male employees to get\nlower salary than females. therefore, explainable discrimination is acceptable and legal as it can\nbe explained through other attributes like working hours. in [77], authors present a methodology\nto quantify the explainable and illegal discrimination in data. they argue that methods that do not\ntake the explainable part of the discrimination into account may result in non-desirable outcomes, so\nthey introduce a reverse discrimination which is equally harmful and undesirable. they explain how\nto quantify and measure discrimination in data or a classifier\u2019s decisions which directly considers\nillegal and explainable discrimination.\n3.3.2\nunexplainable discrimination. in contrast to explainable discrimination, there is unexplain-\nable discrimination in which the discrimination toward a group is unjustified and therefore considered\nillegal. authors in [77] also present local techniques for removing only the illegal or unexplainable\ndiscrimination, allowing only for explainable differences in decisions. these are preprocessing tech-\nniques that change the training data such that it contains no unexplainable discrimination. we expect\nclassifiers trained on this preprocessed data to not capture illegal or unexplainable discrimination.\nunexplainable discrimination consists of direct and indirect discrimination.\n(1) direct discrimination. direct discrimination happens when protected attributes of individuals\nexplicitly result in non-favorable outcomes toward them [164]. typically, there are some traits\nidentified by law on which it is illegal to discriminate against, and it is usually these traits that\nare considered to be \u201cprotected\u201d or \u201csensitive\u201d attributes in computer science literature. a list\nof some of these protected attributes is provided in table 3 as specified in the fair housing\nand equal credit opportunity acts (fha and ecoa) [30].\n(2) indirect discrimination. in indirect discrimination, individuals appear to be treated based\non seemingly neutral and non-protected attributes; however, protected groups, or individuals\nstill get to be treated unjustly as a result of implicit effects from their protected attributes (e.g.,\na survey on bias and fairness in machine learning\n11\nthe residential zip code of a person can be used in decision making processes such as loan\napplications. however, this can still lead to racial discrimination, such as redlining, as despite\nthe fact that zip code appears to be a non-sensitive attribute, it may correlate with race because\nof the population of residential areas.) [130, 164].\n3.3.3\nsources of discrimination.\n(1) systemic discrimination. systemic discrimination refers to policies, customs, or behaviors\nthat are a part of the culture or structure of an organization that may perpetuate discrimination\nagainst certain subgroups of the population [40]. [132] found that employers overwhelmingly\npreferred competent candidates that were culturally similar to them, and shared similar ex-\nperiences and hobbies. if the decision-makers happen to belong overwhelmingly to certain\nsubgroups, this may result in discrimination against competent candidates that do not belong\nto these subgroups.\n(2) statistical discrimination. statistical discrimination is a phenomenon where decision-makers\nuse average group statistics to judge an individual belonging to that group. it usually occurs\nwhen the decision-makers (e.g., employers, or law enforcement officers) use an individual\u2019s\nobvious, recognizable characteristics as a proxy for either hidden or more-difficult-to-determine\ncharacteristics, that may actually be relevant to the outcome [124].\n4\nalgorithmic fairness\nfighting against bias and discrimination has a long history in philosophy and psychology, and\nrecently in machine-learning. however, in order to be able to fight against discrimination and achieve\nfairness, one should first define fairness. philosophy and psychology have tried to define the concept\nof fairness long before computer science. the fact that no universal definition of fairness exists shows\nthe difficulty of solving this problem [138]. different preferences and outlooks in different cultures\nlend a preference to different ways of looking at fairness, which makes it harder to come up with just\na single definition that is acceptable to everyone in a situation. indeed, even in computer science,\nwhere most of the work on proposing new fairness constraints for algorithms has come from the\nwest, and a lot of these papers use the same datasets and problems to show how their constraints\nperform, there is still no clear agreement on which constraints are the most appropriate for those\nproblems. broadly, fairness is the absence of any prejudice or favoritism towards an individual or a\ngroup based on their intrinsic or acquired traits in the context of decision-making [139]. even though\nfairness is an incredibly desirable quality in society, it can be surprisingly difficult to achieve in\npractice. with these challenges in mind, many fairness definitions are proposed to address different\nalgorithmic bias and discrimination issues discussed in the previous section.\n4.1\ndefinitions of fairness\nin [17], authors studied fairness definitions in political philosophy and tried to tie them to machine-\nlearning. authors in [70] studied the 50-year history of fairness definitions in the areas of education\nand machine-learning. in [149], authors listed and explained some of the definitions used for fairness\nin algorithmic classification problems. in [139], authors studied the general public\u2019s perception of\nsome of these fairness definitions in computer science literature. here we will reiterate and provide\nsome of the most widely used definitions, along with their explanations inspired from [149].\ndefinition 1. (equalized odds). the definition of equalized odds, provided by [63], states that\n\u201ca predictor \u02c6y satisfies equalized odds with respect to protected attribute a and outcome y, if \u02c6y and a\nare independent conditional on y. p( \u02c6y=1|a=0,y =y) = p( \u02c6y=1|a=1,y =y) , y\u2208{0,1}\u201d. this means\nthat the probability of a person in the positive class being correctly assigned a positive outcome\n12\nmehrabi et al.\nand the probability of a person in a negative class being incorrectly assigned a positive outcome\nshould both be the same for the protected and unprotected group members [149]. in other words, the\nequalized odds definition states that the protected and unprotected groups should have equal rates for\ntrue positives and false positives.\ndefinition 2. (equal opportunity). \u201ca binary predictor \u02c6y satisfies equal opportunity with respect\nto a and y if p( \u02c6y=1|a=0,y=1) = p( \u02c6y=1|a=1,y=1)\u201d [63]. this means that the probability of a\nperson in a positive class being assigned to a positive outcome should be equal for both protected\nand unprotected (female and male) group members [149]. in other words, the equal opportunity\ndefinition states that the protected and unprotected groups should have equal true positive rates.\ndefinition 3. (demographic parity). also known as statistical parity. \u201ca predictor \u02c6y satisfies demo-\ngraphic parity if p( \u02c6y |a = 0) = p( \u02c6y|a = 1)\u201d [48, 87]. the likelihood of a positive outcome [149]\nshould be the same regardless of whether the person is in the protected (e.g., female) group.\ndefinition 4. (fairness through awareness). \u201can algorithm is fair if it gives similar predictions to\nsimilar individuals\u201d [48, 87]. in other words, any two individuals who are similar with respect to a\nsimilarity (inverse distance) metric defined for a particular task should receive a similar outcome.\ndefinition 5. (fairness through unawareness). \u201can algorithm is fair as long as any protected\nattributes a are not explicitly used in the decision-making process\u201d [61, 87].\ndefinition 6. (treatment equality). \u201ctreatment equality is achieved when the ratio of false negatives\nand false positives is the same for both protected group categories\u201d [15].\ndefinition 7. (test fairness). \u201ca score s = s(x) is test fair (well-calibrated) if it reflects the same\nlikelihood of recidivism irrespective of the individual\u2019s group membership, r. that is, if for all values\nof s, p(y =1|s=s,r=b)=p(y =1|s=s,r=w)\u201d [34]. in other words, the test fairness definition states\nthat for any predicted probability score s, people in both protected and unprotected groups must have\nequal probability of correctly belonging to the positive class [149].\ndefinition 8. (counterfactual fairness). \u201cpredictor \u02c6y is counterfactually fair if under any con-\ntext x =x and a=a, p( \u02c6\ud835\udc4c\ud835\udc34\u2190\u2212\ud835\udc4e(u)=y|x =x,a=a)=p( \u02c6\ud835\udc4c\ud835\udc34\u2190\u2212\ud835\udc4e\u2032(u)=y|x =x,a=a), (for all y and for any\nvalue \ud835\udc4e\u2032 attainable by a\u201d [87]. the counterfactual fairness definition is based on the \u201cintuition that a\ndecision is fair towards an individual if it is the same in both the actual world and a counterfactual\nworld where the individual belonged to a different demographic group.\u201d\ndefinition 9. (fairness in relational domains). \u201ca notion of fairness that is able to capture the\nrelational structure in a domain\u2014not only by taking attributes of individuals into consideration but\nby taking into account the social, organizational, and other connections between individuals\u201d [50].\ndefinition 10. (conditional statistical parity). for a set of legitimate factors l, predictor \u02c6y satisfies\nconditional statistical parity if p( \u02c6y |l=1,a = 0) = p( \u02c6y|l=1,a = 1) [41]. conditional statistical parity\nstates that people in both protected and unprotected (female and male) groups should have equal\nprobability of being assigned to a positive outcome given a set of legitimate factors l [149].\nfairness definitions fall under different types as follows:\na survey on bias and fairness in machine learning\n13\nname\nreference\ngroup\nsubgroup\nindividual\ndemographic parity\n[87][48]\n\u2713\nconditional statistical parity\n[41]\n\u2713\nequalized odds\n[63]\n\u2713\nequal opportunity\n[63]\n\u2713\ntreatment equality\n[15]\n\u2713\ntest fairness\n[34]\n\u2713\nsubgroup fairness\n[79][80]\n\u2713\nfairness through unawareness\n[87][61]\n\u2713\nfairness through awareness\n[48]\n\u2713\ncounterfactual fairness\n[87]\n\u2713\ntable 1. categorizing different fairness notions into group, subgroup, and individual types.\n(1) individual fairness. give similar predictions to similar individuals [48, 87].\n(2) group fairness. treat different groups equally [48, 87].\n(3) subgroup fairness. subgroup fairness intends to obtain the best properties of the group and\nindividual notions of fairness. it is different than these notions but uses them in order to obtain\nbetter outcomes. it picks a group fairness constraint like equalizing false positive and asks\nwhether this constraint holds over a large collection of subgroups [79, 80].\nit is important to note that according to [83], it is impossible to satisfy some of the fairness con-\nstraints at once except in highly constrained special cases. in [83], the authors show the inherent\nincompatibility of two conditions: calibration and balancing the positive and negative classes. these\ncannot be satisfied simultaneously with each other unless under certain constraints; therefore, it\nis important to take the context and application in which fairness definitions need to be used into\nconsideration and use them accordingly [141]. another important aspect to consider is time and\ntemporal analysis of the impacts that these definitions may have on individuals or groups. in [95]\nauthors show that current fairness definitions are not always helpful and do not promote improvement\nfor sensitive groups\u2014and can actually be harmful when analyzed over time in some cases. they\nalso show that measurement errors can also act in favor of these fairness definitions; therefore, they\nshow how temporal modeling and measurement are important in evaluation of fairness criteria and\nintroduce a new range of trade-offs and challenges toward this direction. it is also important to pay\nattention to the sources of bias and their types when trying to solve fairness-related questions.\n5\nmethods for fair machine learning\nthere have been numerous attempts to address bias in artificial intelligence in order to achieve\nfairness; these stem from domains of ai. in this section we will enumerate different domains of\nai, and the work that has been produced by each community to combat bias and unfairness in their\nmethods. table 2 provides an overview of the different areas that we focus upon in this survey.\nwhile this section is largely domain-specific, it can be useful to take a cross-domain view. gener-\nally, methods that target biases in the algorithms fall under three categories:\n(1) pre-processing. pre-processing techniques try to transform the data so that the underlying\ndiscrimination is removed [43]. if the algorithm is allowed to modify the training data, then\npre-processing can be used [11].\n(2) in-processing. in-processing techniques try to modify and change state-of-the-art learning\nalgorithms in order to remove discrimination during the model training process [43]. if it is\n14\nmehrabi et al.\nallowed to change the learning procedure for a machine learning model, then in-processing\ncan be used during the training of a model\u2014 either by incorporating changes into the objective\nfunction or imposing a constraint [11, 14].\n(3) post-processing. post-processing is performed after training by accessing a holdout set which\nwas not involved during the training of the model [43]. if the algorithm can only treat the\nlearned model as a black box without any ability to modify the training data or learning\nalgorithm, then only post-processing can be used in which the labels assigned by the black-box\nmodel initially get reassigned based on a function during the post-processing phase [11, 14].\nexamples of some existing work and their categorization into these types is shown in table 4. these\nmethods are not just limited to general machine learning techniques, but because of ai\u2019s popularity,\nthey have expanded to different domains such as natural language processing and deep learning. from\nlearning fair representations [42, 97, 112] to learning fair word embeddings [20, 58, 169], debiasing\nmethods have been proposed in different ai applications and domains. most of these methods try\nto avoid unethical interference of sensitive or protected attributes into the decision-making process,\nwhile others target exclusion bias by trying to include users from sensitive groups. in addition, some\nworks try to satisfy one or more of the fairness notions in their methods, such as disparate learning\nprocesses (dlps) which try to satisfy notions of treatment disparity and impact disparity by allowing\nthe protected attributes during the training phase but avoiding them during prediction time [94]. a\nlist of protected or sensitive attributes is provided in table 3. they point out what attributes should\nnot affect the outcome of the decision in housing loan or credit card decision-making [30] according\nto the law. some of the existing work tries to treat sensitive attributes as noise to disregard their\neffect on decision-making, while some causal methods use causal graphs, and disregard some paths\nin the causal graph that result in sensitive attributes affecting the outcome of the decision. different\nbias-mitigating methods and techniques are discussed below for different domains\u2014each targeting a\ndifferent problem in different areas of machine learning in detail. this can expand the horizon of\nthe reader on where and how bias can affect the system and try to help researchers carefully look\nat various new problems concerning potential places where discrimination and bias can affect the\noutcome of a system.\n5.1\nunbiasing data\nevery dataset is the result of several design decisions made by the data curator. those decisions have\nconsequences for the fairness of the resulting dataset, which in turn affects the resulting algorithms. in\norder to mitigate the effects of bias in data, some general methods have been proposed that advocate\nhaving good practices while using data, such as having datasheets that would act like a supporting\ndocument for the data reporting the dataset creation method, its characteristics, motivations, and its\nskews [13, 55]. [12] proposes a similar approach for the nlp applications. a similar suggestion has\nbeen proposed for models in [110]. authors in [66] also propose having labels, just like nutrition\nlabels on food, in order to better categorize each data for each task. in addition to these general\ntechniques, some work has targeted more specific types of biases. for example, [81] has proposed\nmethods to test for cases of simpson\u2019s paradox in the data, and [3, 4] proposed methods to discover\nsimpson\u2019s paradoxes in data automatically. causal models and graphs were also used in some work\nto detect direct discrimination in the data along with its prevention technique that modifies the data\nsuch that the predictions would be absent from direct discrimination [163]. [62] also worked on\npreventing discrimination in data mining, targeting direct, indirect, and simultaneous effects. other\npre-processing approaches, such as messaging [74], preferential sampling [75, 76], disparate impact\nremoval [51], also aim to remove biases from the data.\na survey on bias and fairness in machine learning\n15\narea\nreference(s)\nclassification\n[78] [106] [57] [85] [147] [63] [159] [154] [69]\n[25] [155] [122] [49] [73] [75]\nregression\n[14] [1]\npca\n[137]\ncommunity detection\n[104]\nclustering\n[31] [8]\ngraph embedding\n[22]\ncausal inference\n[96] [164] [165] [160] [116] [115] [162] [82] [127]\n[161]\nvariational auto encoders\n[97] [5] [112] [42]\nadversarial learning\n[90] [156]\nword embedding\n[20] [169] [58] [23] [166]\ncoreference resolution\n[168] [134]\nlanguage model\n[21]\nsentence embedding\n[100]\nmachine translation\n[52]\nsemantic role labeling\n[167]\nnamed entity recognition\n[101]\ntable 2. list of papers targeting and talking about bias and fairness in different areas.\nattribute\nfha\necoa\nrace\n\u2713\n\u2713\ncolor\n\u2713\n\u2713\nnational origin\n\u2713\n\u2713\nreligion\n\u2713\n\u2713\nsex\n\u2713\n\u2713\nfamilial status\n\u2713\ndisability\n\u2713\nexercised rights under ccpa\n\u2713\nmarital status\n\u2713\nrecipient of public assistance\n\u2713\nage\n\u2713\ntable 3. a list of the protected attributes as specified in the fair housing and equal credit opportunity\nacts (fha and ecoa), from [30].\n5.2\nfair machine learning\nto address this issue, a variety of methods have been proposed that satisfy some of the fairness\ndefinitions or other new definitions depending on the application.\n5.2.1\nfair classification. since classification is a canonical task in machine learning and is\nwidely used in different areas that can be in direct contact with humans, it is important that these\ntypes of methods be fair and be absent from biases that can harm some populations. therefore,\ncertain methods have been proposed [57, 78, 85, 106] that satisfy certain definitions of fairness in\nclassification. for instance, in [147] authors try to satisfy subgroup fairness in classification, equality\nof opportunity and equalized odds in [63], both disparate treatment and disparate impact in [2, 159],\n16\nmehrabi et al.\nand equalized odds in [154]. other methods try to not only satisfy some fairness constraints but to\nalso be stable toward change in the test set [69]. the authors in [155], propose a general framework\nfor learning fair classifiers. this framework can be used for formulating fairness-aware classification\nwith fairness guarantees. in another work [25], authors propose three different modifications to the\nexisting naive bayes classifier for discrimination-free classification. [122] takes a new approach\ninto fair classification by imposing fairness constraints into a multitask learning (mtl) framework.\nin addition to imposing fairness during training, this approach can benefit the minority groups by\nfocusing on maximizing the average accuracy of each group as opposed to maximizing the accuracy\nas a whole without attention to accuracy across different groups. in a similar work [49], authors\npropose a decoupled classification system where a separate classifier is learned for each group. they\nuse transfer learning to reduce the issue of having less data for minority groups. in [73] authors\npropose to achieve fair classification by mitigating the dependence of the classification outcome on\nthe sensitive attributes by utilizing the wasserstein distance measure. in [75] authors propose the\npreferential sampling (ps) method to create a discrimination free train data set. they then learn\na classifier on this discrimination free dataset to have a classifier with no discrimination. in [102],\nauthors propose a post-processing bias mitigation strategy that utilizes attention mechanism for\nclassification and that can provide interpretability.\nalgorithm\nreference\npre-processing\nin-processing\npost-processing\ncommunity detection\n[104]\n\u2713\nword embedding\n[23]\n\u2713\noptimized pre-processing\n[27]\n\u2713\ndata pre-processing\n[76]\n\u2713\nclassification\n[159]\n\u2713\nregression\n[14]\n\u2713\nclassification\n[78]\n\u2713\nclassification\n[155]\n\u2713\nadversarial learning\n[90]\n\u2713\nclassification\n[63]\n\u2713\nword embedding\n[20]\n\u2713\nclassification\n[125]\n\u2713\nclassification\n[102]\n\u2713\ntable 4. algorithms categorized into their appropriate groups based on being pre-processing, in-\nprocessing, or post-processing.\n5.2.2\nfair regression. [14] proposes a fair regression method along with evaluating it with a\nmeasure introduced as the \u201cprice of fairness\u201d (pof) to measure accuracy-fairness trade-offs. they\nintroduce three fairness penalties as follows:\nindividual fairness: the definition for individual fairness as stated in [14], \u201cfor every cross pair\n(\ud835\udc65,\ud835\udc66) \u2208\ud835\udc461, (\ud835\udc65\u2032,\ud835\udc66\u2032) \u2208\ud835\udc462, a model \ud835\udc64is penalized for how differently it treats \ud835\udc65and \ud835\udc65\u2032 (weighted by a\nfunction of |\ud835\udc66\u2212\ud835\udc66\u2032|) where \ud835\udc461 and \ud835\udc462 are different groups from the sampled population.\u201d formally,\nthis is operationalized as\n\ud835\udc531(\ud835\udc64,\ud835\udc46) =\n1\n\ud835\udc5b1\ud835\udc5b2\n\u2211\ufe01\n(\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56) \u2208\ud835\udc461\n(\ud835\udc65\ud835\udc57,\ud835\udc66\ud835\udc57) \u2208\ud835\udc462\n\ud835\udc51(\ud835\udc66\ud835\udc56,\ud835\udc66\ud835\udc57)(\ud835\udc64.\ud835\udc65\ud835\udc56\u2212\ud835\udc64.\ud835\udc65\ud835\udc57)2\na survey on bias and fairness in machine learning\n17\ngroup fairness: \"on average, the two groups\u2019 instances should have similar labels (weighted by the\nnearness of the labels of the instances)\" [14].\n\ud835\udc532(\ud835\udc64,\ud835\udc46) =\n \n1\n\ud835\udc5b1\ud835\udc5b2\n\u2211\ufe01\n(\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56) \u2208\ud835\udc461\n(\ud835\udc65\ud835\udc57,\ud835\udc66\ud835\udc57) \u2208\ud835\udc462\n\ud835\udc51(\ud835\udc66\ud835\udc56,\ud835\udc66\ud835\udc57)(\ud835\udc64.\ud835\udc65\ud835\udc56\u2212\ud835\udc64.\ud835\udc65\ud835\udc57)\n!2\nhybrid fairness: \"hybrid fairness requires both positive and both negatively labeled cross pairs to be\ntreated similarly in an average over the two groups\" [14].\n\ud835\udc533(\ud835\udc64,\ud835\udc46) =\n \n\u2211\ufe01\n(\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56) \u2208\ud835\udc461\n(\ud835\udc65\ud835\udc57,\ud835\udc66\ud835\udc57) \u2208\ud835\udc462\n\ud835\udc66\ud835\udc56=\ud835\udc66\ud835\udc57=1\n\ud835\udc51(\ud835\udc66\ud835\udc56,\ud835\udc66\ud835\udc57)(\ud835\udc64.\ud835\udc65\ud835\udc56\u2212\ud835\udc64.\ud835\udc65\ud835\udc57)\n\ud835\udc5b1,1\ud835\udc5b2,1\n!2\n+\n \n\u2211\ufe01\n(\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56) \u2208\ud835\udc461\n(\ud835\udc65\ud835\udc57,\ud835\udc66\ud835\udc57) \u2208\ud835\udc462\n\ud835\udc66\ud835\udc56=\ud835\udc66\ud835\udc57=\u22121\n\ud835\udc51(\ud835\udc66\ud835\udc56,\ud835\udc66\ud835\udc57)(\ud835\udc64.\ud835\udc65\ud835\udc56\u2212\ud835\udc64.\ud835\udc65\ud835\udc57)\n\ud835\udc5b1,\u22121\ud835\udc5b2,\u22121\n!2\nin addition to the previous work, [1] considers the fair regression problem formulation with regards\nto two notions of fairness statistical (demographic) parity and bounded group loss. [2] uses decision\ntrees to satisfy disparate impact and treatment in regression tasks in addition to classification.\n5.2.3\nstructured prediction. in [167], authors studied the semantic role-labeling models and a\nfamous dataset, imsitu, and realized that only 33% of agent roles in cooking images are man, and\nthe rest of 67% cooking images have woman as agents in the imsitu training set. they also noticed\nthat in addition to the existing bias in the dataset, the model would amplify the bias such that after\ntraining a model5 on the dataset, bias is magnified for \u201cman\u201d, filling only 16% of cooking images.\nunder these observations, the authors of the paper [167] show that structured prediction models\nhave the risk of leveraging social bias. therefore, they propose a calibration algorithm called rba\n(reducing bias amplification); rba is a technique for debiasing models by calibrating prediction in\nstructured prediction. the idea behind rba is to ensure that the model predictions follow the same\ndistribution in the training data. they study two cases: multi-label object and visual semantic role\nlabeling classification. they show how these methods amplify the existing bias in data.\n5.2.4\nfair pca. in [137] authors show that vanilla pca can exaggerate the error in reconstruction\nin one group of people over a different group of equal size, so they propose a fair method to create\nrepresentations with similar richness for different populations\u2014not to make them indistinguishable,\nor to hide dependence on a sensitive or protected attribute. they show that vanilla pca on the\nlabeled faces in the wild (lfw) dataset [68] has a lower reconstruction error rate for men than for\nwomen faces, even if the sampling is done with an equal weight for both genders. they intend to\nintroduce a dimensionality reduction technique which maintains similar fidelity for different groups\nand populations in the dataset. therefore, they introduce fair pca and define a fair dimensionality\nreduction algorithm. their definition of fair pca (as an optimization function) is as follows, in\nwhich \ud835\udc34and \ud835\udc35denote two subgroups, \ud835\udc48\ud835\udc34and \ud835\udc48\ud835\udc35denote matrices whose rows correspond to rows of\n\ud835\udc48that contain members of subgroups \ud835\udc34and \ud835\udc35given \ud835\udc5adata points in \ud835\udc45\ud835\udc5b:\n\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udc48\u2208\ud835\udc45\ud835\udc5a\u00d7\ud835\udc5b,\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58(\ud835\udc48) \u2264\ud835\udc51\ud835\udc5a\ud835\udc4e\ud835\udc65\n(\n1\n|\ud835\udc34|\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60(\ud835\udc34,\ud835\udc48\ud835\udc34), 1\n|\ud835\udc35|\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60(\ud835\udc35,\ud835\udc48\ud835\udc35)\n)\nand their proposed algorithm is a two-step process listed below:\n(1) relax the fair pca objective to a semidefinite program (sdp) and solve it.\n(2) solve a linear program that would reduce the rank of the solution.\n5specifically, a conditional random field (crf)\n18\nmehrabi et al.\n5.2.5\ncommunity detection/graph embedding/clustering. inequalities in online communities\nand social networks can also potentially be another place where bias and discrimination can affect the\npopulations. for example, in online communities users with a fewer number of friends or followers\nface a disadvantage of being heard in online social media [104]. in addition, existing methods, such\nas community detection methods, can amplify this bias by ignoring these low-connected users in\nthe network or by wrongfully assigning them to the irrelevant and small communities. in [104]\nauthors show how this type of bias exists and is perpetuated by the existing community detection\nmethods. they propose a new attributed community detection method, called clan, to mitigate\nthe harm toward disadvantaged groups in online social communities. clan is a two-step process\nthat considers the network structure alongside node attributes to address exclusion bias, as indicated\nbelow:\n(1) detect communities using modularity values (step 1-unsupervised using only network struc-\nture).\n(2) train a classifier to classify users in the minor groups, putting them into one of the major\ngroups using held-out node attributes (step 2-supervised using other node attributes).\nfair methods in domains similar to community detection are also proposed, such as graph embedding\n[22] and clustering [8, 31].\n5.2.6\ncausal approach to fairness. causal models can ascertain causal relationships between\nvariables. using causal graphs one can represent these causal relationships between variables (nodes\nof the graph) through the edges of the graph. these models can be used to remove unwanted causal\ndependence of outcomes on sensitive attributes such as gender or race in designing systems or policies\n[96]. many researchers have used causal models and graphs to solve fairness-related concerns in\nmachine learning. in [33, 96], authors discuss in detail the subject of causality and its importance\nwhile designing fair algorithms. there has been much research on discrimination discovery and re-\nmoval that uses causal models and graphs in order to make decisions that are irrespective of sensitive\nattributes of groups or individuals. for instance, in [164] authors propose a causal-based framework\nthat detects direct and indirect discrimination in the data along with their removal techniques. [165]\nis an extension to the previous work. [160] gives a nice overview of most of the previous work done\nin this area by the authors, along with discussing system-, group-, and individual-level discrimination\nand solving each using their previous methods, in addition to targeting direct and indirect discrimi-\nnation. by expanding on the previous work and generalizing it, authors in [116] propose a similar\npathway approach for fair inference using causal graphs; this would restrict certain problematic and\ndiscriminative pathways in the causal graph flexibly given any set of constraints. this holds when\nthe path-specific effects can be identified from the observed distribution. in [32] authors introduce\nthe path-specific counterfactual fairness definition which is an extension to counterfactual fairness\ndefinition [87] and propose a method to achieve it further extending the work in [116]. in [115]\nauthors extended a formalization of algorithmic fairness from their previous work to the setting of\nlearning optimal policies that are subject to constraints based on definitions of fairness. they describe\nseveral strategies for learning optimal policies by modifying some of the existing strategies, such as\nq-learning, value search, and g-estimation, based on some fairness considerations. in [162] authors\nonly target discrimination discovery and no removal by finding instances similar to another instance\nand observing if a change in the protected attribute will change the outcome of the decision. if so,\nthey declare the existence of discrimination. in [82], authors define the following two notions of\ndiscrimination\u2014unresolved discrimination and proxy discrimination\u2014as follows:\nunresolved discrimination: \"a variable v in a causal graph exhibits unresolved discrimination if\nthere exists a directed path from a to v that is not blocked by a resolving variable, and v itself is\nnon-resolving\" [82].\na survey on bias and fairness in machine learning\n19\nproxy discrimination: \"a variable v in a causal graph exhibits potential proxy discrimination, if\nthere exists a directed path from a to v that is blocked by a proxy variable and v itself is not a\nproxy\" [82]. they proposed methods to prevent and avoid them. they also show that no observational\ncriterion can determine whether a predictor exhibits unresolved discrimination; therefore, a causal\nreasoning framework needs to be incorporated.\nin [127], instead of using the usual risk difference \ud835\udc45\ud835\udc37= \ud835\udc5d1 \u2212\ud835\udc5d2, authors propose a causal risk\ndifference \ud835\udc45\ud835\udc37\ud835\udc50= \ud835\udc5d1 \u2212\ud835\udc5d\ud835\udc50\n2 for causal discrimination discovery. they define \ud835\udc5d\ud835\udc50\n2 to be:\n\ud835\udc5d\ud835\udc50\n2 =\n\u00ed\ns\u2208\ud835\udc46,\ud835\udc51\ud835\udc52\ud835\udc50(s)=\u2296\ud835\udc64(s)\n\u00ed\ns\u2208\ud835\udc46\ud835\udc64(s)\n\ud835\udc45\ud835\udc37\ud835\udc50not close to zero means that there is a bias in decision value due to group membership (causal\ndiscrimination) or to covariates that have not been accounted for in the analysis (omitted variable\nbias). this \ud835\udc45\ud835\udc37\ud835\udc50then becomes their causal discrimination measure for discrimination discovery. [161]\nis another work of this type that uses causal networks for discrimination discovery.\n5.3\nfair representation learning\n5.3.1\nvariational auto encoders. learning fair representations and avoiding the unfair interfer-\nence of sensitive attributes has been introduced in many different research papers. a well-known\nexample is the variational fair autoencoder introduced in [97]. here,they treat the sensitive variable\nas the nuisance variable, so that by removing the information about this variable they will get a fair\nrepresentation. they use a maximum mean discrepancy regularizer to obtain invariance in the poste-\nrior distribution over latent variables. adding this maximum mean discrepancy (mmd) penalty into\nthe lower bound of their vae architecture satisfies their proposed model for having the variational\nfair autoencoder. similar work, but not targeting fairness specifically, has been introduced in [72].\nin [5] authors also propose a debiased vae architecture called db-vae which learns sensitive latent\nvariables that can bias the model (e.g., skin tone, gender, etc.) and propose an algorithm on top of this\ndb-vae using these latent variables to debias systems like facial detection systems. in [112] authors\nmodel their representation-learning task as an optimization objective that would minimize the loss of\nthe mutual information between the encoding and the sensitive variable. the relaxed version of this\nassumption is shown in equation 1. they use this in order to learn fair representation and show that\nadversarial training is unnecessary and in some cases even counter-productive. in equation 1, c is the\nsensitive variable and z the encoding of x.\n\ud835\udc5a\ud835\udc56\ud835\udc5b\n\ud835\udc5e\nl(\ud835\udc5e,\ud835\udc65) + \ud835\udf06\ud835\udc3c(\ud835\udc67,\ud835\udc50)\n(1)\nin [42], authors introduce flexibly fair representation learning by disentanglement that disentangles\ninformation from multiple sensitive attributes. their flexible and fair variational autoencoder is\nnot only flexible with respect to downstream task labels but also flexible with respect to sensitive\nattributes. they address the demographic parity notion of fairness, which can target multiple sensitive\nattributes or any subset combination of them.\n5.3.2\nadversarial learning. in [90] authors present a framework to mitigate bias in models\nlearned from data with stereotypical associations. they propose a model in which they are trying to\nmaximize accuracy of the predictor on y, and at the same time minimize the ability of the adversary\nto predict the protected or sensitive variable (stereotyping variable z). the model consists of two\nparts\u2014the predictor and the adversary\u2014as shown in figure 6. in their model, the predictor is trained\nto predict y given x. with the help of a gradient-based approach like stochastic gradient descent,\nthe model tries to learn the weights w by minimizing some loss function lp(\u02c6\ud835\udc66, y). the output layer\nis passed to an adversary, which is another network. this network tries to predict z. the adversary\n20\nmehrabi et al.\nmay have different inputs depending on the fairness definition needing to be achieved. for instance,\nin order to satisfy demographic parity, the adversary would try to predict the protected variable z\nusing only the predicted label \u02c6\ud835\udc4cpassed as an input to it, while preventing the adversary from learning\nthis is the goal of the predictor. similarly, to achieve equality of odds, the adversary would get\nthe true label y in addition to the predicted label \u02c6\ud835\udc4c. to satisfy equality of opportunity for a given\nclass y, they would only select instances for the adversary where y=y. [156] takes an interesting\nand different direction toward solving fairness issues using adversarial networks by introducing\nfairgan which generates synthetic data that is free from discrimination and is similar to the real\ndata. they use their newly generated synthetic data from fairgan, which is now debiased, instead\nof the real data for training and testing. they do not try to remove discrimination from the dataset,\nunlike many of the existing approaches, but instead generate new datasets similar to the real one\nwhich is debiased and preserves good data utility. the architecture of their fairgan model is shown\nin figure 5. fairgan consists of two components: a generator \ud835\udc3a\ud835\udc37\ud835\udc52\ud835\udc50which generates the fake data\nconditioned on the protected attribute \ud835\udc43\ud835\udc3a(\ud835\udc65,\ud835\udc66,\ud835\udc60) = \ud835\udc43\ud835\udc3a(\ud835\udc65,\ud835\udc66|\ud835\udc60)\ud835\udc43\ud835\udc3a(\ud835\udc60) where \ud835\udc43\ud835\udc3a(\ud835\udc60) = \ud835\udc43\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e(\ud835\udc60), and two\ndiscriminators \ud835\udc371 and \ud835\udc372. \ud835\udc371 is trained to differentiate the real data denoted by \ud835\udc43\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e(\ud835\udc65,\ud835\udc66,\ud835\udc60) from\nthe generated fake data denoted by \ud835\udc43\ud835\udc3a(\ud835\udc65,\ud835\udc66,\ud835\udc60).\npz\nnoise\nps\nprotected attribute\ngdec\ngenerator\npg (!, #|%)\npdata (!, #|%)\nd2\ndiscriminator\nd1\ndiscriminator\nreal: (!, #, %)\nfake: (&!, &#, \u0302%)\n(&!, &#| \u0302% = 0)\n(&!, &#| \u0302% = 1)\nfig. 5. structure of fairgan as proposed in [156].\nfig. 6. the architecture of adversarial network proposed in [90] \u00a9 brian hu zhang.\nin addition to that, for achieving fairness constraints, such as statistical parity, \ud835\udc43\ud835\udc3a(\ud835\udc65,\ud835\udc66|\ud835\udc60= 1) =\n\ud835\udc43\ud835\udc3a(\ud835\udc65,\ud835\udc66|\ud835\udc60= 0), the training of \ud835\udc372 is such that it emphasizes differentiation of the two types of\na survey on bias and fairness in machine learning\n21\nsynthetic (generated by the model) samples \ud835\udc43\ud835\udc3a(\ud835\udc65,\ud835\udc66|\ud835\udc60= 1) and \ud835\udc43\ud835\udc3a(\ud835\udc65,\ud835\udc66|\ud835\udc60= 0) indicating if the\nsynthetic samples are from the unprotected or protected groups. here s denotes the protected or the\nsensitive variable, and we adapted the same notation as in [156].\n5.4\nfair nlp\n5.4.1\nword embedding. in [20] authors noticed that while using state-of-the-art word embeddings\nin word analogy tests, \u201cman\u201d would be mapped to \u201ccomputer programmer\u201d and \u201cwoman\u201d would\nbe mapped to \u201chomemaker.\u201d this bias toward woman triggered the authors to propose a method to\ndebias word embeddings by proposing a method that respects the embeddings for gender-specific\nwords but debiases embeddings for gender-neutral words by following these steps: (notice that step\n2 has two different options. depending on whether you target hard debiasing or soft debiasing, you\nwould use either step 2a or 2b)\n(1) identify gender subspace. identifying a direction of the embedding that captures the bias\n[20].\n(2) hard debiasing or soft debiasing:\n(a) hard debiasing (neutralize and equalize). neutralize puts away the gender subspace\nfrom gender-neutral words and makes sure that all the gender-neutral words are removed\nand zeroed out in the gender subspace [20]. equalize makes gender-neutral words to be\nequidistant from the equality set of gendered words [20].\n(b) soft bias correction. tries to move as little as possible to retain its similarity to the original\nembedding as much as possible, while reducing the gender bias. this trade-off is controlled\nby a parameter [20].\nfollowing on the footsteps of these authors, other future work attempted to tackle this problem\n[169] by generating a gender-neutral version of (glove called gn-glove) that tries to retain gender\ninformation in some of the word embedding\u2019s learned dimensions, while ensuring that other dimen-\nsions are free from this gender effect. this approach primarily relies on glove as its base model\nwith gender as the protected attribute. however, a recent paper [58] argues against these debiasing\ntechniques and states that many recent works on debiasing word embeddings have been superficial,\nthat those techniques just hide the bias and don\u2019t actually remove it. a recent work [23] took a new\ndirection and proposed a preprocessing method for the discovery of the problematic documents in\nthe training corpus that have biases in them, and tried to debias the system by perturbing or removing\nthese documents efficiently from the training corpus. in a very recent work [166], authors target bias\nin elmo\u2019s contextualized word vectors and attempt to analyze and mitigate the observed bias in the\nembeddings. they show that the corpus used for training of elmo has a significant gender skew,\nwith male entities being nearly three times more common than female entities. this automatically\nleads to gender bias in these pretrained contextualized embeddings. they propose the following two\nmethods for mitigating the existing bias while using the pretrained embeddings in a downstream task,\ncoreference resolution: (1) train-time data augmentation approach, and (2) test-time neutralization\napproach.\n5.4.2\ncoreference resolution. the [168] paper shows that coreference systems have a gender\nbias. they introduce a benchmark, called winobias, focusing on gender bias in coreference resolution.\nin addition to that, they introduce a data-augmentation technique that removes bias in the existing\nstate-of-the-art coreferencing methods, in combination with using word2vec debiasing techniques.\ntheir general approach is as follows: they first generate auxiliary datasets using a rule-based\napproach in which they replace all the male entities with female entities and the other way around.\nthen they train models with a combination of the original and the auxiliary datasets. they use the\nabove solution in combination with word2vec debiasing techniques to generate word embeddings.\n22\nmehrabi et al.\nthey also point out sources of gender bias in coreference systems and propose solutions to them.\nthey show that the first source of bias comes from the training data and propose a solution that\ngenerates an auxiliary data set by swapping male and female entities. another case arises from\nthe resource bias (word embeddings are bias), so the proposed solution is to replace glove with a\ndebiased embedding method. last, another source of bias can come from unbalanced gender lists, and\nbalancing the counts in the lists is a solution they proposed. in another work [134], authors also show\nthe existence of gender bias in three state-of-the-art coreference resolution systems by observing that\nfor many occupations, these systems resolve pronouns in a biased fashion by preferring one gender\nover the other.\n5.4.3\nlanguage model. in [21] authors introduce a metric for measuring gender bias in a\ngenerated text from a language model based on recurrent neural networks that is trained on a text\ncorpus along with measuring the bias in the training text itself. they use equation 2, where \ud835\udc64is any\nword in the corpus, \ud835\udc53is a set of gendered words that belong to the female category, such as she, her,\nwoman, etc., and \ud835\udc5ato the male category, and measure the bias using the mean absolute and standard\ndeviation of the proposed metric along with fitting a univariate linear regression model over it and\nthen analyzing the effectiveness of each of those metrics while measuring the bias.\n\ud835\udc4f\ud835\udc56\ud835\udc4e\ud835\udc60(\ud835\udc64) = \ud835\udc59\ud835\udc5c\ud835\udc54( \ud835\udc43(\ud835\udc64|\ud835\udc53)\n\ud835\udc43(\ud835\udc64|\ud835\udc5a) )\n(2)\nin their language model, they also introduce a regularization loss term that would minimize the\nprojection of embeddings trained by the encoder onto the embedding of the gender subspace following\nthe soft debiasing technique introduced in [20]. finally, they evaluate the effectiveness of their method\non reducing gender bias and conclude by stating that in order to reduce bias, there is a compromise\non perplexity. they also point out the effectiveness of word-level bias metrics over the corpus-level\nmetrics.\n5.4.4\nsentence encoder. in [100] authors extend the research in detecting bias in word embed-\nding techniques to that of sentence embedding. they try to generalize bias-measuring techniques,\nsuch as using the word embedding association test (weat [26]) in the context of sentence en-\ncoders by introducing their new sentence encoding bias-measuring techniques, the sentence encoder\nassociation test (seat). they used state-of-the-art sentence encoding techniques, such as cbow,\ngpt, elmo, and bert, and find that although there was varying evidence of human-like bias in\nsentence encoders using seat, more recent methods like bert are more immune to biases. that\nbeing said, they are not claiming that these models are bias-free, but state that more sophisticated\nbias discovery techniques may be used in these cases, thereby encouraging more future work in this\narea.\n5.4.5\nmachine translation. in [52] authors noticed that when translating the word \"friend\" in the\nfollowing two sentences from english to spanish, they achieved different results\u2014although in both\ncases this word should be translated the same way.\n\"she works in a hospital, my friend is a nurse.\"\n\"she works in a hospital, my friend is a doctor.\"\nin both of these sentences, \"friend\" should be translated to the female version of spanish friend\n\"amiga,\" but the results were not reflecting this expectation. for the second sentence, friend was\ntranslated to \"amigo,\"\u2014the male version of friend in spanish. this is because doctor is more\nstereotypical to males and nurse to females, and the model picks this bias or stereotype and reflects it\nin its performance. to solve this, authors in [52] build an approach that leverages the fact that machine\ntranslation uses word embeddings. they use the existing debiasing methods in word embedding and\napply them in the machine translation pipeline. this not only helped them to mitigate the existing bias\na survey on bias and fairness in machine learning\n23\nin their system, but also boosted the performance of their system by one blue score. in [126] authors\nshow that google\u2019s translate system can suffer from gender bias by making sentences taken from the\nu.s. bureau of labor statistics into a dozen languages that are gender neutral, including yoruba,\nhungarian, and chinese, translating them into english, and showing that google translate shows\nfavoritism toward males for stereotypical fields such as stem jobs. in [148] authors annotated and\nanalyzed the europarl dataset [84], a large political, multilingual dataset used in machine translation,\nand discovered that with the exception of the youngest age group (20-30), which represents only a\nvery small percentage of the total amount of sentences (0.71%), more male data is available in all age\ngroups. they also looked at the entire dataset and showed that 67.39% of the sentences are produced\nby male speakers. furthermore, to mitigate the gender-related issues and to improve morphological\nagreement in machine translation, they augmented every sentence with a tag on the english source\nside, identifying the gender of the speaker. this helped the system in most of the cases, but not\nalways, so further work has been suggested for integrating speaker information in other ways.\n5.4.6\nnamed entity recognition. in [101], authors investigate a type of existing bias in various\nnamed entity recognition (ner) systems. in particular, they observed that in a context where an\nentity should be tagged as a person entity, such as \"john is a person\" or \"john is going to school\",\nmore female names as opposed to male names are being tagged as non-person entities or not being\ntagged at all. to further formalize their observations, authors propose six different evaluation metrics\nthat would measure amount of bias among different genders in ner systems. they curated templated\nsentences pertaining to human actions and applied these metrics on names from u.s census data\nincorporated into the templates. the six introduced measures each aim to demonstrate a certain type\nof bias and serve a specific purpose in showing various results as follows:\n\u2022 error type-1 unweighted: through this type of error, authors wanted to recognize the pro-\nportion of entities that are tagged as anything other than the person entity in each of the male\nvs female demographic groups. this could be the entity not being tagged or tagged as other\nentities, such as location.\n\u00ed\n\ud835\udc5b\u2208\ud835\udc41\ud835\udc53\ud835\udc3c(\ud835\udc5b\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52\u2260\ud835\udc43\ud835\udc38\ud835\udc45\ud835\udc46\ud835\udc42\ud835\udc41)\n|\ud835\udc41\ud835\udc53|\n\u2022 error type-1 weighted: this type of error is similar to its unweighted case except authors\nconsidered the frequency or popularity of names so that they could penalize if a more popular\nname is being tagged wrongfully.\n\u00ed\n\ud835\udc5b\u2208\ud835\udc41\ud835\udc53\ud835\udc53\ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc53(\ud835\udc5b\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52\u2260\ud835\udc43\ud835\udc38\ud835\udc45\ud835\udc46\ud835\udc42\ud835\udc41)\n\u00ed\n\ud835\udc5b\u2208\ud835\udc41\ud835\udc53\ud835\udc53\ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc53(\ud835\udc5b)\n,\nwhere \ud835\udc53\ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc53(\u00b7) indicates the frequency of a name for a particular year in the female census\ndata. likewise, \ud835\udc53\ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc5a(\u00b7) indicates the frequency of a name for a particular year in the male\ncensus data.\n\u2022 error type-2 unweighted: this is a type of error in which the entity is tagged as other entities,\nsuch as location or city. notice that this error does not count if the entity is not tagged.\n\u00ed\n\ud835\udc5b\u2208\ud835\udc41\ud835\udc53\ud835\udc3c(\ud835\udc5b\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52\u2209{\u2205, \ud835\udc43\ud835\udc38\ud835\udc45\ud835\udc46\ud835\udc42\ud835\udc41})\n|\ud835\udc41\ud835\udc53|\n,\nwhere \u2205indicates that the name is not tagged.\n24\nmehrabi et al.\n\u2022 error type-2 weighted: this error is again similar to its unweighted case except the frequency\nis taken into consideration.\n\u00ed\n\ud835\udc5b\u2208\ud835\udc41\ud835\udc53\ud835\udc53\ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc53(\ud835\udc5b\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52\u2209{\u2205, \ud835\udc43\ud835\udc38\ud835\udc45\ud835\udc46\ud835\udc42\ud835\udc41})\n\u00ed\n\ud835\udc5b\u2208\ud835\udc41\ud835\udc53\ud835\udc53\ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc53(\ud835\udc5b)\n\u2022 error type-3 unweighted: this is a type of error in which it reports if the entity is not tagged\nat all. notice that even if the entity is tagged as a non-person entity this error type would not\nconsider it.\n\u00ed\n\ud835\udc5b\u2208\ud835\udc41\ud835\udc53\ud835\udc3c(\ud835\udc5b\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52= \u2205)\n|\ud835\udc41\ud835\udc53|\n\u2022 error type-3 weighted: again, this error is similar to its unweighted case with frequency taken\ninto consideration.\n\u00ed\n\ud835\udc5b\u2208\ud835\udc41\ud835\udc53\ud835\udc53\ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc53(\ud835\udc5b\ud835\udc61\ud835\udc66\ud835\udc5d\ud835\udc52= \u2205)\n\u00ed\n\ud835\udc5b\u2208\ud835\udc41\ud835\udc53\ud835\udc53\ud835\udc5f\ud835\udc52\ud835\udc5e\ud835\udc53(\ud835\udc5b)\nauthors also investigate the data that these ner systems are trained on and find that the data is also\nbiased toward female gender by not including as versatile names as there should be to represent\nfemale names.\n5.5\ncomparison of different mitigation algorithms\nthe field of algorithmic fairness is a relatively new area of research and work still needs to be done\nfor its improvement. with that being said, there are already papers that propose fair ai algorithms and\nbias mitigation techniques and compare different mitigation algorithms using different benchmark\ndatasets in the fairness domain. for instance, authors in [65] propose a geometric solution to learn fair\nrepresentations that removes correlation between protected and unprotected features. the proposed\napproach can control the trade-off between fairness and accuracy via an adjustable parameter. in\nthis work, authors evaluate the performance of their approach on different benchmark datasets, such\nas compas, adult and german, and compare them against various different approaches for fair\nlearning algorithms considering fairness and accuracy measures [65, 72, 158, 159]. in addition,\nibm\u2019s ai fairness 360 (aif360) toolkit [11] has implemented many of the current fair learning\nalgorithms and has demonstrated some of the results as demos which can be utilized by interested\nusers to compare different methods with regards to different fairness measures.\n6\nchallenges and opportunities for fairness research\nwhile there have been many definitions of, and approaches to, fairness in the literature, the study\nin this area is anything but complete. fairness and algorithmic bias still holds a number of research\nopportunities. in this section, we provide pointers to outstanding challenges in fairness research, and\nan overview of opportunities for development of understudied problems.\n6.1\nchallenges\nthere are several remaining challenges to be addressed in the fairness literature. among them are:\n(1) synthesizing a definition of fairness. several definitions of what would constitute fairness\nfrom a machine learning perspective have been proposed in the literature. these definitions\ncover a wide range of use cases, and as a result are somewhat disparate in their view of fairness.\nbecause of this, it is nearly impossible to understand how one fairness solution would fare\nunder a different definition of fairness. synthesizing these definitions into one remains an open\nresearch problem since it can make evaluation of these systems more unified and comparable.\na survey on bias and fairness in machine learning\n25\nhaving a more unified fairness definition and framework can also help with the incompatibility\nissue of some current fairness definitions.\n(2) from equality to equity. the definitions presented in the literature mostly focus on equality,\nensuring that each individual or group is given the same amount of resources, attention or\noutcome. however, little attention has been paid to equity, which is the concept that each\nindividual or group is given the resources they need to succeed [60, 103]. operationalizing this\ndefinition and studying how it augments or contradicts existing definitions of fairness remains\nan exciting future direction.\n(3) searching for unfairness. given a definition of fairness, it should be possible to identify\ninstances of this unfairness in a particular dataset. inroads toward this problem have been made\nin the areas of data bias by detecting instances of simpson\u2019s paradox in arbitrary datasets [3];\nhowever, unfairness may require more consideration due to the variety of definitions and the\nnuances in detecting each one.\nfig. 7. heatmap depicting distribution of previous work in fairness, grouped by domain and fairness\ndefinition.\n6.2\nopportunities\nin this work we have taxonomized and summarized the current state of research into algorithmic\nbiases and fairness\u2014with a particular focus on machine learning. even in this area alone, the research\nis broad. subareas, from natural language processing, to representation learning, to community\ndetection, have all seen efforts to make their methodologies more fair. nevertheless, every area\nhas not received the same amount of attention from the research community. figure 7 provides an\noverview of what has been done in different areas to address fairness\u2014categorized by the fairness\ndefinition type and domain. some areas (e.g., community detection at the subgroup level) have\nreceived no attention in the literature, and could be fertile future research areas.\n7",
  "conclusion": "conclusion\nin this survey we introduced problems that can adversely affect ai systems in terms of bias and\nunfairness. the issues were viewed primarily from two dimensions: data and algorithms. we illus-\ntrated problems that demonstrate why fairness is an important issue. we further showed examples of\n26\nmehrabi et al.\nthe potential real-world harm that unfairness can have on society\u2014such as applications in judicial\nsystems, face recognition, and promoting algorithms. we then went over the definitions of fairness\nand bias that have been proposed by researchers. to further stimulate the interest of readers, we\nprovided some of the work done in different areas in terms of addressing the biases that may affect ai\nsystems and different methods and domains in ai, such as general machine learning, deep learning\nand natural language processing. we then further subdivided the fields into a more fine-grained\nanalysis of each subdomain and the work being done to address fairness constraints in each. the\nhope is to expand the horizons of the readers to think deeply while working on a system or a method\nto ensure that it has a low likelihood of causing potential harm or bias toward a particular group.\nwith the expansion of ai use in our world, it is important that researchers take this issue seriously\nand expand their knowledge in this field. in this survey we categorized and created a taxonomy of\nwhat has been done so far to address different issues in different domains regarding the fairness issue.\nother possible future work and directions can be taken to address the existing problems and biases in\nai that we discussed in the previous sections.\n8\nacknowledgments\nthis material is based upon work supported by the defense advanced research projects agency\n(darpa) under agreement no. hr0011890019. we would like to thank the organizers, speakers\nand the attendees at the ivado-mila 2019 summer school on bias and discrimination in ai. we\nwould like to also thank brian hu zhang and shreya shankar.\n9\nappendix\n9.1\ndatasets for fairness research\naside from the existence of bias in datasets, there are datasets that are specifically used to address\nbias and fairness issues in machine learning. there are also some datasets that are introduced to\ntarget the issues and biases previously observed in older existing datasets. below we list some of the\nwidely known datasets that have the characteristics discussed in this survey.\n9.1.1\nuci adult dataset. uci adult dataset, also known as \"census income\" dataset, contains\ninformation, extracted from the 1994 census data about people with attributes such as age, occupation,\neducation, race, sex, marital-status, native-country, hours-per-week etc., indicating whether the\nincome of a person exceeds $50k/yr or not. it can be used in fairness-related studies that want to\ncompare gender or race inequalities based on people\u2019s annual incomes, or various other studies [7].\n9.1.2\ngerman credit dataset. the german credit dataset contains 1000 credit records contain-\ning attributes such as personal status and sex, credit score, credit amount, housing status etc. it can\nbe used in studies about gender inequalities on credit-related issues [47].\n9.1.3\nwinobias. the winobias dataset follows the winograd format and has 40 occupations in\nsentences that are referenced to human pronouns. there are two types of challenge sentences in the\ndataset requiring linkage of gendered pronouns to either male or female stereotypical occupations. it\nwas used in the coreference resolution study to certify if a system has gender bias or not\u2014in this\ncase, towards stereotypical occupations [168].\n9.1.4\ncommunities and crime dataset. the communities and crime dataset gathers infor-\nmation from different communities in the united states related to several factors that can highly\ninfluence some common crimes such as robberies, murders or rapes. the data includes crime data\nobtained from the 1990 us lemas survey and the 1995 fbi unified crime report. it also contains\nsocio-economic data from the 1990 us census.\na survey on bias and fairness in machine learning\n27\n9.1.5\ncompas dataset. the compas dataset contains records for defendants from broward\ncounty indicating their jail and prison times, demographics, criminal histories, and compas risk\nscores from 2013 to 2014 [89].\n9.1.6\nrecidivism in juvenile justice dataset. the recidivism in juvenile justice dataset\ncontains all juvenile offenders between ages 12-17 who committed a crime between years 2002 and\n2010 and completed a prison sentence in 2010 in catalonia\u2019s juvenile justice system [145].\n9.1.7\npilot parliaments benchmark dataset. the pilot parliaments benchmark dataset, also\nknown as ppb, contains images of 1270 individuals in the national parliaments from three european\n(iceland, finland, sweden) and three african (rwanda, senegal, south africa) countries. this\nbenchmark was released to have more gender and race balance, diversity, and representativeness\n[24].\n9.1.8\ndiversity in faces dataset. the diversity in faces (dif) is an image dataset collected for\nfairness research in face recognition. dif is a large dataset containing one million annotations for\nface images. it is also a diverse dataset with diverse facial features, such as different craniofacial\ndistances, skin color, facial symmetry and contrast, age, pose, gender, resolution, along with diverse\nareas and ratios [107].\ndataset name\nreference size\narea\nuci adult dataset\n[7]\n48,842 income records\nsocial\ngerman credit dataset\n[47]\n1,000 credit records\nfinancial\npilot parliaments benchmark dataset\n[24]\n1,270 images\nfacial images\nwinobias\n[168]\n3,160 sentences\ncoreference resolution\ncommunities and crime dataset\n[129]\n1,994 crime records\nsocial\ncompas dataset\n[89]\n18,610 crime records\nsocial\nrecidivism in juvenile justice dataset\n[28]\n4,753 crime records\nsocial\ndiversity in faces dataset\n[107]\n1 million images\nfacial images\ntable 5. most widely used datasets in the fairness domain with additional information about each of\nthe datasets including their size and area of concentration.\nreferences\n[1] alekh agarwal, miroslav dudik, and zhiwei steven wu. 2019. fair regression: quantitative definitions and reduction-\nbased algorithms. in international conference on machine learning. 120\u2013129.\n[2] sina aghaei, mohammad javad azizi, and phebe vayanos. 2019. learning optimal and fair decision trees for non-\ndiscriminative decision-making. in proceedings of the aaai conference on artificial intelligence, vol. 33. 1418\u20131426.\n[3] nazanin alipourfard, peter g fennell, and kristina lerman. 2018. can you trust the trend?: discovering simpson\u2019s\nparadoxes in social data. in proceedings of the eleventh acm international conference on web search and data\nmining. acm, 19\u201327.\n[4] nazanin alipourfard, peter g fennell, and kristina lerman. 2018. using simpson\u2019s paradox to discover interesting\npatterns in behavioral data. in twelfth international aaai conference on web and social media.\n[5] alexander amini, ava soleimany, wilko schwarting, sangeeta bhatia, and daniela rus. 2019. uncovering and\nmitigating algorithmic bias through learned latent structure. (2019).\n[6] julia angwin, jeff larson, surya mattu, and lauren kirchner. 2016. machine bias: there\u2019s software used across the\ncountry to predict future criminals. and it\u2019s biased against blacks. propublica 2016.\n[7] a. asuncion and d.j. newman. 2007. uci machine learning repository.\nhttp://www.ics.uci.edu/$\\sim$mlearn/\n{mlr}epository.html\n[8] arturs backurs, piotr indyk, krzysztof onak, baruch schieber, ali vakilian, and tal wagner. 2019. scalable fair\nclustering. in proceedings of the 36th international conference on machine learning (proceedings of machine\nlearning research, vol. 97), kamalika chaudhuri and ruslan salakhutdinov (eds.). pmlr, long beach, california,\nusa, 405\u2013413. http://proceedings.mlr.press/v97/backurs19a.html\n28\nmehrabi et al.\n[9] ricardo baeza-yates. 2018. bias on the web. commun. acm 61, 6 (may 2018), 54\u201361.\nhttps://doi.org/10.1145/\n3209581\n[10] samuel barbosa, dan cosley, amit sharma, and roberto m. cesar-jr. 2016. averaging gone wrong: using time-\naware analyses to better understand behavior. (april 2016), 829\u2013841.\n[11] rachel ke bellamy, kuntal dey, michael hind, samuel c hoffman, stephanie houde, kalapriya kannan, pranay\nlohia, jacquelyn martino, sameep mehta, aleksandra mojsilovic, et al. 2018. ai fairness 360: an extensible toolkit\nfor detecting, understanding, and mitigating unwanted algorithmic bias. arxiv preprint arxiv:1810.01943 (2018).\n[12] emily m. bender and batya friedman. 2018. data statements for natural language processing: toward mitigating\nsystem bias and enabling better science. transactions of the association for computational linguistics 6 (2018),\n587\u2013604. https://doi.org/10.1162/tacl_a_00041\n[13] misha benjamin, paul gagnon, negar rostamzadeh, chris pal, yoshua bengio, and alex shee. [n.d.]. towards\nstandardization of data licenses: the montreal data license. ([n. d.]).\n[14] richard berk, hoda heidari, shahin jabbari, matthew joseph, michael kearns, jamie morgenstern, seth neel, and\naaron roth. 2017. a convex framework for fair regression. arxiv:1706.02409 [cs.lg]\n[15] richard berk, hoda heidari, shahin jabbari, michael kearns, and aaron roth. [n.d.]. fairness in criminal justice risk\nassessments: the state of the art. sociological methods & research ([n. d.]), 0049124118782533.\n[16] peter j bickel, eugene a hammel, and j william o\u2019connell. 1975. sex bias in graduate admissions: data from\nberkeley. science 187, 4175 (1975), 398\u2013404.\n[17] rdp binns. 2018. fairness in machine learning: lessons from political philosophy. journal of machine learning\nresearch (2018).\n[18] colin r blyth. 1972. on simpson\u2019s paradox and the sure-thing principle. j. amer. statist. assoc. 67, 338 (1972),\n364\u2013366.\n[19] miranda bogen and aaron rieke. 2018. help wanted: an examination of hiring algorithms, equity. technical report.\nand bias. technical report, upturn.\n[20] tolga bolukbasi, kai-wei chang, james y zou, venkatesh saligrama, and adam t kalai. 2016. man is to computer\nprogrammer as woman is to homemaker? debiasing word embeddings. in advances in neural information processing\nsystems. 4349\u20134357.\n[21] shikha bordia and samuel bowman. 2019. identifying and reducing gender bias in word-level language models. in\nproceedings of the 2019 conference of the north american chapter of the association for computational linguistics:\nstudent research workshop. 7\u201315.\n[22] avishek bose and william hamilton. 2019. compositional fairness constraints for graph embeddings. in international\nconference on machine learning. 715\u2013724.\n[23] marc-etienne brunet, colleen alkalay-houlihan, ashton anderson, and richard zemel. 2019. understanding the\norigins of bias in word embeddings. in proceedings of the 36th international conference on machine learning\n(proceedings of machine learning research, vol. 97), kamalika chaudhuri and ruslan salakhutdinov (eds.). pmlr,\nlong beach, california, usa, 803\u2013811. http://proceedings.mlr.press/v97/brunet19a.html\n[24] joy buolamwini and timnit gebru. 2018. gender shades: intersectional accuracy disparities in commercial gender\nclassification. in proceedings of the 1st conference on fairness, accountability and transparency (proceedings of\nmachine learning research, vol. 81), sorelle a. friedler and christo wilson (eds.). pmlr, new york, ny, usa,\n77\u201391. http://proceedings.mlr.press/v81/buolamwini18a.html\n[25] toon calders and sicco verwer. 2010. three naive bayes approaches for discrimination-free classification. data\nmining and knowledge discovery 21, 2 (2010), 277\u2013292.\n[26] aylin caliskan, joanna j bryson, and arvind narayanan. 2017. semantics derived automatically from language corpora\ncontain human-like biases. science 356, 6334 (2017), 183\u2013186.\n[27] flavio calmon, dennis wei, bhanukiran vinzamuri, karthikeyan natesan ramamurthy, and kush r varshney. 2017.\noptimized pre-processing for discrimination prevention. in advances in neural information processing systems 30,\ni. guyon, u. v. luxburg, s. bengio, h. wallach, r. fergus, s. vishwanathan, and r. garnett (eds.). curran associates,\ninc., 3992\u20134001. http://papers.nips.cc/paper/6988-optimized-pre-processing-for-discrimination-prevention.pdf\n[28] manel capdevila, marta ferrer, and eul\u00e1lia luque. 2005. la reincidencia en el delito en la justicia de menores. centro\nde estudios jur\u00eddicos y formaci\u00f3n especializada, generalitat de catalunya. documento no publicado (2005).\n[29] allison jb chaney, brandon m stewart, and barbara e engelhardt. 2018. how algorithmic confounding in recom-\nmendation systems increases homogeneity and decreases utility. in proceedings of the 12th acm conference on\nrecommender systems. acm, 224\u2013232.\n[30] jiahao chen, nathan kallus, xiaojie mao, geoffry svacha, and madeleine udell. 2019. fairness under unawareness:\nassessing disparity when protected class is unobserved. in proceedings of the conference on fairness, accountability,\nand transparency. acm, 339\u2013348.\na survey on bias and fairness in machine learning\n29\n[31] xingyu chen, brandon fain, liang lyu, and kamesh munagala. 2019. proportionally fair clustering. in international\nconference on machine learning. 1032\u20131041.\n[32] s. chiappa. 2019. path-specific counterfactual fairness. in thirty-third aaai conference on artificial intelligence.\n7801\u20137808.\n[33] s. chiappa and w. s. isaac. 2019. a causal bayesian networks viewpoint on fairness. in e. kosta, j. pierson,\nd. slamanig, s. fischer-h\u00fcbner, s. krenn (eds) privacy and identity management. fairness, accountability, and\ntransparency in the age of big data. privacy and identity 2018. ifip advances in information and communication\ntechnology, vol. 547. springer, cham.\n[34] alexandra chouldechova. 2017. fair prediction with disparate impact: a study of bias in recidivism prediction\ninstruments. big data 5, 2 (2017), 153\u2013163.\n[35] alexandra chouldechova, diana benavides-prado, oleksandr fialko, and rhema vaithianathan. 2018. a case study\nof algorithm-assisted decision making in child maltreatment hotline screening decisions. in proceedings of the 1st\nconference on fairness, accountability and transparency (proceedings of machine learning research, vol. 81),\nsorelle a. friedler and christo wilson (eds.). pmlr, new york, ny, usa, 134\u2013148. http://proceedings.mlr.press/\nv81/chouldechova18a.html\n[36] alexandra chouldechova and aaron roth. 2018. the frontiers of fairness in machine learning. arxiv preprint\narxiv:1810.08810 (2018).\n[37] john s chuang, olivier rivoire, and stanislas leibler. 2009. simpson\u2019s paradox in a synthetic microbial system.\nscience 323, 5911 (2009), 272\u2013275.\n[38] kevin a clarke. 2005. the phantom menace: omitted variable bias in econometric research. conflict management and\npeace science 22, 4 (2005), 341\u2013352.\n[39] lee cohen, zachary c. lipton, and yishay mansour. 2019. efficient candidate screening under multiple tests and\nimplications for fairness. arxiv:1905.11361 [cs.lg]\n[40] united states. equal employment opportunity commission. [n.d.]. eeoc compliance manual. [washington, d.c.] :\nu.s. equal employment opportunity commission, [1992].\n[41] sam corbett-davies, emma pierson, avi feller, sharad goel, and aziz huq. 2017. algorithmic decision making and\nthe cost of fairness. in proceedings of the 23rd acm sigkdd international conference on knowledge discovery and\ndata mining. acm, 797\u2013806.\n[42] elliot creager, david madras, joern-henrik jacobsen, marissa weis, kevin swersky, toniann pitassi, and richard\nzemel. 2019. flexibly fair representation learning by disentanglement. in international conference on machine\nlearning. 1436\u20131445.\n[43] brian d\u2019alessandro, cathy o\u2019neil, and tom lagatta. 2017. conscientious classification: a data scientist\u2019s guide to\ndiscrimination-aware classification. big data 5, 2 (2017), 120\u2013134.\n[44] david danks and alex john london. 2017. algorithmic bias in autonomous systems.. in ijcai. 4691\u20134697.\n[45] shai danziger, jonathan levav, and liora avnaim-pesso. 2011. extraneous factors in judicial decisions. proceedings\nof the national academy of sciences 108, 17 (2011), 6889\u20136892.\n[46] julia dressel and hany farid. 2018. the accuracy, fairness, and limits of predicting recidivism. science advances 4, 1\n(2018). https://doi.org/10.1126/sciadv.aao5580 arxiv:https://advances.sciencemag.org/content/4/1/eaao5580.full.pdf\n[47] dheeru dua and casey graff. 2017. uci machine learning repository. http://archive.ics.uci.edu/ml\n[48] cynthia dwork, moritz hardt, toniann pitassi, omer reingold, and richard zemel. 2012. fairness through awareness.\nin proceedings of the 3rd innovations in theoretical computer science conference (cambridge, massachusetts) (itcs\n\u201912). acm, new york, ny, usa, 214\u2013226. https://doi.org/10.1145/2090236.2090255\n[49] cynthia dwork, nicole immorlica, adam tauman kalai, and max leiserson. 2018. decoupled classifiers for\ngroup-fair and efficient machine learning. in proceedings of the 1st conference on fairness, accountability and\ntransparency (proceedings of machine learning research, vol. 81), sorelle a. friedler and christo wilson (eds.).\npmlr, new york, ny, usa, 119\u2013133. http://proceedings.mlr.press/v81/dwork18a.html\n[50] golnoosh farnadi, behrouz babaki, and lise getoor. 2018. fairness in relational domains. in proceedings of the\n2018 aaai/acm conference on ai, ethics, and society (new orleans, la, usa) (aies \u201918). acm, new york, ny,\nusa, 108\u2013114. https://doi.org/10.1145/3278721.3278733\n[51] michael feldman, sorelle a. friedler, john moeller, carlos scheidegger, and suresh venkatasubramanian. 2015.\ncertifying and removing disparate impact. in proceedings of the 21th acm sigkdd international conference on\nknowledge discovery and data mining (sydney, nsw, australia) (kdd \u201915). association for computing machinery,\nnew york, ny, usa, 259\u2013268. https://doi.org/10.1145/2783258.2783311\n[52] joel escud\u00e9 font and marta r costa-juss\u00e0. 2019. equalizing gender biases in neural machine translation with word\nembeddings techniques. arxiv preprint arxiv:1901.03116 (2019).\n[53] batya friedman and helen nissenbaum. 1996. bias in computer systems. acm trans. inf. syst. 14, 3 (july 1996),\n330\u2013347. https://doi.org/10.1145/230538.230561\n30\nmehrabi et al.\n[54] anna fry, thomas j littlejohns, cathie sudlow, nicola doherty, ligia adamska, tim sprosen, rory collins, and\nnaomi e allen. 2017. comparison of sociodemographic and health-related characteristics of uk biobank participants\nwith those of the general population. american journal of epidemiology 186, 9 (06 2017), 1026\u20131034.\nhttps:\n//doi.org/10.1093/aje/kwx246 arxiv:http://oup.prod.sis.lan/aje/article-pdf/186/9/1026/24330720/kwx246.pdf\n[55] timnit gebru, jamie morgenstern, briana vecchione, jennifer wortman vaughan, hanna wallach, hal daum\u00e9 iii, and\nkate crawford. [n.d.]. datasheets for datasets. ([n. d.]).\n[56] c. e. gehlke and katherine biehl. 1934. certain effects of grouping upon the size of the correlation coefficient in\ncensus tract material. j. amer. statist. assoc. 29, 185a (1934), 169\u2013170. https://doi.org/10.2307/2277827\n[57] naman goel, mohammad yaghini, and boi faltings. 2018. non-discriminatory machine learning through convex\nfairness criteria. in thirty-second aaai conference on artificial intelligence.\n[58] hila gonen and yoav goldberg. 2019. lipstick on a pig: debiasing methods cover up systematic gender biases in\nword embeddings but do not remove them. arxiv preprint arxiv:1903.03862 (2019).\n[59] sandra gonz\u00e1lez-bail\u00f3n, ning wang, alejandro rivero, javier borge-holthoefer, and yamir moreno. 2014. assessing\nthe bias in samples of large online networks. social networks 38 (2014), 16\u201327.\n[60] susan t gooden. 2015. race and social equity: a nervous area of government. routledge.\n[61] nina grgic-hlaca, muhammad bilal zafar, krishna p gummadi, and adrian weller. 2016. the case for process\nfairness in learning: feature selection for fair decision making. in nips symposium on machine learning and the law,\nvol. 1. 2.\n[62] s. hajian and j. domingo-ferrer. 2013.\na methodology for direct and indirect discrimination prevention in\ndata mining. ieee transactions on knowledge and data engineering 25, 7 (july 2013), 1445\u20131459.\nhttps:\n//doi.org/10.1109/tkde.2012.72\n[63] moritz hardt, eric price, nati srebro, et al. 2016. equality of opportunity in supervised learning. in advances in neural\ninformation processing systems. 3315\u20133323.\n[64] eszter hargittai. 2007. whose space? differences among users and non-users of social network sites. journal of\ncomputer-mediated communication 13, 1 (10 2007), 276\u2013297. https://doi.org/10.1111/j.1083-6101.2007.00396.x\narxiv:http://oup.prod.sis.lan/jcmc/article-pdf/13/1/276/22317170/jjcmcom0276.pdf\n[65] yuzi he, keith burghardt, and kristina lerman. 2020. a geometric solution to fair representations. in proceedings\nof the aaai/acm conference on ai, ethics, and society. 279\u2013285.\n[66] sarah holland, ahmed hosny, sarah newman, joshua joseph, and kasia chmielinski. 2018. the dataset nutrition\nlabel: a framework to drive higher data quality standards. arxiv preprint arxiv:1805.03677 (2018).\n[67] ayanna howard and jason borenstein. 2018. the ugly truth about ourselves and our robot creations: the problem of\nbias and social inequity. science and engineering ethics 24, 5 (2018), 1521\u20131536.\n[68] gary b huang, marwan mattar, tamara berg, and eric learned-miller. 2008. labeled faces in the wild: a database\nforstudying face recognition in unconstrained environments.\n[69] lingxiao huang and nisheeth vishnoi. 2019. stable and fair classification. in international conference on machine\nlearning. 2879\u20132890.\n[70] ben hutchinson and margaret mitchell. 2019. 50 years of test (un) fairness: lessons for machine learning. in\nproceedings of the conference on fairness, accountability, and transparency. acm, 49\u201358.\n[71] l. introna and h. nissenbaum. 2000. defining the web: the politics of search engines. computer 33, 1 (jan 2000),\n54\u201362. https://doi.org/10.1109/2.816269\n[72] ayush jaiswal, yue wu, wael abdalmageed, and premkumar natarajan. 2018. unsupervised adversarial invariance.\narxiv:1809.10083 [cs.lg]\n[73] ray jiang, aldo pacchiano, tom stepleton, heinrich jiang, and silvia chiappa. [n.d.]. wasserstein fair classification.\n([n. d.]).\n[74] f. kamiran and t. calders. 2009. classifying without discriminating. in 2009 2nd international conference on\ncomputer, control and communication. 1\u20136. https://doi.org/10.1109/ic4.2009.4909197\n[75] faisal kamiran and toon calders. 2010. classification with no discrimination by preferential sampling. in proc. 19th\nmachine learning conf. belgium and the netherlands. citeseer, 1\u20136.\n[76] faisal kamiran and toon calders. 2012. data preprocessing techniques for classification without discrimination.\nknowledge and information systems 33, 1 (01 oct 2012), 1\u201333. https://doi.org/10.1007/s10115-011-0463-8\n[77] faisal kamiran and indr\u02d9e \u017eliobait\u02d9e. 2013. explainable and non-explainable discrimination in classification. springer\nberlin heidelberg, berlin, heidelberg, 155\u2013170. https://doi.org/10.1007/978-3-642-30487-3_8\n[78] toshihiro kamishima, shotaro akaho, hideki asoh, and jun sakuma. 2012. fairness-aware classifier with prejudice\nremover regularizer. in joint european conference on machine learning and knowledge discovery in databases.\nspringer, 35\u201350.\n[79] michael kearns, seth neel, aaron roth, and zhiwei steven wu. 2018. preventing fairness gerrymandering: auditing\nand learning for subgroup fairness. in international conference on machine learning. 2569\u20132577.\na survey on bias and fairness in machine learning\n31\n[80] michael kearns, seth neel, aaron roth, and zhiwei steven wu. 2019. an empirical study of rich subgroup fairness for\nmachine learning. in proceedings of the conference on fairness, accountability, and transparency. acm, 100\u2013109.\n[81] rogier kievit, willem eduard frankenhuis, lourens waldorp, and denny borsboom. 2013. simpson\u2019s paradox in\npsychological science: a practical guide. frontiers in psychology 4 (2013), 513.\n[82] niki kilbertus, mateo rojas carulla, giambattista parascandolo, moritz hardt, dominik janzing, and bernhard\nsch\u00f6lkopf. 2017. avoiding discrimination through causal reasoning. in advances in neural information processing\nsystems. 656\u2013666.\n[83] jon kleinberg, sendhil mullainathan, and manish raghavan. 2016. inherent trade-offs in the fair determination of risk\nscores. arxiv preprint arxiv:1609.05807 (2016).\n[84] philipp koehn. 2005. europarl: a parallel corpus for statistical machine translation. in mt summit, vol. 5. 79\u201386.\n[85] emmanouil krasanakis, eleftherios spyromitros-xioufis, symeon papadopoulos, and yiannis kompatsiaris. 2018.\nadaptive sensitive reweighting to mitigate bias in fairness-aware classification. in proceedings of the 2018 world\nwide web conference (lyon, france) (www \u201918). international world wide web conferences steering committee,\nrepublic and canton of geneva, switzerland, 853\u2013862. https://doi.org/10.1145/3178876.3186133\n[86] ivan krasin, tom duerig, neil alldrin, vittorio ferrari, sami abu-el-haija, alina kuznetsova, hassan rom, jasper\nuijlings, stefan popov, andreas veit, et al. 2017. openimages: a public dataset for large-scale multi-label and\nmulti-class image classification. dataset available from https://github. com/openimages 2, 3 (2017), 2\u20133.\n[87] matt j kusner, joshua loftus, chris russell, and ricardo silva. 2017. counterfactual fairness. in advances in neural\ninformation processing systems 30, i. guyon, u. v. luxburg, s. bengio, h. wallach, r. fergus, s. vishwanathan, and\nr. garnett (eds.). curran associates, inc., 4066\u20134076. http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf\n[88] anja lambrecht and catherine e tucker. 2018. algorithmic bias? an empirical study into apparent gender-based\ndiscrimination in the display of stem career ads. an empirical study into apparent gender-based discrimination in\nthe display of stem career ads (march 9, 2018) (2018).\n[89] j larson, s mattu, l kirchner, and j angwin. 2016.\ncompas analysis.\ngithub, available at: https://github.\ncom/propublica/compas-analysis[google scholar] (2016).\n[90] blake lemoine, brian zhang, and m mitchell. 2018. mitigating unwanted biases with adversarial learning. (2018).\n[91] kristina lerman. 2018. computational social scientist beware: simpson\u2019s paradox in behavioral data. journal of\ncomputational social science 1, 1 (2018), 49\u201358.\n[92] kristina lerman and tad hogg. 2014. leveraging position bias to improve peer recommendation. plos one 9, 6\n(2014), e98914. http://www.plosone.org/article/info%3adoi%2f10.1371%2fjournal.pone.0098914\n[93] kristina lerman and tad hogg. 2014. leveraging position bias to improve peer recommendation. plos one 9, 6 (2014),\ne98914.\n[94] zachary c lipton, alexandra chouldechova, and julian mcauley. 2017. does mitigating ml\u2019s disparate impact require\ndisparate treatment? stat 1050 (2017), 19.\n[95] lydia t liu, sarah dean, esther rolf, max simchowitz, and moritz hardt. 2018. delayed impact of fair machine\nlearning. in proceedings of the 35th international conference on machine learning.\n[96] joshua r loftus, chris russell, matt j kusner, and ricardo silva. 2018. causal reasoning for algorithmic fairness.\narxiv preprint arxiv:1805.05859 (2018).\n[97] christos louizos, kevin swersky, yujia li, max welling, and richard zemel. 2016. the variational fair\nautoencoder. stat 1050 (2016), 4.\n[98] arjun k. manrai, birgit h. funke, heidi l. rehm, morten s. olesen, bradley a. maron, peter szolovits, david m.\nmargulies, joseph loscalzo, and isaac s. kohane. 2016. genetic misdiagnoses and the potential for health dis-\nparities. new england journal of medicine 375, 7 (2016), 655\u2013665.\nhttps://doi.org/10.1056/nejmsa1507092\narxiv:https://doi.org/10.1056/nejmsa1507092 pmid: 27532831.\n[99] ray marshall. 1974. the economics of racial discrimination: a survey. journal of economic literature 12, 3 (1974),\n849\u2013871.\n[100] chandler may, alex wang, shikha bordia, samuel r bowman, and rachel rudinger. 2019. on measuring social\nbiases in sentence encoders. arxiv preprint arxiv:1903.10561 (2019).\n[101] ninareh mehrabi, thamme gowda, fred morstatter, nanyun peng, and aram galstyan. 2019. man is to person as\nwoman is to location: measuring gender bias in named entity recognition. arxiv preprint arxiv:1910.10872 (2019).\n[102] ninareh mehrabi, umang gupta, fred morstatter, greg ver steeg, and aram galstyan. 2021. attributing fair decisions\nwith attention interventions. arxiv preprint arxiv:2109.03952 (2021).\n[103] ninareh mehrabi, yuzhong huang, and fred morstatter. 2020. statistical equity: a fairness classification objective.\narxiv preprint arxiv:2005.07293 (2020).\n[104] ninareh mehrabi, fred morstatter, nanyun peng, and aram galstyan. 2019. debiasing community detection: the\nimportance of lowly-connected nodes. arxiv preprint arxiv:1903.08136 (2019).\n32\nmehrabi et al.\n[105] ninareh mehrabi, pei zhou, fred morstatter, jay pujara, xiang ren, and aram galstyan. 2021. lawyers are dishonest?\nquantifying representational harms in commonsense knowledge resources. in proceedings of the 2021 conference\non empirical methods in natural language processing. association for computational linguistics, online and punta\ncana, dominican republic, 5016\u20135033. https://doi.org/10.18653/v1/2021.emnlp-main.410\n[106] aditya krishna menon and robert c williamson. 2018. the cost of fairness in binary classification. in proceedings\nof the 1st conference on fairness, accountability and transparency (proceedings of machine learning research,\nvol. 81), sorelle a. friedler and christo wilson (eds.). pmlr, new york, ny, usa, 107\u2013118. http://proceedings.mlr.\npress/v81/menon18a.html\n[107] michele merler, nalini ratha, rogerio s feris, and john r smith. 2019.\ndiversity in faces.\narxiv preprint\narxiv:1901.10436 (2019).\n[108] hannah jean miller, jacob thebault-spieker, shuo chang, isaac johnson, loren terveen, and brent hecht. 2016.\n\u201cblissfully happy\u201d or \u201cready tofight\u201d: varying interpretations of emoji. in tenth international aaai conference on\nweb and social media.\n[109] i minchev, g matijevic, dw hogg, g guiglion, m steinmetz, f anders, c chiappini, m martig, a queiroz, and c\nscannapieco. 2019. yule-simpson\u2019s paradox in galactic archaeology. arxiv preprint arxiv:1902.01421 (2019).\n[110] margaret mitchell, simone wu, andrew zaldivar, parker barnes, lucy vasserman, ben hutchinson, elena spitzer,\ninioluwa deborah raji, and timnit gebru. 2019. model cards for model reporting. in proceedings of the conference\non fairness, accountability, and transparency (atlanta, ga, usa) (fat* \u201919). acm, new york, ny, usa, 220\u2013229.\nhttps://doi.org/10.1145/3287560.3287596\n[111] fred morstatter, j\u00fcrgen pfeffer, huan liu, and kathleen m carley. 2013. is the sample good enough? comparing data\nfrom twitter\u2019s streaming api with twitter\u2019s firehose. in 7th international aaai conference on weblogs and social\nmedia, icwsm 2013. aaai press.\n[112] daniel moyer, shuyang gao, rob brekelmans, aram galstyan, and greg ver steeg. 2018. invariant representations\nwithout adversarial training. in advances in neural information processing systems. 9084\u20139093.\n[113] amitabha mukerjee, rita biswas, kalyanmoy deb, and amrit p mathur. 2002. multi\u2013objective evolutionary algorithms\nfor the risk\u2013return trade\u2013off in bank loan management. international transactions in operational research 9, 5 (2002),\n583\u2013597.\n[114] david b mustard. 2003. reexamining criminal behavior: the importance of omitted variable bias. review of economics\nand statistics 85, 1 (2003), 205\u2013211.\n[115] razieh nabi, daniel malinsky, and ilya shpitser. 2018.\nlearning optimal fair policies.\narxiv preprint\narxiv:1809.02244 (2018).\n[116] razieh nabi and ilya shpitser. 2018. fair inference on outcomes. in thirty-second aaai conference on artificial\nintelligence.\n[117] azadeh nematzadeh, giovanni luca ciampaglia, filippo menczer, and alessandro flammini. 2017. how algorithmic\npopularity bias hinders or promotes quality. arxiv preprint arxiv:1707.00574 (2017).\n[118] dong-phuong nguyen, rilana gravel, rudolf berend trieschnigg, and theo meder. 2013. \"how old do you think\ni am?\": a study of language and age in twitter. in proceedings of the seventh international aaai conference on\nweblogs and social media, icwsm 2013. aaai press, 439\u2013448. eemcs-eprint-23604.\n[119] anne o\u2019keeffe and michael mccarthy. 2010. the routledge handbook of corpus linguistics. routledge.\n[120] alexandra olteanu, carlos castillo, fernando diaz, and emre kiciman. 2016. social data: biases, methodological\npitfalls, and ethical boundaries. (2016).\n[121] cathy o\u2019neil. 2016. weapons of math destruction: how big data increases inequality and threatens democracy.\ncrown publishing group, new york, ny, usa.\n[122] luca oneto, michele doninini, amon elders, and massimiliano pontil. 2019. taking advantage of multitask learning\nfor fair classification. in proceedings of the 2019 aaai/acm conference on ai, ethics, and society. 227\u2013237.\n[123] osonde a osoba and william welser iv. 2017. an intelligence in our image: the risks of bias and errors in artificial\nintelligence. rand corporation.\n[124] edmund s phelps. 1972. the statistical theory of racism and sexism. the american economic review 62, 4 (1972),\n659\u2013661.\n[125] geoff pleiss, manish raghavan, felix wu, jon kleinberg, and kilian q weinberger. 2017.\non fairness and\ncalibration.\nin advances in neural information processing systems 30, i. guyon, u. v. luxburg, s. bengio,\nh. wallach, r. fergus, s. vishwanathan, and r. garnett (eds.). curran associates, inc., 5680\u20135689.\nhttp:\n//papers.nips.cc/paper/7151-on-fairness-and-calibration.pdf\n[126] marcelo or prates, pedro h avelar, and lu\u00eds c lamb. 2018. assessing gender bias in machine translation: a case\nstudy with google translate. neural computing and applications (2018), 1\u201319.\n[127] bilal qureshi, faisal kamiran, asim karim, and salvatore ruggieri. 2016. causal discrimination discovery through\npropensity score analysis. arxiv preprint arxiv:1608.03735 (2016).\na survey on bias and fairness in machine learning\n33\n[128] inioluwa deborah raji and joy buolamwini. 2019. actionable auditing: investigating the impact of publicly naming\nbiased performance results of commercial ai products.\n[129] m redmond. 2011. communities and crime unnormalized data set. uci machine learning repository. in website:\nhttp://www. ics. uci. edu/mlearn/mlrepository. html (2011).\n[130] willy e rice. 1996. race, gender, redlining, and the discriminatory access to loans, credit, and insurance:\nan historical and empirical analysis of consumers who sued lenders and insurers in federal and state courts,\n1950-1995. san diego l. rev. 33 (1996), 583.\n[131] stephanie k riegg. 2008. causal inference and omitted variable bias in financial aid research: assessing solutions.\nthe review of higher education 31, 3 (2008), 329\u2013354.\n[132] lauren a rivera. 2012. hiring as cultural matching: the case of elite professional service firms. american sociological\nreview 77, 6 (2012), 999\u20131022.\n[133] andrea romei and salvatore ruggieri. 2011. a multidisciplinary survey on discrimination analysis.\n[134] rachel rudinger, jason naradowsky, brian leonard, and benjamin van durme. 2018. gender bias in coreference\nresolution. in proceedings of the 2018 conference of the north american chapter of the association for computational\nlinguistics: human language technologies, volume 2 (short papers). association for computational linguistics, new\norleans, louisiana, 8\u201314. https://doi.org/10.18653/v1/n18-2002\n[135] olga russakovsky, jia deng, hao su, jonathan krause, sanjeev satheesh, sean ma, zhiheng huang, andrej karpathy,\naditya khosla, michael bernstein, et al. 2015. imagenet large scale visual recognition challenge. international journal\nof computer vision 115, 3 (2015), 211\u2013252.\n[136] pedro saleiro, benedict kuester, abby stevens, ari anisfeld, loren hinkson, jesse london, and rayid ghani. 2018.\naequitas: a bias and fairness audit toolkit. arxiv preprint arxiv:1811.05577 (2018).\n[137] samira samadi, uthaipon tantipongpipat, jamie morgenstern, mohit singh, and santosh vempala. 2018. the price\nof fair pca: one extra dimension. in proceedings of the 32nd international conference on neural information\nprocessing systems (montr&#233;al, canada) (nips\u201918). curran associates inc., usa, 10999\u201311010. http://dl.acm.\norg/citation.cfm?id=3327546.3327755\n[138] nripsuta ani saxena. 2019. perceptions of fairness. in proceedings of the 2019 aaai/acm conference on ai, ethics,\nand society (honolulu, hi, usa) (aies \u201919). acm, new york, ny, usa, 537\u2013538. https://doi.org/10.1145/3306618.\n3314314\n[139] nripsuta ani saxena, karen huang, evan defilippis, goran radanovic, david c parkes, and yang liu. 2019. how do\nfairness definitions fare?: examining public attitudes towards algorithmic definitions of fairness. in proceedings of\nthe 2019 aaai/acm conference on ai, ethics, and society. acm, 99\u2013106.\n[140] tobias schnabel, adith swaminathan, ashudeep singh, navin chandak, and thorsten joachims. 2016. recom-\nmendations as treatments: debiasing learning and evaluation. in international conference on machine learning.\n1670\u20131679.\n[141] andrew d selbst, danah boyd, sorelle a friedler, suresh venkatasubramanian, and janet vertesi. 2019. fairness and\nabstraction in sociotechnical systems. in proceedings of the conference on fairness, accountability, and transparency.\nacm, 59\u201368.\n[142] shreya shankar, yoni halpern, eric breck, james atwood, jimbo wilson, and d sculley. 2017. no classification\nwithout representation: assessing geodiversity issues in open data sets for the developing world. stat 1050 (2017),\n22.\n[143] richard shaw and manuel corpas. [n.d.]. further bias in personal genomics? ([n. d.]).\n[144] harini suresh and john v guttag. 2019. a framework for understanding unintended consequences of machine\nlearning. arxiv preprint arxiv:1901.10002 (2019).\n[145] song\u00fcl tolan, marius miron, emilia g\u00f3mez, and carlos castillo. 2019. why machine learning may lead to\nunfairness: evidence from risk assessment for juvenile justice in catalonia. (2019).\n[146] zeynep tufekci. 2014. big questions for social media big data: representativeness, validity and other methodological\npitfalls. in eighth international aaai conference on weblogs and social media.\n[147] berk ustun, yang liu, and david parkes. 2019. fairness without harm: decoupled classifiers with preference\nguarantees. in proceedings of the 36th international conference on machine learning (proceedings of machine\nlearning research, vol. 97), kamalika chaudhuri and ruslan salakhutdinov (eds.). pmlr, long beach, california,\nusa, 6373\u20136382. http://proceedings.mlr.press/v97/ustun19a.html\n[148] eva vanmassenhove, christian hardmeier, and andy way. 2018. getting gender right in neural machine translation. in\nproceedings of the 2018 conference on empirical methods in natural language processing. 3003\u20133008.\n[149] sahil verma and julia rubin. 2018. fairness definitions explained. in 2018 ieee/acm international workshop on\nsoftware fairness (fairware). ieee, 1\u20137.\n[150] selwyn vickers, mona fouad, and moon s chen jr. 2014. enhancing minority participation in clinical trials\n(empact): laying the groundwork for improving minority clinical trial accrual. cancer 120 (2014), vi\u2013vii.\n34\nmehrabi et al.\n[151] ting wang and dashun wang. 2014. why amazon\u2019s ratings might mislead you: the story of herding effects. big data\n2, 4 (2014), 196\u2013204.\n[152] steven l willborn. 1984. the disparate impact model of discrimination: theory and limits. am. ul rev. 34 (1984),\n799.\n[153] christo wilson, bryce boe, alessandra sala, krishna pn puttaswamy, and ben y zhao. 2009. user interactions in\nsocial networks and their implications. in proceedings of the 4th acm european conference on computer systems.\nacm, 205\u2013218.\n[154] blake woodworth, suriya gunasekar, mesrob i ohannessian, and nathan srebro. 2017. learning non-discriminatory\npredictors. arxiv preprint arxiv:1702.06081 (2017).\n[155] yongkai wu, lu zhang, and xintao wu. 2018. fairness-aware classification: criterion, convexity, and bounds.\narxiv:1809.04737 [cs.lg]\n[156] depeng xu, shuhan yuan, lu zhang, and xintao wu. 2018. fairgan: fairness-aware generative adversarial networks.\nin 2018 ieee international conference on big data (big data). ieee, 570\u2013575.\n[157] irene y chen, peter szolovits, and marzyeh ghassemi. 2019. can ai help reduce disparities in general medical and\nmental health care? ama journal of ethics 21 (02 2019), e167\u2013179. https://doi.org/10.1001/amajethics.2019.167\n[158] muhammad bilal zafar, isabel valera, manuel gomez rodriguez, and krishna p gummadi. 2017. fairness beyond\ndisparate treatment & disparate impact: learning classification without disparate mistreatment. in proceedings of the\n26th international conference on world wide web. 1171\u20131180.\n[159] muhammad bilal zafar, isabel valera, manuel gomez rodriguez, and krishna p gummadi. 2015. fairness constraints:\nmechanisms for fair classification. arxiv preprint arxiv:1507.05259 (2015).\n[160] lu zhang and xintao wu. 2017. anti-discrimination learning: a causal modeling-based framework. international\njournal of data science and analytics 4, 1 (01 aug 2017), 1\u201316. https://doi.org/10.1007/s41060-017-0058-x\n[161] lu zhang, yongkai wu, and xintao wu. 2016. on discrimination discovery using causal networks. in social,\ncultural, and behavioral modeling, kevin s. xu, david reitter, dongwon lee, and nathaniel osgood (eds.). springer\ninternational publishing, cham, 83\u201393.\n[162] lu zhang, yongkai wu, and xintao wu. 2016. situation testing-based discrimination discovery: a causal inference\napproach. in proceedings of the twenty-fifth international joint conference on artificial intelligence (new york, new\nyork, usa) (ijcai\u201916). aaai press, 2718\u20132724. http://dl.acm.org/citation.cfm?id=3060832.3061001\n[163] lu zhang, yongkai wu, and xintao wu. 2017. achieving non-discrimination in data release. in proceedings of the\n23rd acm sigkdd international conference on knowledge discovery and data mining. acm, 1335\u20131344.\n[164] lu zhang, yongkai wu, and xintao wu. 2017. a causal framework for discovering and removing direct and indirect\ndiscrimination. in proceedings of the twenty-sixth international joint conference on artificial intelligence, ijcai-17.\n3929\u20133935. https://doi.org/10.24963/ijcai.2017/549\n[165] l. zhang, y. wu, and x. wu. 2018. causal modeling-based discrimination discovery and removal: criteria, bounds,\nand algorithms. ieee transactions on knowledge and data engineering (2018), 1\u20131. https://doi.org/10.1109/tkde.\n2018.2872988\n[166] jieyu zhao, tianlu wang, mark yatskar, ryan cotterell, vicente ordonez, and kai-wei chang. 2019. gender bias\nin contextualized word embeddings. in proceedings of the 2019 conference of the north american chapter of\nthe association for computational linguistics: human language technologies, volume 1 (long and short papers).\n629\u2013634.\n[167] jieyu zhao, tianlu wang, mark yatskar, vicente ordonez, and kai-wei chang. 2017. men also like shopping:\nreducing gender bias amplification using corpus-level constraints. in proceedings of the 2017 conference on\nempirical methods in natural language processing.\n[168] jieyu zhao, tianlu wang, mark yatskar, vicente ordonez, and kai-wei chang. 2018. gender bias in coreference\nresolution: evaluation and debiasing methods. arxiv:1804.06876 [cs.cl]\n[169] jieyu zhao, yichao zhou, zeyu li, wei wang, and kai-wei chang. 2018. learning gender-neutral word embeddings.\nin proceedings of the 2018 conference on empirical methods in natural language processing. 4847\u20134853.\n[170] james zou and londa schiebinger. 2018. ai can be sexist and racist it\u2019s time to make it fair. nature publishing group."
}