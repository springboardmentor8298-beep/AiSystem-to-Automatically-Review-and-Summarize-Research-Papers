{
  "paper_1.pdf": {
    "introduction": "introduction of generative AI \nto healthcare. These characteristics are unique to various \njurisdictions and continue to evolve rapidly, therefore are \nconsidered beyond the scope of this article.\nFirst component: acceptance and\u00a0adoption\nThe successful implementation of AI in healthcare hinges \non the understanding and acceptance of its applications \nby end users [54], including medical professionals and \npatients. This comprehension fosters trust in AI systems, \nenables their effective use and aids in navigating ethi-\ncal and regulatory challenges. Moreover, a solid grasp of \nAI promotes continuous learning and adaptation to the \nevolving landscape of AI technology. Therefore, invest-\nment in improving awareness for all partners is crucial \nto ensure the effective adoption and utilisation of AI in \nhealthcare.\nUtilising the TAM and NASSS frameworks to the \nimplementation generative AI in healthcare involves con-\nsideration of the following components:\n\u25aa Perceived usefulness: This refers to the degree to \nwhich a person believes that using a particular sys-\ntem would enhance his or her job performance. In \nthe context of generative AI in healthcare, this could \nbe how the AI can help in diagnosing diseases, pre-\ndicting patient outcomes, personalizing treatment \nplans and improving administrative efficiency. For \ninstance, AI could generate predictive models for \npatient outcomes based on their medical history, cur-\nrent health status and a vast database of similar cases.\n\u25aa Perceived ease of use: This refers to the degree to \nwhich a person believes that using a particular sys-\ntem would be free of effort. For generative AI in \nhealthcare, this could mean how easy it is for health-\ncare professionals to understand and use the AI sys-\ntem. This includes the user interface, the clarity of the \nAI\u2019s outputs and the level of technical support avail-\nable.\n\u25aa Attitude towards using: The value proposition of \ngenerative AI in healthcare is compelling, offering \nbenefits like cost-effectiveness, speed and personal-\nized treatment options [5]. If healthcare professionals \nperceive the AI system as useful and easy to use, they \nare likely to develop a positive attitude towards using \nit. This positive attitude could be further enhanced \nby providing adequate training and support and by \ndemonstrating the successful use of AI in similar \nhealthcare settings.\n\u25aa Behavioural intention to use: Once healthcare pro-\nfessionals have a positive attitude towards the AI \nsystem, they are more likely to intend to use it. This \nintention could be turned into actual use by provid-\ning opportunities to use the AI system in a safe and \nsupportive environment and by integrating the AI \nsystem into existing workflows.\n\u25aa Actual system use: The final step is the actual use of \nthe AI system in daily healthcare practice. This could \nbe encouraged by providing ongoing support and by \ncontinuously monitoring and improving the AI sys-\ntem based on user feedback and performance data.\nIn addition to these factors, the model also suggests \nthat external factors like social influence and facilitat-\ning conditions can influence the acceptance and use of \na new technology [57, 59]. In the case of generative AI \nin healthcare, these could include regulatory approval, \nethical considerations, patient acceptance and the overall \nhealthcare policy and economic environment.\nSecond component: data and\u00a0resources\nAdopting generative AI involves preparing data and \nresources within an organisation to effectively utilise this \ntechnology. This is a complex process requiring a sys-\ntematic and strategic approach that involves several key \nsteps.\n\u25aa Identifying use cases: Healthcare organisations need \nto begin by identifying the specific use cases where \ngenerative AI can bring value. Generative AI aims \nto address various medical conditions, from chronic \ndiseases like diabetes to acute conditions like stroke \n[6, 38, 60]. The complexity of the medical condition \noften dictates the level of sophistication required \nfrom the AI model. For instance, using AI for diag-\nnostic imaging in cancer is complex and requires \nhigh accuracy. Understanding the specific use cases \nwill help guide the data preparation process.\n\u25aa Data collection: Generative AI models learn from \ndata [8], so the healthcare organisation needs to col-\nlect and prepare relevant data for training the mod-\nels. This could involve gathering existing primary \ndata from various sources within the organisation or \ncollecting new data if necessary. The data then needs \nto be cleaned and preprocessed, which may involve \ntasks such as removing duplicates, handling missing \nvalues and normalizing data.\n\u25aa Data cleaning and preprocessing: It is neces-\nsary to clean and preprocess the collected data to \nensure its quality and consistency [61, 62]. This \nPage 10 of 15\nReddy \ufeffImplementation Science           (2024) 19:27 \nmay involve removing duplicates, handling missing \nvalues, standardizing formats and addressing any \nother data quality issues. Preprocessing steps may \nalso include data normalization, feature scaling and \ndata augmentation techniques to enhance the train-\ning process. It is important to highlight the need for \nuniformity in the quality of the datasets to enable \nseamless cross-functional data integration. Also, \ndata quality is crucial as generative AI algorithms \nlearn from data. The quality of data can be affected \nby various factors such as noise, missing values, \noutliers, biased data, lack of balance in distribu-\ntion, inconsistency, redundancy, heterogeneity, data \nduplication and integration.\n\u25aa Data annotation and labelling: Depending on the \nuse case, the organisation may need to annotate and \nlabel the data to provide ground truth and clinical \nstandard information for training the generative AI \nmodels, specifically for fine-tuning LLMs with local \ndata [10]. This could involve tasks such as image \nsegmentation, object detection, sentiment analysis \nor text categorization. Accurate and comprehensive \nannotations are essential for training models effec-\ntively.\n\u25aa Data storage and management: Their will be a \nrequirement to establish or utilise a robust data stor-\nage and management system to handle the large \nvolumes of data required for generative AI. This \nmay involve setting up a data warehouse, cloud stor-\nage or utilising data management platforms. All the \nwhile ensuring that the data is organised, accessible \nand secure for efficient training and model deploy-\nment. Data federation is a technology that can be \nconsidered here as it enables the creation of a physi-\ncally decentralized but functionally unified database. \nThis technology is particularly useful in healthcare \nas it allows various sources of data to keep the data \nwithin their firewalls. However, this step may not be \nrequired in most instances of the use of LLMs, par-\nticularly if they are drawn upon through application \nprogramming interface (API) calls or cloud services.\n\u25aa Computational resources: Generative AI models \noften require significant computational power and \nresources for training and inference such as GPUs \nor cloud computing services [8, 15]. In-house devel-\nopment and training of LLMs requires significant \ncomputational resources, which organisations must \ncarefully consider [63]. Commercial LLMs offered \nthrough cloud services or APIs spare organisations \nthis infrastructure burden. However, for those intent \non training proprietary models tuned to their specific \ndata and use cases, securing sufficient computing \ncapacity is critical.\nFactors that impact computational requirements \ninclude model size, training data volume and speed of \niteration desired. For example, a firm aiming to train a \nmodel with over a billion parameters on tens of billions \nof text examples would likely pursue a high-performance \ncomputing cluster or leverage cloud\u2013based machine \nlearning platforms. The precise hardware configuration\u2014\nincluding GPUs/TPUs, CPUs, memory, storage and net-\nworking\u2014scales with the model architecture and training \nplan [63].\nOngoing model development and fine-tuning also \nnecessitates available compute. Organisations \ncan \nchoose between continuing to allocate internal resources \nor outsourcing cycles via cloud services [63]. Budget-\nary planning should account for these recurring com-\npute demands if continually enhancing in-house LLMs \nis a priority. Overall, while leveraging external LLMs \ncan minimise infrastructure investments, serious inter-\nnal LLM initiatives can rival the computational scale of \nindustrial research labs.\nThird component: technical integration\nIntegrating generative AI into a healthcare information \nsystem or platform can bring numerous benefits, such \nas improved disease diagnosis, enhanced patient moni-\ntoring and more efficient healthcare delivery. However, \ngenerative AI technologies like GANs and LLMs are \ncomplex to understand and implement [8]. The tech-\nnology\u2019s maturity, reliability and ease of integration into \nexisting systems are crucial factors affecting its adoption \n[58]. Therefore, integrating generative AI into a hospital \nor healthcare information system involves several steps \nranging from understanding the needs of the system to \nimplementing and maintaining the AI solution. The first \nstep in integrating generative AI into a healthcare system \nis to identify the focus area of implementation [62]. This \ncould be anything from improving patient care, stream-\nlining administrative tasks, enhancing diagnostic accu-\nracy or predicting patient outcomes. Once the need is \nidentified, the right AI model needs to be chosen. Gener-\native AI models, such as GANs, can be used for tasks like \nsynthesising medical images or generating patient data \n[6, 37). LLMs can be used for EHR analysis and as a clini-\ncal decision support tool [40]. Once the model is chosen, \nit needs to be trained on the collected data. This involves \nfeeding the data into the model and adjusting the model\u2019s \nparameters until it can accurately predict outcomes or \ngenerate useful outputs.\nOnce the AI model is trained and tested, it can be inte-\ngrated into the healthcare information system [56, 62]. \nThis involves developing an interface between the AI \nmodel and the existing system, ensuring that the model \ncan access the data it needs and that its outputs can be \nPage 11 of 15\nReddy \ufeffImplementation Science           (2024) 19:27 \n\t\nused by the system. Developing such an interface or API \nallows the generative AI models to be seamlessly inte-\ngrated into the organisational or clinical workflow. After \nintegration, the AI system needs to be extensively tested \nto ensure its functionality, usability and reliability.\nRegular maintenance is also necessary to update the \nmodel as new data becomes available and to retrain it \nif its performance drops [56, 62]. Furthermore, gather-\ning regular/scheduled feedback from healthcare profes-\nsionals will ensure the organisation can make necessary \nrefinements to improve the system\u2019s performance.\nWhen leveraging external LLMs for healthcare applica-\ntions, stringent data governance practices are imperative \nto safeguard sensitive patient information [64]. As text or \nspeech data gets routed to third-party LLM services for \nanalysis, the contents contain protected health informa-\ntion (PHI) and personally identifiable information (PII) \nthat must remain confidential.\nWhile LLMs themselves are static analysis models \nrather than continuously learning systems, the vendors \nhosting these models and powering predictions still phys-\nically or computationally access submitted data [65, 66]. \nIrrespective of the vendors\u2019 reassurances about privacy \ncommitments, obligations and restrictions on ingesting \ncustomer content for model retraining, residual risks of \ndata leakage or unintended retention persist. To miti-\ngate these risks, comprehensive legal contracts between \nthe healthcare organisation and LLM vendor are founda-\ntional to ensuring PHI/PII protection in accordance with \nhealth regulations. Business associate agreements, data \nusage agreements and master service provider contracts \nallow formally codifying allowable LLM data process-\ning, storage, transmission and disposal protocols. Such \ncontracts also establish liability and enforcement mecha-\nnisms in case of a breach attributed to the vendor, includ-\ning notification, indemnification and restitution clauses. \nStrict access controls, encryption schemes, activity audit \nprotocols and authorization procedures should comple-\nment these contractual protections. While LLMs them-\nselves may not endlessly accumulate healthcare data like \nperpetually learning systems, due diligence around the \nlong-term fate of data sent to LLM prediction services \nremains highly advisable for risk-averse legal and compli-\nance teams [14]. Establishing robust data governance for \nemerging clinical LLM integration can prevent problem-\natic regulatory, ethical and reputational exposure [64].\nWhile beyond the scope of this article to discuss in \ndetail, the organisation will additionally have a respon-\nsibility to ensure the AI system complies with relevant \nhealthcare regulations and privacy laws [55], such as \nHealth Insurance Portability and Accountability Act \n(HIPAA) in the USA or General Data Protection Regula-\ntion (GDPR) in the European Union.\nFourth component: governance\nWhile generative AI has several potential applications in \nclinical medicine, there are also several challenges asso-\nciated with its implementation. Some of the challenges \ninclude the following:\n\u25aa Data availability: Generative AI requires large \namounts of data to train models effectively [8]. How-\never, in clinical medicine, data is often limited due to \nprivacy concerns and regulations. This can make it \ndifficult to train models effectively.\n\u25aa Bias in training data: Generative AI models require \nlarge amounts of training data to learn patterns and \ngenerate new data. If the training data is biased, the \ngenerative AI model will also be biased [13]. For \nexample, if the training data is skewed towards a par-\nticular demographic group, the generative AI model \nmay produce biased results for that group.\n\u25aa Transparency: While powerful LLMs like Chat-\nGPT demonstrate impressive conversational abil-\nity, the opaque sourcing of their massive training \ncorpora has rightly drawn scrutiny [64, 65]. Absent \ntransparency around the origin, copyright status and \nconsent policies of underlying data, legal and ethical \nblind spots remain. For commercially offered LLMs, \ndetails of training processes understandably remain \nproprietary intellectual property. However, the use \nof scraped web pages, private discussions, or copy-\nrighted content without permission during model \ndevelopment can still create liability. Recent lawsuits \nalleging unauthorised scraping by LLM providers \nexemplify the growing backlash.\n\u25aa Model interpretability: Generative AI models can \nbe complex and difficult to interpret, making it chal-\nlenging for clinicians to understand how the model \narrived at its conclusions [13, 67]. This can make it \ndifficult to trust the model\u2019s output and incorporate it \ninto clinical decision-making.\n\u25aa Inaccurate generation: While LLMs demonstrate \nimpressive fluency and versatility in conversational \napplications, their reliability breaks down when \napplied to high-stakes domains like healthcare [14, \n55]. Without the contextual grounding in factual \nknowledge and reasoning capacity needed for medi-\ncal decision-making, LLMs pose substantial patient \nsafety risks if overly trusted by clinicians [14]. Hal-\nlucination errors represent one demonstrated fail-\nure mode, where LLMs confidently generate plau-\nsible-sounding but entirely fabricated responses \nlied outside their training distributions. For patient \nassessments, treatment plans or other clinical sup-\nport functions, such creative falsehoods could read-\nily culminate in patient harm if not rigorously vali-\nPage 12 of 15\nReddy \ufeffImplementation Science           (2024) 19:27 \ndated [64]. Additionally, LLMs often ignore nuanced \ndependencies in multi-step reasoning that underlie \nsound medical judgments. Their capabilities centre \non statistical associations rather than causal implica-\ntions [68]. As such, they frequently oversimplify the \ncomplex decision chains requiring domain exper-\ntise that clinicians must weigh. Blindly accepting an \nLLM-generated diagnostic or therapeutic suggestion \nwithout scepticism can thus propagate errors.\n\u25aa Regulatory and ethical issues: The use of genera-\ntive AI in clinical medicine raises several regulatory \nand ethical issues [14], including patient privacy, \ndata ownership and accountability. Regulatory poli-\ncies, ethical considerations and public opinion form \nthe wider context. Data privacy laws like GDPR in \nEurope or HIPAA in the USA have implications \nfor AI in healthcare [65]. These aspects need to be \naddressed to ensure that the use of generative AI is \nethical and legal.\n\u25aa Validation: Generative AI models need to be vali-\ndated to ensure that they are accurate and reliable \n[62]. This requires large datasets and rigorous testing, \nwhich can be time-consuming and expensive.\nTo minimise risks arising from the application of gen-\nerative AI in healthcare, it is important to establish a \ngovernance and evaluation framework grounded in \nimplementation science [64]. Frameworks such as the \nNASSS framework and the TAM should inform sub-\nsequent steps to promote responsible and ethical use \nof generative AI [58, 69]. This implementation science \ninformed approach includes several steps to ensure \nappropriate testing, monitoring and iteration of the tech-\nnology. The NASSS framework provides a useful lens \nfor assessing the complex adaptive systems into which \ngenerative AI solutions would be embedded [58]. This \nframework examines factors like the condition, technol-\nogy, value proposition, adopter system, organisation, \nwider context, and interaction and mutual adaptation \nover time. Analysing these elements can reveal barriers \nand enablers to adopting generative AI across health-\ncare organisations. Similarly, the TAM focuses specifi-\ncally on human and social factors influencing technology \nuptake [59]. By evaluating perceived usefulness and per-\nceived ease of use of generative AI systems, TAM pro-\nvides insights into how both patients and providers \nmay respond to and interact with the technology. TAM \nencourages stakeholder participation in system design to \noptimize user acceptance.\nBoth NASSS and TAM demand a thoughtful change \nmanagement strategy for introducing new technologies \nlike generative AI. This means conducting iterative test-\ning and piloting of systems, co-developing governance \npolicies with diverse voices, emphasizing transparency, \nproviding extensive user training resources, developing \nprotocols to assess AI quality and fairness, allowing user \ncustomization of tools, and continually evaluating impact \nto enable appropriate adaptation over time. Drawing \nfrom these models ensures responsible and ethical inte-\ngration guided by end-user needs. The following are cor-\nresponding steps:\n\u25aa Establish or utilise a governance committee: This \ncommittee should be composed of experts in AI, \nhealthcare, ethics, law and patient advocacy. The \ncommittee\u2019s responsibility is to supervise the creation \nand implementation of generative AI applications in \nhealthcare, making sure they adhere to the highest \nmoral, statutory and professional standards.\n\u25aa Develop relevant policies and guidelines: Create \npolicies and guidelines that address issues like data \nprotection and security, informed consent, openness, \nresponsibility and fairness in relation to the usage of \ngenerative AI in healthcare. The guidelines should \nalso cover potential AI abuse and lay out precise \nreporting and resolution processes.\n\u25aa Implement robust data management practices: This \nincludes ensuring data privacy and security, obtain-\ning informed consent for data use, and ensuring data \nquality and integrity. It also involves using diverse \nand representative datasets to avoid bias in AI out-\nputs.\n\u25aa Mitigate inaccurate generated data: Overall, while \nLLMs have strengths in certain narrow applications, \ntheir limitations in recalling latest findings, ground-\ning advice in biomedical knowledge and deliberative \nanalytical thinking pose risks in clinical roles [14]. \nMitigating these requires both technological and pro-\ncess safeguards. At minimum, meticulous testing on \nmassive, validated datasets, transparent uncertainty \nquantification, multi-modal human-AI collaboration \nand consistent expert oversight prove essential before \ncontemplating LLM adoption for patient-impacting \nfunctions. With careful governance, LLMs may aid \nclinicians but cannot replace them.\n\u25aa Risk assessment: Prior to implementation, health-\ncare organisations must undertake structured risk \nassessments to inventory and quantify potential \npatient harms from generative AI adoption. Multi-\ndisciplinary teams including clinicians, IT security, \nlegal/compliance, risk management and AI engineers \nshould participate. A broad examination of use cases, \ndata dependencies, performance assumptions, safe-\nguards, governance and liability scenarios provide \nthe foundation. Identified dangers span clinical inac-\ncuracies like inappropriate treatment suggestions to \nPage 13 of 15\nReddy \ufeffImplementation Science           (2024) 19:27 \n\t\noperational risks like biased outputs or diagnostics \nhalted by technical outages. Other key considerations \nare malicious misuse, defects propagating as training \ndata and breach of sensitive records compromising \nprivacy or trust.\nFor each plausible risk, the assessment calibrates probabil-\nity and severity estimates for variables like user types, infor-\nmation classes and mitigating controls. Continuous risk \nmonitoring based on leading indicators and usage audits \nensures the initial assessment adapts alongside inevitable \nmodel and application changes over time. Periodic proba-\nbilistic modelling using safety assurance ",
    "methodology": "Methods\u2002 This article aims to\u00a0provide a\u00a0comprehensive overview of\u00a0the\u00a0use of\u00a0generative AI in\u00a0healthcare, focus-\ning on\u00a0the\u00a0utility of\u00a0the\u00a0technology in\u00a0healthcare and\u00a0its translational application highlighting the\u00a0need for\u00a0careful \nplanning, execution and\u00a0management of\u00a0expectations in\u00a0adopting generative AI in\u00a0clinical medicine. Key consid-\nerations include factors such as\u00a0data privacy, security and\u00a0the\u00a0irreplaceable role of\u00a0clinicians\u2019 expertise. Frameworks \nlike\u00a0the\u00a0technology acceptance model (TAM) and\u00a0the\u00a0Non-Adoption, Abandonment, Scale-up, Spread and\u00a0Sustain-\nability (NASSS) model are considered to\u00a0promote responsible integration. These frameworks allow anticipating \nand\u00a0proactively addressing barriers to\u00a0adoption, facilitating stakeholder participation and\u00a0responsibly transitioning \ncare systems to\u00a0harness generative AI\u2019s potential.\n",
    "results": "Results\u2002 Generative AI has\u00a0the\u00a0potential to\u00a0transform healthcare through\u00a0automated systems, enhanced clinical \ndecision-making and\u00a0democratization of\u00a0expertise with\u00a0diagnostic support tools providing timely, personalized \nsuggestions. Generative AI applications across\u00a0billing, diagnosis, treatment and\u00a0research can also\u00a0make healthcare \ndelivery more efficient, equitable and\u00a0effective. However, integration of\u00a0generative AI necessitates meticulous change \nmanagement and\u00a0risk mitigation strategies. Technological capabilities alone cannot shift complex care ecosystems \novernight; rather, structured adoption programs grounded in\u00a0implementation science are imperative.\n",
    "conclusion": ""
  },
  "paper_2.pdf": {
    "introduction": "INTRODUCTION\nThe rapid advancements in arti\ufb01cial intelligence (AI) have led to\nthe development of sophisticated large language models (LLM)\nsuch as OpenAI\u2019s GPT-4 and Google\u2019s Bard1,2. The unprecedented\npopularity of ChatGPT, GPT-4\u2019s predecessor released in November\n2022, is re\ufb02ected by the most rapid uptake of users - 100 million in\n2 months - for any new technology.\nThis rapid growth sparked global debates about the role such\nconversational chatbots could play in healthcare and the practice\nof\nmedicine.\nDiverse\napplications\nof\nLLMs\nhave\nappeared\nincluding facilitating clinical documentation; creating discharge\nsummaries; generating clinic, operation, and procedure notes;\nobtaining\ninsurance\npre-authorization;\nsummarizing\nresearch\npapers; or working as a chatbot to answer questions for the\npatients with their speci\ufb01c data and concerns. LLMs can also assist\nphysicians in diagnosing conditions based on medical records,\nimages, laboratory results, and suggest treatment options or plans.\nAt the same time, patients can potentially become more\nautonomous than with prior search ",
    "methodology": "methods by obtaining\nindividualized assessment of their data, symptoms, and concerns.\nSystematic reviews highlighted other potential bene\ufb01ts too\nsuch as improved scienti\ufb01c writing, enhancing research equity,\nstreamlining the healthcare work\ufb02ow, cost saving, and improved\npersonalized learning in medical education3,4.\nGiven the potential implications on patient outcomes and\npublic health, it is imperative to consider how these new AI-based\ntools should be regulated. The regulation of these LLMs in\nmedicine and healthcare without damaging their promising\nprogress is a timely and critical challenge to ensure safety,\nmaintain ethical standards, pre-empt unfairness and bias, and\nprotect patient privacy. Whatever concerns have been previously\nrecognized with AI are now markedly ampli\ufb01ed with the multi-\npotency of LLMs.\nThis paper explores the potential risks and bene\ufb01ts of applying\nLLMs in healthcare settings and argues for the necessity of\nregulating LLMs differently than AI-based medical technologies\nthat are already on the market to mitigate potential harm and\nmaintain public trust in these breakthrough technologies.\nLLMS DIFFER FROM ALREADY REGULATED AI-BASED\nTECHNOLOGIES\nLLMs differ signi\ufb01cantly from prior deep learning methods in\nterms of their scale, capabilities, and potential impact. Here we\noutline the key characteristics of LLMs that set them apart from\ntraditional deep learning techniques.\nScale and complexity\nLLMs are trained on massive datasets and utilize billions of\nparameters, ",
    "results": "results it provides to varying reliability to a new level. Besides being an advanced LLM, it will be able to read\ntexts on images and analyze the context of those images. The regulation of GPT-4 and generative AI in medicine and healthcare\nwithout damaging their exciting and transformative potential is a timely and critical challenge to ensure safety, maintain ethical\nstandards, and protect patient privacy. We argue that regulatory oversight should assure medical professionals and patients can use\nLLMs without causing harm or compromising their data or privacy. This paper summarizes our practical recommendations for what\nwe can expect from regulators to bring this vision to reality.\nnpj Digital Medicine  (2023) 6:120 ; https://doi.org/10.1038/s41746-023-00873-0\nINTRODUCTION\nThe rapid advancements in arti\ufb01cial intelligence (AI) have led to\nthe development of sophisticated large language models (LLM)\nsuch as OpenAI\u2019s GPT-4 and Google\u2019s Bard1,2. The unprecedented\npopularity of ChatGPT, GPT-4\u2019s predecessor released in November\n2022, is re\ufb02ected by the most rapid uptake of users - 100 million in\n2 months - for any new technology.\nThis rapid growth sparked global debates about the role such\nconversational chatbots could play in healthcare and the practice\nof\nmedicine.\nDiverse\napplications\nof\nLLMs\nhave\nappeared\nincluding facilitating clinical documentation; creating discharge\nsummaries; generating clinic, operation, and procedure notes;\nobtaining\ninsurance\npre-authorization;\nsummarizing\nresearch\npapers; or working as a chatbot to answer questions for the\npatients with their speci\ufb01c data and concerns. LLMs can also assist\nphysicians in diagnosing conditions based on medical records,\nimages, laboratory results, and suggest treatment options or plans.\nAt the same time, patients can potentially become more\nautonomous than with prior search methods by obtaining\nindividualized assessment of their data, symptoms, and concerns.\nSystematic reviews highlighted other potential bene\ufb01ts too\nsuch as improved scienti\ufb01c writing, enhancing research equity,\nstreamlining the healthcare work\ufb02ow, cost saving, and improved\npersonalized learning in medical education3,4.\nGiven the potential implications on patient outcomes and\npublic health, it is imperative to consider how these new AI-based\ntools should be regulated. The regulation of these LLMs in\nmedicine and healthcare without damaging their promising\nprogress is a timely and critical challenge to ensure safety,\nmaintain ethical standards, pre-empt unfairness and bias, and\nprotect patient privacy. Whatever concerns have been previously\nrecognized with AI are now markedly ampli\ufb01ed with the multi-\npotency of LLMs.\nThis paper explores the potential risks and bene\ufb01ts of applying\nLLMs in healthcare settings and argues for the necessity of\nregulating LLMs differently than AI-based medical technologies\nthat are already on the market to mitigate potential harm and\nmaintain public trust in these breakthrough technologies.\nLLMS DIFFER FROM ALREADY REGULATED AI-BASED\nTECHNOLOGIES\nLLMs differ signi\ufb01cantly from prior deep learning methods in\nterms of their scale, capabilities, and potential impact. Here we\noutline the key characteristics of LLMs that set them apart from\ntraditional deep learning techniques.\nScale and complexity\nLLMs are trained on massive datasets and utilize billions of\nparameters, resulting in unprecedented complexity. This level of\nsophistication\nrequires\nregulatory\noversight\nthat\ntakes\ninto\naccount the challenges associated with interpretability, fairness,\nand unintended consequences. Moreover, LLMs use tokens that\ncan be words, subwords, or even characters as the smallest units\nof text used to represent and process language during the training\nand generation processes. Tokenization is a crucial step in natural\nlanguage processing (NLP) and allows LLMs to ef\ufb01ciently analyze\nand generate text, as these models are designed to process\nsequences of tokens rather than entire sentences or paragraphs.\nCurrently, tokenization is not covered by healthcare regulators.\n1The Medical Futurist Institute, Budapest, Hungary. 2Department of Behavioural Sciences, Semmelweis University, Budapest, Hungary. 3Scripps Research Translational Institute,\nScripps Research, La Jolla, CA, USA. \u2709email: berci@medicalfuturist.com\nwww.nature.com/npjdigitalmed\nPublished in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nHardware requirements\nLLMs require massive computational resources in terms of\n\ufb02oating-point\noperations\nper\nsecond\n(FLOPs)\nand\ngraphics\nprocessing unit (GPU) usage compared to previous deep learning\nmodels due to their large scale, extensive training data, a type of\nneural network\nmodel designed\nfor NLP tasks called\nthe\nTransformer architecture, and the need for \ufb01ne-tuning.\nBroad applicability\nUnlike specialized deep learning models that were trained to\naddress a speci\ufb01c medical issue or clinical need, LLMs possess\nversatile capabilities that span various domains, such as health-\ncare, \ufb01nance, and education. As a result, a one-size-\ufb01ts-all\nregulatory framework is ill-suited for LLMs, and oversight must\nbe adaptable to address diverse industry-speci\ufb01c concerns.\nReal-time adaptation\nLLMs can adapt their responses in real-time, based on user input\nand evolving contexts. This dynamic behavior demands that\nregulatory oversight incorporates continuous monitoring and\nevaluation mechanisms to ensure responsible usage and adher-\nence to ethical guidelines. This is similar to what adaptive AI-based\nmedical technologies would require from regulators.\nSocietal impact\nThe widespread adoption of LLMs has the potential to fundamen-\ntally transform various aspects of society. Consequently, regula-\ntory oversight must address not only the technical aspects of LLMs\nbut also their broader ethical, social, and economic implications.\nData privacy and security\nLLMs\u2019 reliance on extensive training data raises concerns related\nto data privacy and security. Regulatory oversight should establish\nrobust frameworks to protect sensitive information and prevent\nunauthorized access or misuse of these powerful models.\nThese unique characteristics of LLMs necessitate a tailored\napproach to regulatory oversight. Such an approach must be\nadaptive, holistic, and cognizant of the diverse challenges and\npotential\nconsequences\nthat\nLLMs\npresent,\nensuring\ntheir\nresponsible and ethical use across various domains.\nTHE FDA\u2019S PRE-LLM OVERSIGHT OF AI\nThe United States\u2019 Food And Drug Administration (FDA) has been\nleading the global discussions on regulatory oversight and has\nbeen a prominent example in providing regulations about\nemerging technologies from 3D printed medications to AI-based\nmedical tools5.\nWith the increasing adoption of digital health technologies, the\nFDA started regulating Software as a Medical Device (SaMD) that\nrefers to software solutions that perform medical functions and\nare used in the prevention, diagnosis, treatment, or monitoring of\nvarious diseases or conditions.\nAs a continuation of that approach, the FDA has been adapting\nits regulatory framework to speci\ufb01cally address AI and machine\nlearning (ML) technologies in medical devices6. The FDA released\na discussion paper that outlined their potential regulatory\napproach tailored to AI and ML technologies used in medical\ndevices7. The discussion paper proposed a total product lifecycle\n(TPLC) approach to regulating AI/ML-based SaMD, which focuses\non the continuous monitoring and improvement of these\ntechnologies throughout their lifespan. The proposed framework\nalso emphasized the importance of transparency, real-world\nperformance monitoring, and clear expectations for modi\ufb01cations\nand updates to AI/ML algorithms.\nCurrently, the FDA does not have speci\ufb01c categories exclusively\nfor AI-based technologies but evaluates them within the existing\nregulatory framework for medical devices8. They classify such\ndevices into three main categories based on their level of risk:\n\u25cf\nClass I (Low risk): These devices pose the least risk and are\nsubject to general controls, such as registration and listing,\nlabeling, and good manufacturing practices. Examples of Class\nI devices include non-powered surgical instruments and\ndental \ufb02oss. Some low-risk AI-based medical technologies\nmay fall under this category, depending on their intended use.\n\u25cf\nClass II (Moderate risk): These devices carry a higher level of\nrisk than Class I devices and are subject to both general\ncontrols and special controls, such as performance standards,\npostmarket surveillance, or speci\ufb01c labeling requirements.\nExamples of Class II devices include infusion pumps, surgical\ndrapes, and powered wheelchairs. Many AI-based medical\ntechnologies, such as diagnostic imaging systems, may fall\nunder this category.\n\u25cf\nClass III (High risk): These devices pose the highest risk and are\nsubject to general controls, special controls, and premarket\napproval (PMA). Class III devices often support or sustain\nhuman life, are of substantial importance in preventing\nimpairment of human health, or present a potential unreason-\nable risk of illness or injury. Examples of Class III devices\ninclude implantable pacemakers, arti\ufb01cial heart valves, and\nsome AI-based technologies used in critical medical decision-\nmaking.\nAI-based medical technologies may also be subject to the FDA\u2019s\nDigital Health Software Precerti\ufb01cation (Pre-Cert) Program, which\nis designed to streamline the regulatory process for SaMD,\nincluding AI-based technologies.\nA milestone in that process was the release of their database of\nspeci\ufb01cally\nAI-based\nmedical\ntechnologies\nwith\nregulatory\napprovals in 20219. As of April, 2023, 521 devices are included\nin that database. The most popular categories are radiology,\ncardiovascular and hematology with 392, 57 and 15 devices,\nrespectively. The vast majority (96%) were approved with a 510(k)\nclearance, while 18 (3.5%) received de novo pathway clearance\nand 3 (0.5%) premarket approval (PMA) clearance.\nAs other papers have pointed out, only a few of these devices\nwere tested in randomized controlled trials (RCTs) trials; and only a\nlimited\nnumber\nof\nstudies\nhave\nused\nexternal\nvalidation,\nprospective evaluation and diverse metrics to explore the full\nimpact of AI in real clinical settings, and the range of assessed use\ncases\nhas\nbeen\nrelatively\nnarrow\nwith\nno\nor\nvery\nlittle\ntransparency10.\nIn summary, while there has been progress in regulating AI, the\nFDA has not been able to solve the regulation of two advanced\ntechnological issues that are related but not the same. One is\nabout regulating adaptive algorithms that can adjust its para-\nmeters or behavior based on the input data or its performance on\na speci\ufb01c task. This adaptability allows the algorithm to improve\nits performance over time or respond to changing conditions.\nThe other one is related to the so-called autodidactic function in\ndeep learning. It refers to the ability of a system to teach itself\nwithout direct supervision, an approach that often requires\nunsupervised or self-supervised learning, where the model learns\npatterns and representations from the input data without relying\non labeled examples. Such an autodidactic deep learning model\ncan discover underlying structures and relationships in the data by\noptimizing its internal representations without explicit guidance.\nTHE LLM ERA IN THE PRACTICE OF MEDICINE\nTo date, no LLM has had pre-training with the corpus of medical\ninformation or with millions of patient records, images, lab data,\nand of\ufb01ce visit or bedside conversations. Details about the training\nB. Mesk\u00f3 and E.J. Topol\n2\nnpj Digital Medicine (2023)  120 \nPublished in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nof GPT-4, the most advanced LLM that was pubished in March\n2023, have not been released. Nevertheless, LLMs have transfor-\nmative potential, with use cases ranging from clinical documenta-\ntion to providing personalized health plans11. Figure 1 describes\n10 use cases for medical professionals and 10 for patients.\nAt the same time, the introduction of these models into\nhealthcare leads to the ampli\ufb01cation of risks and challenges.\nIt started posing a new challenge to physicians as patients\narrive to the meeting with not only responses received after\ngoogling their symptoms but also from ChatGPT-like chatbots.\nThere have been discussions about to what extent ChatGPT can\nbe used for medical research and summarizing peer-reviewed\npapers when it only provides sources it based its responses on\nafter speci\ufb01cally asking for it. Moreover, some of those sources\nhave been reported to be made up3.\nLLMs can sometimes \"hallucinate\" results, which refers to\ngenerating outputs that are not grounded in the input data or\nfactual information. Such misinformation may be related to a\ndiagnosis, treatment, or a recommended test. For the uninitiated,\nsuch outputs are conveyed with a high level of con\ufb01dence and\ncould easily be accepted by the prompter as truth\u2014which has the\npotential to be dangerous. Whether it is due to incomplete or\nbiased training data, its probabilistic nature or the lack of context;\nit poses a signi\ufb01cant risk of providing unreliable or outright false\nanswers\nin\nthe\nmedical\nsetting\nthat\nmight\nhave\nserious\nconsequences.\nAnother issue, bias in medicine while using LLMs can affect\nclinical decision-making, patient outcomes, and healthcare equity.\nIf the training data contains biases, such as underrepresentation of\ncertain demographic groups, overemphasis on speci\ufb01c treat-\nments, or outdated medical practices, LLMs may inadvertently\nlearn and propagate these biases in its outputs. Biased outputs\nfrom GPT-4 may lead to incorrect diagnoses or suboptimal\ntreatment recommendations, potentially causing harm to patients\nor delaying appropriate care.\nGPT-4 brings the potentials and the risks to a new level. It will\nbe able to read texts on images (including physicians\u2019 hand-\nwritten notes), and analyze the content and context of images.\nTable 1 summarizes the key differences between the previous and\nthe\nnew\nversion\nregarding\nhealthcare-related\nand\nmedical\nprompts. It shows that GPT-3 could handle simple prompts with\ngeneral queries, while GPT-4 is able to analyze complex, multi-\nlevel prompts, and provide more sophisticated results such as case\ndescriptions or research paper summaries.\nThe application of GPT-4 in healthcare raises ethical concerns\nthat warrant a regulatory framework. Issues such as transparency,\naccountability, and fairness need to be addressed to prevent\npotential ethical lapses. For instance, healthcare professionals and\npatients should be made aware of the AI\u2019s involvement in the\ndecision-making process and be provided with explanations for\nthe AI\u2019s recommendations.\nMoreover, regulatory oversight can help ensure that AI-driven\nmodels do not perpetuate or exacerbate existing healthcare\ndisparities. By mandating diverse and representative data sources,\nregulators can counteract potential biases within the AI\u2019s training\ndata, thus promoting fairness in the delivery of healthcare services.\nThe use of GPT-4 and ChatGPT in such environments calls for\nrobust regulations to ensure the con\ufb01dentiality and security of\npatient information. This could include speci\ufb01c guidelines for data\nanonymization, encryption, and secure storage, as well as measures\nto prevent unauthorized access or misuse of data by third parties.\nAs a sign of wide implementation, medical companies, digital\nhealth services and healthcare organizations have already started\nto implement ChatGPT into their core business. Examples include\nthe Microsoft-owned Nuance as they decided to add GPT-4 AI to\nits medical note-taking tool; and a French startup called Nabla that\nclaimed to be the \ufb01rst to build a tool using GPT-3 to help\nphysicians do their paperwork12,13.\nAll these examples and challenges prompt regulatory bodies to\nnot only start regulating LLMs as those models are being\ndeployed, but to regulate them differently that AI-technologies\ncurrently on the market.\nTHE REGULATORY CHALLENGES OF LLMS\nMost LLMs have been released globally and no country-speci\ufb01c\niterations are available requiring a global approach from regulators.\nIt is also not clear what technical category LLMs will fall into from the\nregulatory perspective. However, based on the differences between\nLLMs and prior deep learning methods, a new regulatory category\nmight be needed to address LLM-speci\ufb01c challenges and risks.\nA regulatory body only has to design regulations for LLMs if\neither the developers of LLMs make claims that their LLM can be\nused for a medical purpose; or if LLMs are developed for, adapted,\nmodi\ufb01ed or directed toward speci\ufb01cally medical purposes. Even if\ncurrently widespread LLMs won\u2019t fall into either category, the\nmedical alternatives of LLMs speci\ufb01cally trained on medical data\nand databases probably will.\nFig. 1\nTen examples of use cases of LLMs for medical professionals; and ten examples for patients.\nB. Mesk\u00f3 and E.J. Topol\n3\nPublished in partnership with Seoul National University Bundang Hospital\nnpj Digital Medicine (2023)  120 \nOne prominent example is Med-PaLM that DeepMind and\nGoogle researchers have published about. In that study, authors\nproposed a framework for human evaluation of model answers\nalong multiple axes including factuality, precision, possible harm,\nand bias. In addition, using a combination of prompting strategies,\ntheir model achieved 67.6% accuracy on the US Medical License\nExam questions, surpassing prior state-of-the-art by over 17%. As\nhuman evaluation reveals key gaps in the responses provided by\nthe LLM, they introduced instruction prompt tuning and the\nresulting model, Med-PaLM, performs encouragingly, but remains\ninferior to clinicians. Since then, GPT-4 could achieve an accuracy\nover 85% on the same exam14.\nWith the release of GPT-4 that can analyze not only texts but\nimages, it can be expected that the model will grow to analyze\nuploaded documents, research papers, hand-written notes, sound,\nand video in the near future. (Table 2).\nThis underscores the notion that it is not enough to regulate\ncurrent LLM models as the new iterations with those advanced\ncapabilities can be expected to get implemented at a similar rate\nof the previous iterations. Without taking these future additions\ninto consideration, a regulation that focuses on language models\nonly could miss important updates by the time those updates\nbecome widely accessible.\nCompanies with approved devices that decide to implement\nLLMs into their services face an additional challenge. Namely, how\nwill the FDA regulate an AI-based medical technology recently\ninfused with LLM if the technology was already approved for\nmedical uses? Table 3 summarizes the regulatory challenges.\nThere have been proposals about regulating LLMs, although those\ncome from outside healthcare. In a working paper, Hacker et al.\nsuggests a novel terminology to capture the AI value chain by\ndifferentiating between developers, deployers, professional and non-\nprofessional users, as well as recipients of LLM output. Authors also\nsuggested four strategies to ensure that these models are\ntrustworthy and deployed for the bene\ufb01t of society at large. In\ndetails, regulation should focus on concrete high-risk applications,\nand not the pre-trained model itself, and should include (i)\nobligations regarding transparency, (ii) risk management, (iii) non-\ndiscrimination provisions, and (iv) content moderation rules15.\nM\u00f6kander at al pointed out that existing auditing procedures\nfail to address the governance challenges posed by LLMs, and\noffered three contributions to \ufb01ll that gap namely 1) establishing\nthe need to develop new auditing procedures that capture the\nrisks posed by LLMs; 2) outlining a blueprint to audit LLMs in\nfeasible and effective ways by drawing on best practices from IT\ngovernance and system engineering; and 3) discussing the\nlimitations of the prospect of auditing LLMs at all16.\nSuch potential solutions could serve as a benchmark for new\nregulations in healthcare. In either case, regulators and lawmakers\nneed to act fast to keep track with the dynamics of the\nunprecedented evolution and progress of LLMs.\nAs a sign of the rising pressure on regulators, in March 2023, a\ngroup of prominent computer scientists and technology industry\nexecutives such as Elon Musk and Steve Wozniak called for \u201call AI\nlabs to immediately pause for at least 6 months the training of AI\nsystems more powerful than GPT-4\u201d17. Their letter mentioned that\n\u201crecent months have seen AI labs locked in an out-of-control race\nto develop and deploy ever more powerful digital minds that no\none \u2013 not even their creators \u2013 can understand, predict, or reliably\ncontrol. This pause should be public and veri\ufb01able, and include all\nTable 2.\nA list of types of content forms that LLMs could analyze now and possible new versions in the future.\nType of content\nPotential applications\nAvailability\ntext/conversations\nchatbots, text analysis, documentation\nYes\nimage analysis\ndetecting the content and the context of images\nIn 2023\ndocument/PDF analysis\nanalyzing research papers and creating summaries of documents\nN/A\nsound\nvoice-to-text applications and sound-based interactions\nN/A\nvideo\nanalyzing the content of videos and creating deepfakes\nN/A\nTable 1.\nDifferences between the depth and details of prompts for ChatGPT and GPT-4.\nPrompts\nChatGPT\nGPT-4\nPrompt 1 \u2013 Diagnosing a patient\nwith ambiguous symptoms\nA patient presents with fatigue, weight loss,\nand occasional dizziness. What are some\npossible causes for these symptoms?\nA 45-year-old male patient presents with a 3-month history of\nprogressive fatigue, unintentional weight loss of 15 pounds,\nand episodes of dizziness. Please provide a differential\ndiagnosis and suggest relevant diagnostic tests.\nPrompt 2 \u2013 Treatment\nrecommendations\nWhat are some common treatments for type 2\ndiabetes?\nA 55-year-old female with a recent diagnosis of type 2\ndiabetes has an HbA1c level of 8.5%. Outline a comprehensive\ntreatment plan, including lifestyle modi\ufb01cations,\npharmacological options, and follow-up monitoring.\nPrompt 3 \u2013 Patient education\nExplain high blood pressure in simple terms.\nCreate a patient-friendly educational handout on\nhypertension, including an overview of the condition, risk\nfactors, symptoms, potential complications, and management\nstrategies.\nPrompt 4 \u2013 Reviewing medical\nresearch\nTell me about the bene\ufb01ts of exercise for\nmental health.\nSummarize recent research \ufb01ndings on the relationship\nbetween physical activity and mental health outcomes,\nincluding potential mechanisms, types of exercise, and\nrecommendations for various populations.\nPrompt 5 \u2013 Clinical case scenario\nDescribe a patient with pneumonia.\nCreate a detailed clinical case scenario involving a 65-year-old\npatient presenting with community-acquired pneumonia,\nincluding history of present illness, relevant past medical\nhistory, physical examination \ufb01ndings, diagnostic test results,\nand treatment plan.\nB. Mesk\u00f3 and E.J. Topol\n4\nnpj Digital Medicine (2023)  120 \nPublished in partnership with Seoul National University Bundang Hospital\nkey actors. If such a pause cannot be enacted quickly, govern-\nments should step in and institute a moratorium.\u201d\nNotable AI experts such as Andrew Ng objected the idea and\ninstead, called for seeking a balance between the huge value AI is\ncreating vs realistic risks. We agree that a moratorium cannot be\nimplemented in practice unless governments step in; and \u201chaving\ngovernments pause emerging technologies they don\u2019t understand\nis anti-competitive, sets a terrible precedent, and is awful\ninnovation policy\u201d18.\nTo reinforce our concerns, it is worthy of mention that Italy became\nthe \ufb01rst Western country to temporarily block ChatGPT in April 2023\ndue to privacy concerns and the lack of proper regulation19.\n",
    "conclusion": ""
  }
}